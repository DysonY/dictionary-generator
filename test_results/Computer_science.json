{
    "Computer_science": "Computer science is the study of computation and information. Computer science deals with theory of computation, algorithms, computational problems and the design of computer systems hardware, software and applications. Computer science addresses both human-made and natural information processes, such as communication, control, perception, learning and intelligence especially in human-made computing systems and machines. According to Peter Denning, the fundamental question underlying computer science is, What can be automated? Its fields can be divided into theoretical and practical disciplines. Computational complexity theory is highly abstract, while computer graphics and computational geometry emphasizes real-world applications. Algorithmics is called the heart of computer science. Programming language theory considers approaches to the description of computational processes, while software engineering involves the use of programming languages and complex systems. Computer architecture and computer engineering deals with construction of computer components and computer-controlled equipment. Human\u2013computer interaction considers the challenges in making computers useful, usable, and accessible. Artificial intelligence aims to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, motion planning, learning, and communication found in humans and animals.",
    "Cambridge_Diploma_in_Computer_Science": "Diploma in Computer Science, originally known as the \"Diploma in Numerical Analysis and Automatic Computing\", was a conversion course in Computer Science offered by the University of Cambridge, England. It was equivalent to a master's degree in present-day nomenclature but the title diploma was retained for historic reasons, \"diploma\" being the archaic term for a master's degree. The Diploma was the world's first full-year taught course in computer science, starting in 1953. It attracted students with degrees in mathematics, science and engineering. At its peak, there were 50 students on the course. UK government (EPSRC) funding was withdrawn in 2001 and student numbers dropped dramatically. In 2007, the University took the decision to withdraw the Diploma at the end of the 2007-08 academical year, after 55 years of service.",
    "Peter_J._Denning": "Peter James Denning (born January 6, 1942) is an American computer scientist and writer. He is best known for pioneering work in virtual memory, especially for inventing the working-set model for program behavior, which addressed thrashing in operating systems and became the reference standard for all memory management policies. He is also known for his works on principles of operating systems, operational analysis of queueing network systems, design and implementation of CSNET, the ACM digital library, codifying the great principles of computing, and most recently for the book The Innovator's Way, on innovation as a set of learnable practices.",
    "Circuit_(computer_science)": "In theoretical computer science, a circuit is a model of computation in which input values proceed through a sequence of gates, each of which computes a function. Circuits of this kind provide a generalization of Boolean circuits and a mathematical model for digital logic circuits. Circuits are defined by the gates they contain and the values the gates can produce. For example, the values in a Boolean circuit are boolean values, and the circuit includes conjunction, disjunction, and negation gates. The values in an integer circuit are sets of integers and the gates compute set union, set intersection, and set complement, as well as the arithmetic operations addition and multiplication.",
    "Analysis_of_algorithms": "In computer science, the analysis of algorithms is the process of finding the computational complexity of algorithms \u2013 the amount of time, storage, or other resources needed to execute them. Usually, this involves determining a function that relates the length of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). An algorithm is said to be efficient when this function's values are small, or grow slowly compared to a growth in the size of the input. Different inputs of the same length may cause the algorithm to have different behavior, so best, worst and average case descriptions might all be of practical interest. When not otherwise specified, the function describing the performance of an algorithm is usually an upper bound, determined from the worst case inputs to the algorithm. The term \"analysis of algorithms\" was coined by Donald Knuth. Algorithm analysis is an important part of a broader computational complexity theory, which provides theoretical estimates for the resources needed by any algorithm which solves a given computational problem. These estimates provide an insight into reasonable directions of search for efficient algorithms. In theoretical analysis of algorithms it is common to estimate their complexity in the asymptotic sense, i.e., to estimate the complexity function for arbitrarily large input. Big O notation, Big-omega notation and Big-theta notation are used to this end. For instance, binary search is said to run in a number of steps proportional to the logarithm of the length of the sorted list being searched, or in O(log(n)), colloquially \"in logarithmic time\". Usually asymptotic estimates are used because different implementations of the same algorithm may differ in efficiency. However the efficiencies of any two \"reasonable\" implementations of a given algorithm are related by a constant multiplicative factor called a hidden constant. Exact (not asymptotic) measures of efficiency can sometimes be computed but they usually require certain assumptions concerning the particular implementation of the algorithm, called model of computation. A model of computation may be defined in terms of an abstract computer, e.g., Turing machine, and/or by postulating that certain operations are executed in unit time.For example, if the sorted list to which we apply binary search has n elements, and we can guarantee that each lookup of an element in the list can be done in unit time, then at most log2 n + 1 time units are needed to return an answer.",
    "Herman_Hollerith": "Herman Hollerith (February 29, 1860 \u2013 November 17, 1929) was an American businessman, inventor, and statistician who developed an electromechanical tabulating machine for punched cards to assist in summarizing information and, later, in accounting. His invention of the punched card tabulating machine, patented in 1884, marks the beginning of the era of semiautomatic data processing systems, and his concept dominated that landscape for nearly a century. Hollerith founded a company that was amalgamated in 1911 with several other companies to form the Computing-Tabulating-Recording Company. In 1924, the company was renamed \"International Business Machines\" (IBM) and became one of the largest and most successful companies of the 20th century. Hollerith is regarded as one of the seminal figures in the development of data processing.",
    "Computer_algebra": "In mathematics and computer science, computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols. Software applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc. Computer algebra is widely used to experiment in mathematics and to design the formulas that are used in numerical programs. It is also used for complete scientific computations, when purely numerical methods fail, as in public key cryptography, or for some non-linear problems.",
    "Algebraic_data_type": "In computer programming, especially functional programming and type theory, an algebraic data type is a kind of composite type, i.e., a type formed by combining other types. Two common classes of algebraic types are product types (i.e., tuples and records) and sum types (i.e., tagged or disjoint unions, coproduct types or variant types). The values of a product type typically contain several values, called fields. All values of that type have the same combination of field types. The set of all possible values of a product type is the set-theoretic product, i.e., the Cartesian product, of the sets of all possible values of its field types. The values of a sum type are typically grouped into several classes, called variants. A value of a variant type is usually created with a quasi-functional entity called a constructor. Each variant has its own constructor, which takes a specified number of arguments with specified types. The set of all possible values of a sum type is the set-theoretic sum, i.e., the disjoint union, of the sets of all possible values of its variants. Enumerated types are a special case of sum types in which the constructors take no arguments, as exactly one value is defined for each constructor. Values of algebraic types are analyzed with pattern matching, which identifies a value by its constructor or field names and extracts the data it contains. Algebraic data types were introduced in Hope, a small functional programming language developed in the 1970s at the University of Edinburgh.",
    "Zentralblatt_MATH": "zbMATH, formerly Zentralblatt MATH, is a major international reviewing service providing reviews and abstracts for articles in pure and applied mathematics, produced by the Berlin office of FIZ Karlsruhe \u2013 Leibniz Institute for Information Infrastructure GmbH. Editors are the European Mathematical Society (EMS), FIZ Karlsruhe, and the Heidelberg Academy of Sciences. zbMATH is distributed by Springer Science+Business Media. It uses the Mathematics Subject Classification codes for organising the reviews by topic.",
    "Wilhelm_Schickard": "Wilhelm Schickard (22 April 1592 \u2013 24 October 1635) was a German professor of Hebrew and astronomy who became famous in the second part of the 20th century after , a biographer (along with ) of Johannes Kepler, claimed that the drawings of a calculating clock, predating the public release of Pascal's calculator by twenty years, had been discovered in two unknown letters written by Schickard to Johannes Kepler in 1623 and 1624. Hammer asserted that because these letters had been lost for three hundred years, Blaise Pascal had been called and celebrated as the inventor of the mechanical calculator in error during all this time. After careful examination it was found that Schickard's drawings had been published at least once per century starting from 1718, that his machine was not complete and required additional wheels and springs and that it was designed around a single tooth carry mechanism that didn't work properly when used in calculating clocks. Schickard's machine was the first of several designs of direct entry calculating machines in the 17th century (including the designs of Blaise Pascal, Tito Burattini, Samuel Morland and Ren\u00e9 Grillet). The Schickard machine was particularly notable for its integration of an ingenious system of rotated Napier's rods for multiplication with a first known design for an adding machine, operated by rotating knobs for input, and with a register of rotated numbers showing in windows for output. Taton has argued that Schickard's work had no impact on the development of mechanical calculators. However, whilst there can be debate about what constitutes a \"mechanical calculator\" later devices, such as Moreland's multiplying and adding instruments when used together, Caspar Schott's Cistula, Ren\u00e9 Grillet's machine arithm\u00e9tique, and Claude Perrault's rhabdologique at the end of the century, and later, the  developed in the early 20th century, certainly followed the same path pioneered by Schickard with his ground breaking combination of a form of Napier's rods and adding machine designed to assist multiplication.",
    "Cybernetics": "Cybernetics is a transdisciplinary approach for exploring regulatory systems\u2014their structures, constraints, and possibilities. Norbert Wiener defined cybernetics in 1948 as \"the scientific study of control and communication in the animal and the machine\". Cybernetics is applicable when a system being analyzed incorporates a closed signaling loop\u2014originally referred to as a \"circular causal\" relationship\u2014that is, where action by the system generates some change in its environment and that change is reflected in the system in some manner (feedback) that triggers a system change. Cybernetics is relevant to, for example, mechanical, physical, biological, cognitive, and social systems. The essential goal of the broad field of cybernetics is to understand and define the functions and processes of systems that have goals and that participate in circular, causal chains that move from action to sensing to comparison with desired goal, and again to action. Its focus is how anything (digital, mechanical or biological) processes information, reacts to information, and changes or can be changed to better accomplish the first two tasks. Cybernetics includes the study of feedback, black boxes and derived concepts such as communication and control in living organisms, machines and organizations including self-organization. Concepts studied by cyberneticists include, but are not limited to: learning, cognition, adaptation, social control, emergence, convergence, communication, efficiency, efficacy, and connectivity. In cybernetics these concepts (otherwise already objects of study in other disciplines such as biology and engineering) are abstracted from the context of the specific organism or device. The word cybernetics comes from Greek \u03ba\u03c5\u03b2\u03b5\u03c1\u03bd\u03b7\u03c4\u03b9\u03ba\u03ae (kybern\u0113tik\u1e17), meaning \"governance\", i.e., all that are pertinent to \u03ba\u03c5\u03b2\u03b5\u03c1\u03bd\u03ac\u03c9 (kybern\u00e1\u014d), the latter meaning \"to steer, navigate or govern\", hence \u03ba\u03c5\u03b2\u03ad\u03c1\u03bd\u03b7\u03c3\u03b9\u03c2 (kyb\u00e9rn\u0113sis), meaning \"government\", is the government while \u03ba\u03c5\u03b2\u03b5\u03c1\u03bd\u03ae\u03c4\u03b7\u03c2 (kybern\u1e17t\u0113s) is the governor or \"helmperson\" of the \"ship\". Contemporary cybernetics began as an interdisciplinary study connecting the fields of control systems, electrical network theory, mechanical engineering, logic modeling, evolutionary biology, neuroscience, anthropology, and psychology in the 1940s, often attributed to the Macy Conferences. During the second half of the 20th century cybernetics evolved in ways that distinguish first-order cybernetics (about observed systems) from second-order cybernetics (about observing systems). More recently there is talk about a third-order cybernetics (doing in ways that embraces first and second-order). Studies in cybernetics provide a means for examining the design and function of any system, including social systems such as business management and organizational learning, including for the purpose of making them more efficient and effective. Fields of study which have influenced or been influenced by cybernetics include game theory, system theory (a mathematical counterpart to cybernetics), perceptual control theory, sociology, psychology (especially neuropsychology, behavioral psychology, cognitive psychology), philosophy, architecture, and organizational theory. System dynamics, originated with applications of electrical engineering control theory to other kinds of simulation models (especially business systems) by Jay Forrester at MIT in the 1950s, is a related field.",
    "University_of_Cambridge": "The University of Cambridge (legally The Chancellor, Masters, and Scholars of the University of Cambridge) is a collegiate research university in Cambridge, United Kingdom. Founded in 1209 and granted a royal charter by King Henry III in 1231, Cambridge is the second-oldest university in the English-speaking world and the world's fourth-oldest surviving university. The university grew out of an association of scholars who left the University of Oxford after a dispute with the townspeople. The two 'ancient universities' share many common features and are often referred to jointly as 'Oxbridge'. Cambridge is formed from a variety of institutions which include 31 semi-autonomous constituent Colleges and over 100 academic departments organised into six schools. All the colleges are self-governing institutions within the university, each controlling its own membership and with its own internal structure and activities. All students are members of a college. Cambridge does not have a main campus, and its colleges and central facilities are scattered throughout the city. Undergraduate teaching at Cambridge is organised around weekly small-group supervisions in the colleges - a feature unique to the Oxbridge system. These are supported by classes, lectures, seminars, laboratory work and occasionally further supervisions provided by the central university faculties and departments. Postgraduate teaching is provided predominantly centrally. Cambridge University Press, a department of the university, is the second-largest university press in the world. Cambridge Assessment, also a department of the university, is one of the world's leading examining bodies and provides assessment to over eight million learners globally every year. The university also operates eight cultural and scientific museums, including the Fitzwilliam Museum, as well as a botanic garden. Cambridge's libraries, of which there are 116, hold a total of around 16 million books, around nine million of which are in Cambridge University Library, a legal deposit library. The university is home to, but independent of, the Cambridge Union \u2013 the world's oldest debating society. The university is closely linked to the development of the high-tech business cluster known as 'Silicon Fen'. It is the central member of Cambridge University Health Partners, an academic health science centre based around the Cambridge Biomedical Campus. In the fiscal year ending 31 July 2019, the central university, excluding colleges, had a total income of \u00a32.192 billion, of which \u00a3592.4 million was from research grants and contracts. At the end of the same financial year, the central university and colleges together possessed a combined endowment of over \u00a36.9 billion and overall consolidated net assets (excluding 'immaterial' historical assets) of \u00a312.569 billion. By both endowment size and consolidated assets, Cambridge is the wealthiest university in the United Kingdom. It is a member of numerous associations and forms part of the 'golden triangle' of English universities. Cambridge has educated many notable alumni, including eminent mathematicians, scientists, politicians, lawyers, philosophers, writers, actors, monarchs and other heads of state. As of October 2019, 120 Nobel Laureates, 11 Fields Medalists, 7 Turing Award winners and 14 British Prime Ministers have been affiliated with Cambridge as students, alumni, faculty or research staff. University alumni have won 194 Olympic medals.",
    "University_of_Manchester": "The University of Manchester is a public research university in Manchester, England, formed in 2004 by the merger of the University of Manchester Institute of Science and Technology and the Victoria University of Manchester. The University of Manchester is a red brick university, a product of the civic university movement of the late 19th century. The main campus is south of Manchester city centre on Oxford Road. The university owns and operates major cultural assets such as the Manchester Museum, Whitworth Art Gallery, John Rylands Library and Jodrell Bank Observatory\u2014a UNESCO World Heritage Site.In 2018/19, the university had 40,250 students and 10,400 staff, making it the second largest university in the UK (out of 169 including the Open University), and the largest single-site university. The university had a consolidated income of \u00a31.1 billion in 2018\u201319, of which \u00a3323.6 million was from research grants and contracts (6th place nationally behind Oxford, UCL, Cambridge, Imperial and Edinburgh). It has the fifth-largest endowment of any university in the UK, after the universities of Cambridge, Oxford, Edinburgh and King's. It is a member of the worldwide Universities Research Association, the Russell Group of British research universities and the N8 Group. The University of Manchester has 25 Nobel laureates among its past and present students and staff, the fourth-highest number of any single university in the United Kingdom. Four Nobel laureates are currently among its staff \u2013 more than any other British university.",
    "Batch_processing": "Computerized batch processing is the running of \"jobs that can run without end user interaction, or can be scheduled to run as resources permit.\"",
    "Science": "Science (from the Latin word scientia, meaning \"knowledge\") is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe. The earliest roots of science can be traced to Ancient Egypt and Mesopotamia in around 3500 to 3000 BCE. Their contributions to mathematics, astronomy, and medicine entered and shaped Greek natural philosophy of classical antiquity, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes. After the fall of the Western Roman Empire, knowledge of Greek conceptions of the world deteriorated in Western Europe during the early centuries (400 to 1000 CE) of the Middle Ages but was preserved in the Muslim world during the Islamic Golden Age. The recovery and assimilation of Greek works and Islamic inquiries into Western Europe from the 10th to 13th century revived \"natural philosophy\", which was later transformed by the Scientific Revolution that began in the 16th century as new ideas and discoveries departed from previous Greek conceptions and traditions. The scientific method soon played a greater role in knowledge creation and it was not until the 19th century that many of the institutional and professional features of science began to take shape; along with the changing of \"natural philosophy\" to \"natural science.\" Modern science is typically divided into three major branches that consist of the natural sciences (e.g., biology, chemistry, and physics), which study nature in the broadest sense; the social sciences (e.g., economics, psychology, and sociology), which study individuals and societies; and the formal sciences (e.g., logic, mathematics, and theoretical computer science), which study abstract concepts. There is disagreement, however, on whether the formal sciences actually constitute a science as they do not rely on empirical evidence. Disciplines that use existing scientific knowledge for practical purposes, such as engineering and medicine, are described as applied sciences. Science is based on research, which is commonly conducted in academic and research institutions as well as in government agencies and companies. The practical impact of scientific research has led to the emergence of science policies that seek to influence the scientific enterprise by prioritizing the development of commercial products, armaments, health care, and environmental protection.",
    "Computer_animation": "Computer animation is the process used for digitally generating animated images. The more general term computer-generated imagery (CGI) encompasses both static scenes and dynamic images, while computer animation only refers to moving images. Modern computer animation usually uses 3D computer graphics, although 2D computer graphics are still used for stylistic, low bandwidth, and faster real-time renderings. Sometimes, the target of the animation is the computer itself, but sometimes film as well. Computer animation is essentially a digital successor to stop motion techniques, but using 3D models, and traditional animation techniques using frame-by-frame animation of 2D illustrations. Computer-generated animations are more controllable than other, more physically based processes, like constructing miniatures for effects shots, or hiring extras for crowd scenes, because it allows the creation of images that would not be feasible using any other technology. It can also allow a single graphic artist to produce such content without the use of actors, expensive set pieces, or props. To create the illusion of movement, an image is displayed on the computer monitor and repeatedly replaced by a new image that is similar to it but advanced slightly in time (usually at a rate of 24, 25, or 30 frames/second). This technique is identical to how the illusion of movement is achieved with television and motion pictures. For 3D animations, objects (models) are built on the computer monitor (modeled) and 3D figures are rigged with a virtual skeleton. For 2D figure animations, separate objects (illustrations) and separate transparent layers are used with or without that virtual skeleton. Then the limbs, eyes, mouth, clothes, etc. of the figure are moved by the animator on key frames. The differences in appearance between key frames are automatically calculated by the computer in a process known as tweening or morphing. Finally, the animation is rendered. For 3D animations, all frames must be rendered after the modeling is complete. For 2D vector animations, the rendering process is the key frame illustration process, while tweened frames are rendered as needed. For pre-recorded presentations, the rendered frames are transferred to a different format or medium, like digital video. The frames may also be rendered in real time as they are presented to the end-user audience. Low bandwidth animations transmitted via the internet (e.g. Adobe Flash, X3D) often use software on the end-user's computer to render in real time as an alternative to streaming or pre-loaded high bandwidth animations.",
    "Programming_language": "A programming language is a formal language comprising a set of instructions that produce various kinds of output. Programming languages are used in computer programming to implement algorithms. Most programming languages consist of instructions for computers. There are programmable machines that use a set of specific instructions, rather than general programming languages. Early ones preceded the invention of the digital computer, the first probably being the automatic flute player described in the 9th century by the brothers Musa in Baghdad, during the Islamic Golden Age. Since the early 1800s, programs have been used to direct the behavior of machines such as Jacquard looms, music boxes and player pianos. The programs for these machines (such as a player piano's scrolls) did not produce different behavior in response to different inputs or conditions. Thousands of different programming languages have been created, and more are being created every year. Many programming languages are written in an imperative form (i.e., as a sequence of operations to perform) while other languages use the declarative form (i.e. the desired result is specified, not how to achieve it). The description of a programming language is usually split into the two components of syntax (form) and semantics (meaning). Some languages are defined by a specification document (for example, the C programming language is specified by an ISO Standard) while other languages (such as Perl) have a dominant implementation that is treated as a reference. Some languages have both, with the basic language defined by a standard and extensions taken from the dominant implementation being common.",
    "Norman_E._Gibbs": "Norman E. Gibbs (November 27, 1941 \u2013 April 25, 2002) was an American software engineer, scholar and educational leader. He studied to a B.Sc. in mathematics at Ursinus College (1964) and M.Sc. (1966) and Ph.D. (1969) in Computer Science at Purdue University, advised by Robert R. Korfhage. His research area was cycle generation, an area in graph theory.Gibbs joined the faculty at Bowdoin College in Maine, Arizona State University and College of William and Mary (mathematics) in Virginia before moving to Pittsburgh, joining Carnegie Mellon University as professor of computer science and becoming the first director of the educational program at the Software Engineering Institute (1987\u201397). Since then he was chief information officer at Guilford College in Greensboro and University of Connecticut, jointly serving as professor of Operations and Information management. He eventually worked for Ball State University as chair of computer science (2000\u201302).",
    "Software_engineering": "Software engineering is the systematic application of engineering approaches to the development of software. Software engineering is a branch of computing science.",
    "GCE_Advanced_Level": "The A Level (Advanced Level) is a subject-based qualification conferred as part of the General Certificate of Education, as well as a school leaving qualification offered by the educational bodies in the United Kingdom and the educational authorities of British Crown dependencies to students completing secondary or pre-university education. They were introduced in England and Wales in 1951 to replace the Higher School Certificate. A number of countries, including Singapore, Uganda, Kenya, Mauritius and Zimbabwe have developed qualifications with the same name as and a similar format to the British A Levels. Obtaining an A Level, or equivalent qualifications, is generally required for university entrance, with universities granting offers based on grades achieved. A Levels are generally worked towards over two years. Normally, students take three or four A Level courses in their first year of sixth form, and most taking four cut back to three in their second year. This is because university offers are normally based on three A Level grades, and taking a fourth can have an impact on grades. Unlike other level-3 qualifications, such as the International Baccalaureate, A Levels have no specific subject requirements, so students have the opportunity to combine any subjects they wish to take. However, students normally pick their courses based on the degree they wish to pursue at university: most degrees require specific A Levels for entry. In legacy modular courses (last assessment Summer 2019), A Levels are split into two parts, with students within their first year of study pursuing an Advanced Subsidiary qualification, commonly referred to as an AS or AS Level, which can either serve as an independent qualification or contribute 50% of the marks towards a full A Level award. The second part is known as an A2 or A2 Level, which is generally more in-depth and academically rigorous than the AS. The AS and A2 marks are combined for a full A Level award. The A2 Level is not a qualification on its own, and must be accompanied with an AS Level in the same subject for certification. Due to the fact that AS Levels are considered less academically rigorous, the A* grade is reserved for those taking the subject to A2 standard, so only A2 units contribute to this grade. Additionally, students who are displeased with results from their AS units have the ability to resit. However, this has been criticised as nurturing a 'resit culture' and causing perceived 'grade inflation'.",
    "Software_engineer": "A software engineer is a person who applies the principles of software engineering to the design, development, maintenance, testing, and evaluation of computer software. Prior to the mid-1970s, software practitioners generally called themselves computer scientists, computer programmers or software developers, regardless of their actual jobs. Many people prefer to call themselves software developer and programmer, because most widely agree what these terms mean, while the exact meaning of software engineer is still being debated.",
    "Statistics": "Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments. See glossary of probability and statistics. When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena. A standard statistical procedure involves the collection of data leading to test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected and an actual relationship between populations is missed giving a \"false negative\"). Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis. Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems. The earliest writings on probability and statistics, statistical methods drawing from probability theory, date back to Arab mathematicians and cryptographers, notably Al-Khalil (717\u2013786) and Al-Kindi (801\u2013873). In the 18th century, statistics also started to draw heavily from calculus. In more recent years statistics has relied more on statistical software to produce these tests such as descriptive analysis.",
    "Mohamed_M._Atalla": "Mohamed Mohamed Atalla (Arabic: \u0645\u062d\u0645\u062f \u0645\u062d\u0645\u062f \u0639\u0637\u0627\u0627\u0644\u0644\u0647\u200e; August 4, 1924 \u2013 December 30, 2009) was an Egyptian engineer, physical chemist, cryptographer, inventor and entrepreneur. He was a semiconductor pioneer who made important contributions to modern electronics. His invention of the MOSFET (metal\u2013oxide\u2013semiconductor field-effect transistor, or MOS transistor) in 1959, along with his earlier surface passivation and thermal oxidation processes, revolutionized the electronics industry. He is also known as the founder of the data security company Atalla Corporation (now Utimaco Atalla), founded in 1972. He received the Stuart Ballantine Medal (now the Benjamin Franklin Medal in physics) and was inducted into the National Inventors Hall of Fame for his important contributions to semiconductor technology as well as data security. Born in Port Said, Egypt, he was educated at Cairo University in Egypt and then Purdue University in the United States, before joining Bell Labs in 1949 and later adopting the more anglicized \"John\" or \"Martin\" M. Atalla as professional names. He made several important contributions to semiconductor technology at Bell, including his development of the surface passivation and thermal oxidation processes (the basis for silicon semiconductor technology such as the planar process and monolithic integrated circuit chips), his invention of the MOSFET (with Dawon Kahng) in 1959, and the PMOS and NMOS fabrication processes. Atalla's pioneering work at Bell contributed to modern electronics, the silicon revolution, and Digital Revolution. The MOSFET in particular is the basic building block of modern electronics, and is considered one of the most important inventions in electronics. It is also the most widely manufactured device in history, and the US Patent and Trademark Office calls it a \"groundbreaking invention that transformed life and culture around the world\". His invention of the MOSFET was initially overlooked at Bell, which led to him resigning from Bell and joining Hewlett-Packard (HP), founding its Semiconductor Lab in 1962 and then HP Labs in 1966, before leaving to join Fairchild Semiconductor, founding its Microwave & Optoelectronics division in 1969. His work at HP and Fairchild included research on Schottky diode, gallium arsenide (GaAs), gallium arsenide phosphide (GaAsP), indium arsenide (InAs) and light-emitting diode (LED) technologies. He later left the semiconductor industry, and became an entrepreneur in cryptography and data security. In 1972, he founded Atalla Corporation, and filed a patent for a remote Personal Identification Number (PIN) security system. In 1973, he released the first hardware security module, the \"Atalla Box\" which encrypted PIN and ATM messages, and went on to secure the majority of the world's ATM transactions. He later founded the Internet security company TriStrata Security in the 1990s. In recognition of his work on the PIN system of information security management as well as cybersecurity, Atalla has been referred to as the \"Father of the PIN\" and an information security pioneer. He died in Atherton, California, on December 30, 2009.",
    "William_J._Rapaport": "William Joseph Rapaport is an American philosopher. He is Associate Professor Emeritus at the University at Buffalo.",
    "Biology": "Biology is the natural science that studies life and living organisms, including their physical structure, chemical processes, molecular interactions, physiological mechanisms, development and evolution. Despite the complexity of the science, certain unifying concepts consolidate it into a single, coherent field. Biology recognizes the cell as the basic unit of life, genes as the basic unit of heredity, and evolution as the engine that propels the creation and extinction of species. Living organisms are open systems that survive by transforming energy and decreasing their local entropy to maintain a stable and vital condition defined as homeostasis. Sub-disciplines of biology are defined by the research methods employed and the kind of system studied: theoretical biology uses mathematical methods to formulate quantitative models while experimental biology performs empirical experiments to test the validity of proposed theories and understand the mechanisms underlying life and how it appeared and evolved from non-living matter about 4 billion years ago through a gradual increase in the complexity of the system. See branches of biology.",
    "Detection_theory": "Detection theory or signal detection theory is a means to measure the ability to differentiate between information-bearing patterns (called stimulus in living organisms, signal in machines) and random patterns that distract from the information (called noise, consisting of background stimuli and random activity of the detection machine and of the nervous system of the operator). In the field of electronics, the separation of such patterns from a disguising background is referred to as signal recovery. According to the theory, there are a number of determiners of how a detecting system will detect a signal, and where its threshold levels will be. The theory can explain how changing the threshold will affect the ability to discern, often exposing how adapted the system is to the task, purpose or goal at which it is aimed. When the detecting system is a human being, characteristics such as experience, expectations, physiological state (e.g., fatigue) and other factors can affect the threshold applied. For instance, a sentry in wartime might be likely to detect fainter stimuli than the same sentry in peacetime due to a lower criterion, however they might also be more likely to treat innocuous stimuli as a threat. Much of the early work in detection theory was done by radar researchers. By 1954, the theory was fully developed on the theoretical side as described by Peterson, Birdsall and Fox and the foundation for the psychological theory was made by Wilson P. Tanner, David M. Green, and John A. Swets, also in 1954.Detection theory was used in 1966 by John A. Swets and David M. Green for psychophysics. Green and Swets criticized the traditional methods of psychophysics for their inability to discriminate between the real sensitivity of subjects and their (potential) response biases. Detection theory has applications in many fields such as diagnostics of any kind, quality control, telecommunications, and psychology. The concept is similar to the signal to noise ratio used in the sciences and confusion matrices used in artificial intelligence. It is also usable in alarm management, where it is important to separate important events from background noise.",
    "Randomized_algorithm": "A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the \"average case\" over all possible choices of random bits. Formally, the algorithm's performance will be a random variable determined by the random bits; thus either the running time, or the output (or both) are random variables. One has to distinguish between algorithms that use the random input so that they always terminate with the correct answer, but where the expected running time is finite (Las Vegas algorithms, for example Quicksort), and algorithms which have a chance of producing an incorrect result (Monte Carlo algorithms, for example the Monte Carlo algorithm for the MFAS problem) or fail to produce a result either by signaling a failure or failing to terminate. In some cases, probabilistic algorithms are the only practical means of solving a problem. In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior.",
    "Scientific_modelling": "Scientific modelling is a scientific activity, the aim of which is to make a particular part or feature of the world easier to understand, define, quantify, visualize, or simulate by referencing it to existing and usually commonly accepted knowledge. It requires selecting and identifying relevant aspects of a situation in the real world and then using different types of models for different aims, such as conceptual models to better understand, operational models to operationalize, mathematical models to quantify, and graphical models to visualize the subject. Modelling is an essential and inseparable part of many scientific disciplines, each of which have their own ideas about specific types of modelling. The following was said by John von Neumann. ... the sciences do not try to explain, they hardly even try to interpret, they mainly make models. By a model is meant a mathematical construct which, with the addition of certain verbal interpretations, describes observed phenomena. The justification of such a mathematical construct is solely and precisely that it is expected to work\u2014that is, correctly to describe phenomena from a reasonably wide area. There is also an increasing attention to scientific modelling in fields such as science education, philosophy of science, systems theory, and knowledge visualization. There is a growing collection of methods, techniques and meta-theory about all kinds of specialized scientific modelling.",
    "CSAB_(professional_organization)": "CSAB, Inc., formerly called the Computing Sciences Accreditation Board, Inc., is a non-profit professional organization in the United States, focused on the quality of education in computing disciplines. The Association for Computing Machinery (ACM) and the IEEE Computer Society (IEEE-CS) are the member societies of CSAB.The Association for Information Systems (AIS) was a member society between 2002 and September 2009. CSAB itself is a member society of ABET, to support the accreditation of several computing (related) disciplines: \n* It is leading for computer science, information systems, information technology and software engineering \n* It is working together with other ABET member societies for computer engineering, information engineering and biological engineering Who is doing what: \n* For the disciplines where CSAB is leading, it develops the accreditation criteria and it educate the so-called Program Evaluators (PEVs). \n* But the accreditation activities themselves are conducted by the appropriate ABET accreditation commission. For computing this is the Computing Accreditation Commission (CAC).",
    "Robotics": "Robotics is an interdisciplinary research area at the interface of computer science and engineering. Robotics involves design, construction, operation, and use of robots. The goal of robotics is to design intelligent machines that can help and assist humans in their day-to-day lives and keep everyone safe. Robotics draws on the achievement of information engineering, computer engineering, mechanical engineering, electronic engineering and others. Robotics develops machines that can substitute for humans and replicate human actions. Robots can be used in many situations and for many purposes, but today many are used in dangerous environments (including inspection of radioactive materials, bomb detection and deactivation), manufacturing processes, or where humans cannot survive (e.g. in space, underwater, in high heat, and clean up and containment of hazardous materials and radiation). Robots can take on any form but some are made to resemble humans in appearance. This is said to help in the acceptance of a robot in certain replicative behaviors usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, or any other human activity. Many of today's robots are inspired by nature, contributing to the field of bio-inspired robotics. The concept of creating machines that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century. Throughout history, it has been frequently assumed by various scholars, inventors, engineers, and technicians that robots will one day be able to mimic human behavior and manage tasks in a human-like fashion. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes, whether domestically, commercially, or militarily. Many robots are built to do jobs that are hazardous to people, such as defusing bombs, finding survivors in unstable ruins, and exploring mines and shipwrecks. Robotics is also used in STEM (science, technology, engineering, and mathematics) as a teaching aid. Robotics is a branch of engineering that involves the conception, design, manufacture, and operation of robots. This field overlaps with computer engineering, computer science (especially artificial intelligence), electronics, mechatronics, mechanical, nanotechnology and bioengineering.",
    "Mathematics": "Mathematics (from Greek: \u03bc\u03ac\u03b8\u03b7\u03bc\u03b1, m\u00e1th\u0113ma, 'knowledge, study, learning') includes the study of such topics as quantity (number theory), structure (algebra), space (geometry), and change (mathematical analysis). It has no generally accepted definition. Mathematicians seek and use patterns to formulate new conjectures; they resolve the truth or falsity of such by mathematical proof. When mathematical structures are good models of real phenomena, mathematical reasoning can be used to provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity from as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry. Rigorous arguments first appeared in Greek mathematics, most notably in Euclid's Elements. Since the pioneering work of Giuseppe Peano (1858\u20131932), David Hilbert (1862\u20131943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day. Mathematics is essential in many fields, including natural science, engineering, medicine, finance, and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians engage in pure mathematics (mathematics for its own sake) without having any application in mind, but practical applications for what began as pure mathematics are often discovered later.",
    "Object-oriented_programming": "Object-oriented programming (OOP) is a programming paradigm based on the concept of \"objects\", which can contain data, in the form of fields (often known as attributes or properties), and code, in the form of procedures (often known as methods). A feature of objects is an object's procedures that can access and often modify the data fields of the object with which they are associated (objects have a notion of \"this\" or \"self\"). In OOP, computer programs are designed by making them out of objects that interact with one another. OOP languages are diverse, but the most popular ones are class-based, meaning that objects are instances of classes, which also determine their types. Many of the most widely used programming languages (such as C++, Java, Python, etc.) are multi-paradigm and they support object-oriented programming to a greater or lesser degree, typically in combination with imperative, procedural programming. Significant object-oriented languages includeJava,C++,C#,Python,R,PHP,JavaScript,Ruby,Perl,Object Pascal,Objective-C,Dart,Swift,Scala,Kotlin,Common Lisp,MATLAB,andSmalltalk.",
    "Control_flow": "In computer science, control flow (or flow of control) is the order in which individual statements, instructions or function calls of an imperative program are executed or evaluated. The emphasis on explicit control flow distinguishes an imperative programming language from a declarative programming language. Within an imperative programming language, a control flow statement is a statement that results in a choice being made as to which of two or more paths to follow. For non-strict functional languages, functions and language constructs exist to achieve the same result, but they are usually not termed control flow statements. A set of statements is in turn generally structured as a block, which in addition to grouping, also defines a lexical scope. Interrupts and signals are low-level mechanisms that can alter the flow of control in a way similar to a subroutine, but usually occur as a response to some external stimulus or event (that can occur asynchronously), rather than execution of an in-line control flow statement. At the level of machine language or assembly language, control flow instructions usually work by altering the program counter. For some central processing units (CPUs), the only control flow instructions available are conditional or unconditional branch instructions, also termed jumps.",
    "Type_system": "In programming languages, a type system is a logical system comprising a set of rules that assigns a property called a type to the various constructs of a computer program, such as variables, expressions, functions or modules. These types formalize and enforce the otherwise implicit categories the programmer uses for algebraic data types, data structures, or other components (e.g. \"string\", \"array of float\", \"function returning boolean\"). The main purpose of a type system is to reduce possibilities for bugs in computer programs by defining interfaces between different parts of a computer program, and then checking that the parts have been connected in a consistent way. This checking can happen statically (at compile time), dynamically (at run time), or as a combination of both. Type systems have other purposes as well, such as expressing business rules, enabling certain compiler optimizations, allowing for multiple dispatch, providing a form of documentation, etc. A type system associates a type with each computed value and, by examining the flow of these values, attempts to ensure or prove that no type errors can occur. The given type system in question determines what constitutes a type error, but in general the aim is to prevent operations expecting a certain kind of value from being used with values for which that operation does not make sense (logic errors). Type systems are often specified as part of programming languages and built into interpreters and compilers, although the type system of a language can be extended by optional tools that perform added checks using the language's original type syntax and grammar.",
    "Automated_planning_and_scheduling": "Automated planning and scheduling, sometimes denoted as simply AI planning, is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory. In known environments with available models, planning can be done offline. Solutions can be found and evaluated prior to execution. In dynamically unknown environments, the strategy often needs to be revised online. Models and policies must be adapted. Solutions usually resort to iterative trial and error processes commonly seen in artificial intelligence. These include dynamic programming, reinforcement learning and combinatorial optimization. Languages used to describe planning and scheduling are often called action languages.",
    "Kolmogorov_complexity": "In algorithmic information theory (a subfield of computer science and mathematics), the Kolmogorov complexity of an object, such as a piece of text, is the length of a shortest computer program (in a predetermined programming language) that produces the object as output. It is a measure of the computational resources needed to specify the object, and is also known as algorithmic complexity, Solomonoff\u2013Kolmogorov\u2013Chaitin complexity, program-size complexity, descriptive complexity, or algorithmic entropy. It is named after Andrey Kolmogorov, who first published on the subject in 1963. The notion of Kolmogorov complexity can be used to state and prove impossibility results akin to Cantor's diagonal argument, G\u00f6del's incompleteness theorem, and Turing's halting problem.In particular, no program P computing a lower bound for each text's Kolmogorov complexity can return a value essentially larger than P's own length (see section ); hence no single program can compute the exact Kolmogorov complexity for infinitely many texts.",
    "Bell_Labs": "Nokia Bell Labs (formerly named Bell Labs Innovations (1996\u20132007), AT&T Bell Laboratories (1984\u20131996) and Bell Telephone Laboratories (1925\u20131984)) is an American industrial research and scientific development company owned by Finnish company Nokia. With headquarters located in Murray Hill, New Jersey, the company operates several laboratories in the United States and around the world. Bell Labs has its origins in the complex past of the Bell System. In the late 19th century, the laboratory began as the Western Electric Engineering Department and was located at 463 West Street in New York City. In 1925, after years of conducting research and development under Western Electric, the Engineering Department was reformed into Bell Telephone Laboratories and under the shared ownership of American Telephone & Telegraph Company and Western Electric. Researchers working at Bell Labs are credited with the development of radio astronomy, the transistor, the laser, the photovoltaic cell, the charge-coupled device (CCD), information theory, the Unix operating system, and the programming languages B, C, C++, and S. Nine Nobel Prizes have been awarded for work completed at Bell Laboratories.",
    "Econet": "Econet was Acorn Computers's low-cost local area network system, intended for use by schools and small businesses. Econet software was mostly superseded by AUN, though some suppliers were still offering bridging kits to interconnect old and new networks. In turn, AUN was superseded by the Acorn Access+ software. Support for Econet was removed from the Linux kernel at version 3.5 in 2012, citing lack of use and privilege escalation vulnerabilities.",
    "Computer_programming": "Computer programming is the process of designing and building an executable computer program to accomplish a specific computing result. Programming involves tasks such as: analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding). The source code of a program is written in one or more languages that are intelligible to programmers, rather than machine code, which is directly executed by the central processing unit. The purpose of programming is to find a sequence of instructions that will automate the performance of a task (which can be as complex as an operating system) on a computer, often for solving a given problem. Proficient programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic. Tasks accompanying and related to programming include: testing, debugging, source code maintenance, implementation of build systems, and management of derived artifacts, such as the machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of code. Software engineering combines engineering techniques with software development practices. Reverse engineering is the opposite process. A hacker is any skilled computer expert that uses their technical knowledge to overcome a problem, but it can also mean a security hacker in common language.",
    "Combinatorial_optimization": "In operations research, applied mathematics and theoretical computer science, combinatorial optimization is a topic that consists of finding an optimal object from a finite set of objects. In many such problems, exhaustive search is not tractable. It operates on the domain of those optimization problems in which the set of feasible solutions is discrete or can be reduced to discrete, and in which the goal is to find the best solution. Some common problems involving combinatorial optimization are the travelling salesman problem (\"TSP\"), the minimum spanning tree problem (\"MST\"), and the knapsack problem. Combinatorial optimization is a subset of mathematical optimization that is related to operations research, algorithm theory, and computational complexity theory. It has important applications in several fields, including artificial intelligence, machine learning, auction theory, and software engineering. Some research literature considers discrete optimization to consist of integer programming together with combinatorial optimization (which in turn is composed of optimization problems dealing with graph structures) although all of these topics have closely intertwined research literature. It often involves determining the way to efficiently allocate resources used to find solutions to mathematical problems.",
    "Type_theory": "In mathematics, logic, and computer science, a type theory or type system is a formal system in which every term has a \"type\" which is used to restrict the operations that may be performed on it. Type Theory is the academic study of type theories. Some type theories can serve as alternatives to set theory as a foundation of mathematics. Two well-known such theories are Alonzo Church's typed \u03bb-calculus and Per Martin-L\u00f6f's intuitionistic type theory. Type Theory was created to avoid paradoxes in previous foundations such as naive set theory, formal logics and rewrite systems. Type Theory is closely related to, and in some cases overlaps with, computational type systems, which are a programming language feature used to reduce bugs.",
    "Computer_science_and_engineering": "Computer Science & Engineering (CSE) is an academic program at some universities that integrates the fields of computer engineering and computer science, providing knowledge of computing systems in both hardware and software design. The study program is modeled on German technical universities, where CS, CE and IT are treated as respectively the theoretical, technical and practical part of one field called \"Informatik\".",
    "Theory_of_computation": "In theoretical computer science and mathematics, the theory of computation is the branch that deals with how efficiently problems can be solved on a model of computation, using an algorithm. The field is divided into three major branches: automata theory and formal languages, computability theory, and computational complexity theory, which are linked by the question: \"What are the fundamental capabilities and limitations of computers?\". In order to perform a rigorous study of computation, computer scientists work with a mathematical abstraction of computers called a model of computation. There are several models in use, but the most commonly examined is the Turing machine. Computer scientists study the Turing machine because it is simple to formulate, can be analyzed and used to prove results, and because it represents what many consider the most powerful possible \"reasonable\" model of computation (see Church\u2013Turing thesis). It might seem that the potentially infinite memory capacity is an unrealizable attribute, but any decidable problem solved by a Turing machine will always require only a finite amount of memory. So in principle, any problem that can be solved (decided) by a Turing machine can be solved by a computer that has a finite amount of memory.",
    "Algorithmic_information_theory": "Algorithmic information theory (AIT) is a \"merger of information theory and computer science\" that concerns itself with the relationship between computation and information of computably generated objects (as opposed to stochastically generated), such as strings or any other data structure. In other words, it is shown within algorithmic information theory that computational incompressibility \"mimics\" (except for a constant that only depends on the chosen universal programming language) the relations or inequalities found in information theory. According to Gregory Chaitin, it is \"the result of putting Shannon's information theory and Turing's computability theory into a cocktail shaker and shaking vigorously.\" Algorithmic information theory principally studies measures of irreducible information content of strings (or other data structures). Because most mathematical objects can be described in terms of strings, or as the limit of a sequence of strings, it can be used to study a wide variety of mathematical objects, including integers. Indeed, one of the main motivations behind AIT is the very study of the information carried by mathematical objects as in the field of metamathematics, e.g., as shown by the incompleteness results mentioned below. Other main motivations came from: surpassing the limitations of classical information theory for single and fixed objects; formalizing the concept of randomness; and finding a meaningful probabilistic inference without prior knowledge of the probability distribution (e.g., whether it is independent and identically distributed, markovian, or even stationary). In this way, AIT is known to be basically found upon three main mathematical concepts and the relations between them: algorithmic complexity, algorithmic randomness, and algorithmic probability. Besides the formalization of a universal measure for irreducible information content of computably generated objects, some main achievements of AIT were to show that: in fact algorithmic complexity follows (in the self-delimited case) the same inequalities (except for a constant) that entropy does, as in classical information theory; randomness is incompressibility; and, within the realm of randomly generated software, the probability of occurrence of any data structure is of the order of the shortest program that generates it when running on a universal machine.",
    "Semantics_(computer_science)": "In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages. It does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation. Formal semantics, for instance, helps to write compilers, better understand what a program is doing, and to prove, e.g., that the following if statement if 1 == 1 then S1 else S2 has the same effect as S1 alone.",
    "Model_of_computation": "In computer science, and more specifically in computability theory and computational complexity theory, a model of computation is a model which describes how an output of a mathematical function is computed given an input. A model describes how units of computations, memories, and communications are organized. The computational complexity of an algorithm can be measured given a model of computation. Using a model allows studying the performance of algorithms independently of the variations that are specific to particular implementations and specific technology.",
    "Domain_theory": "Domain theory is a branch of mathematics that studies special kinds of partially ordered sets (posets) commonly called domains. Consequently, domain theory can be considered as a branch of order theory. The field has major applications in computer science, where it is used to specify denotational semantics, especially for functional programming languages. Domain theory formalizes the intuitive ideas of approximation and convergence in a very general way and has close relations to topology.",
    "Samuel_Morse": "Samuel Finley Breese Morse (April 27, 1791 \u2013 April 2, 1872) was an American inventor and painter. After having established his reputation as a portrait painter, in his middle age Morse contributed to the invention of a single-wire telegraph system based on European telegraphs. He was a co-developer of Morse code and helped to develop the commercial use of telegraphy.",
    "Artificial_intelligence": "In computer science, artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\". As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. A quip in Tesler's Theorem says \"AI is whatever hasn't been done yet.\" For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology. Modern machine capabilities generally classified as AI include successfully understanding human speech, competing at the highest level in strategic game systems (such as chess and Go), autonomously operating cars, intelligent routing in content delivery networks, and military simulations. Artificial intelligence was founded as an academic discipline in 1955, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an \"AI winter\"), followed by new approaches, success and renewed funding. For most of its history, AI research has been divided into sub-fields that often fail to communicate with each other. These sub-fields are based on technical considerations, such as particular goals (e.g. \"robotics\" or \"machine learning\"), the use of particular tools (\"logic\" or artificial neural networks), or deep philosophical differences. Sub-fields have also been based on social factors (particular institutions or the work of particular researchers). The traditional problems (or goals) of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects. General intelligence is among the field's long-term goals. Approaches include , , and . Many tools are used in AI, including versions of , artificial neural networks, and . The AI field draws upon computer science, information engineering, mathematics, psychology, linguistics, philosophy, and many other fields. The field was founded on the assumption that human intelligence \"can be so precisely described that a machine can be made to simulate it\". This raises philosophical arguments about the mind and the ethics of creating artificial beings endowed with human-like intelligence. These issues have been explored by myth, fiction and philosophy since antiquity. Some people also consider AI to be a danger to humanity if it progresses unabated. Others believe that AI, unlike previous technological revolutions, will create a risk of mass unemployment. In the twenty-first century, AI techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science, software engineering and operations research.",
    "Knowledge_representation_and_reasoning": "Knowledge representation and reasoning (KR\u00b2, KR&R) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build. Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning, such as the application of rules or the relations of sets and subsets. Examples of knowledge representation formalisms include semantic nets, systems architecture, frames, rules, and ontologies. Examples of automated reasoning engines include inference engines, theorem provers, and classifiers.",
    "Code": "In communications and information processing, code is system of rules to convert information\u2014such as a letter, word, sound, image, or gesture\u2014into another form, sometimes shortened or secret, for communication through a communication channel or storage in a storage medium (source?). An early example is the invention of language, which enabled a person, through speech, to communicate what they saw, heard, felt, or thought to others. But speech limits the range of communication to the distance a voice can carry, and limits the audience to those present when the speech is uttered. The invention of writing, which converted spoken language into visual symbols, extended the range of communication across space and time. The process of encoding converts information from a source into symbols for communication or storage. Decoding is the reverse process, converting code symbols back into a form that the recipient understands, such as English or/and Spanish. One reason for coding is to enable communication in places where ordinary plain language, spoken or written, is difficult or impossible. For example, semaphore, where the configuration of flags held by a signaler or the arms of a semaphore tower encodes parts of the message, typically individual letters and numbers. Another person standing a great distance away can interpret the flags and reproduce the words sent.",
    "Linguistics": "Linguistics is the scientific study of language. It involves the analysis of language form, language meaning, and language in context. Linguists traditionally analyse human language by observing an interplay between sound and meaning. Linguistics also deals with the social, cultural, historical, and political factors that influence language, through which linguistic and language-based context is often determined. Research on language through the sub-branches of historical and evolutionary linguistics also focuses on how languages change and grow, particularly over an extended period of time. The earliest activities in the documentation and description of language have been attributed to the 6th-century-BC Indian grammarian P\u0101\u1e47ini who wrote a formal description of the Sanskrit language in his A\u1e63\u1e6d\u0101dhy\u0101y\u012b. Related areas of study include the disciplines of semiotics (the study of direct and indirect language through signs and symbols), literary criticism (the historical and ideological analysis of literature, cinema, art, or published material), translation (the conversion and documentation of meaning in written/spoken text from one language or dialect onto another), and speech-language pathology (a corrective method to cure phonetic disabilities and dis-functions at the cognitive level).",
    "Computer_vision": "Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do. Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner or medical scanning device. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems. Sub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration.",
    "Stepped_reckoner": "The step reckoner (or stepped reckoner) was a digital mechanical calculator invented by the German mathematician Gottfried Wilhelm Leibniz around 1672 and completed in 1694. The name comes from the translation of the German term for its operating mechanism, Staffelwalze, meaning \"stepped drum\". It was the first calculator that could perform all four arithmetic operations. Its intricate precision gearwork, however, was somewhat beyond the fabrication technology of the time; mechanical problems, in addition to a design flaw in the carry mechanism, prevented the machines from working reliably. Two prototypes were built; today only one survives in the National Library of Lower Saxony (Nieders\u00e4chsische Landesbibliothek) in Hanover, Germany. Several later replicas are on display, such as the one at the Deutsches Museum, Munich. Despite the mechanical flaws of the stepped reckoner, it suggested possibilities to future calculator builders. The operating mechanism, invented by Leibniz, called the stepped cylinder or Leibniz wheel, was used in many calculating machines for 200 years, and into the 1970s with the Curta hand calculator.",
    "Halting_problem": "In computability theory, the halting problem is the problem of determining, from a description of an arbitrary computer program and an input, whether the program will finish running, or continue to run forever. Alan Turing proved in 1936 that a general algorithm to solve the halting problem for all possible program-input pairs cannot exist. For any program f that might determine if programs halt, a \"pathological\" program g called with an input can pass its own source and its input to f and then specifically do the opposite of what f predicts g will do. No f can exist that handles this case. A key part of the proof was a mathematical definition of a computer and program, which became known as a Turing machine; the halting problem is undecidable over Turing machines. Turing's proof is one of the first cases of decision problems to be concluded. The theoretical conclusion that it is not solvable is significant to practical computing efforts, defining a class of applications which no programming invention can possibly perform perfectly. Jack Copeland (2004) attributes the introduction of the term halting problem to the work of Martin Davis in the 1950s.",
    "Inter-process_communication": "In computer science, inter-process communication or interprocess communication (IPC) refers specifically to the mechanisms an operating system provides to allow the processes to manage shared data. Typically, applications can use IPC, categorized as clients and servers, where the client requests data and the server responds to client requests. Many applications are both clients and servers, as commonly seen in distributed computing. IPC is very important to the design process for microkernels and nanokernels, which reduce the number of functionalities provided by the kernel. Those functionalities are then obtained by communicating with servers via IPC, leading to a large increase in communication when compared to a regular monolithic kernel. IPC interfaces generally encompass variable analytic framework structures. These processes ensure compatibility between the multi-vector protocols upon which IPC models rely. An IPC mechanism is either synchronous or asynchronous. Synchronization primitives may be used to have synchronous behavior with an asynchronous IPC mechanism.",
    "Database_model": "A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized and manipulated. The most popular example of a database model is the relational model, which uses a table-based format.",
    "Ada_Lovelace": "Augusta Ada King, Countess of Lovelace (n\u00e9e Byron; 10 December 1815 \u2013 27 November 1852) was an English mathematician and writer, chiefly known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. She was the first to recognise that the machine had applications beyond pure calculation, and published the first algorithm intended to be carried out by such a machine. As a result, she is widely regarded as the first to recognise the full potential of computers and one of the first computer programmers. Augusta Byron was the only legitimate child of poet Lord Byron and his wife Lady Byron. All of Byron's other children were born out of wedlock to other women. Byron separated from his wife a month after Ada was born and left England forever four months later. He commemorated the parting in a poem that begins, \"Is thy face like thy mother's my fair child! ADA! sole daughter of my house and heart?\". He died of disease in the Greek War of Independence when Ada was eight years old. Her mother remained bitter and promoted Ada's interest in mathematics and logic in an effort to prevent her from developing her father's perceived insanity. Despite this, Ada remained interested in Byron, naming her two sons Byron and Gordon. Upon her eventual death, she was buried next to him at her request. Although often ill in her childhood, Ada pursued her studies assiduously. She married William King in 1835. King was made Earl of Lovelace in 1838, Ada thereby becoming Countess of Lovelace. Her educational and social exploits brought her into contact with scientists such as Andrew Crosse, Charles Babbage, Sir David Brewster, Charles Wheatstone, Michael Faraday and the author Charles Dickens, contacts which she used to further her education. Ada described her approach as \"poetical science\" and herself as an \"Analyst (& Metaphysician)\". When she was a teenager, her mathematical talents led her to a long working relationship and friendship with fellow British mathematician Charles Babbage, who is known as \"the father of computers\". She was in particular interested in Babbage's work on the Analytical Engine. Lovelace first met him in June 1833, through their mutual friend, and her private tutor, Mary Somerville. Between 1842 and 1843, Ada translated an article by Italian military engineer Luigi Menabrea on the calculating engine, supplementing it with an elaborate set of notes, simply called \"Notes\". Lovelace's notes are important in the early history of computers, containing what many consider to be the first computer program\u2014that is, an algorithm designed to be carried out by a machine. Other historians reject this perspective and point out that Babbage's personal notes from the years 1836/1837 contain the first programs for the engine. She also developed a vision of the capability of computers to go beyond mere calculating or number-crunching, while many others, including Babbage himself, focused only on those capabilities. Her mindset of \"poetical science\" led her to ask questions about the Analytical Engine (as shown in her notes) examining how individuals and society relate to technology as a collaborative tool. She died of uterine cancer in 1852 at the age of 36.",
    "Bernoulli_number": "In mathematics, the Bernoulli numbers Bn are a sequence of rational numbers which occur frequently in number theory. The Bernoulli numbers appear in (and can be defined by) the Taylor series expansions of the tangent and hyperbolic tangent functions, in Faulhaber's formula for the sum of m-th powers of the first n positive integers, in the Euler\u2013Maclaurin formula, and in expressions for certain values of the Riemann zeta function. The values of the first 20 Bernoulli numbers are given in the adjacent table. Two conventions are used in the literature, denoted here by  and ; they differ only for n = 1, where  and . For every odd n > 1, Bn = 0. For every even n > 0, Bn is negative if n is divisible by 4 and positive otherwise. The Bernoulli numbers are special values of the Bernoulli polynomials , with  and  (). The Bernoulli numbers were discovered around the same time by the Swiss mathematician Jacob Bernoulli, after whom they are named, and independently by Japanese mathematician Seki Takakazu. Seki's discovery was posthumously published in 1712 (, p. 891; , p. 108) in his work Katsuy\u014d Sanp\u014d; Bernoulli's, also posthumously, in his Ars Conjectandi of 1713. Ada Lovelace's note G on the Analytical Engine from 1842 describes an algorithm for generating Bernoulli numbers with Babbage's machine (, Note G). As a result, the Bernoulli numbers have the distinction of being the subject of the first published complex computer program.",
    "Punched_card": "A punched card or punch card is a piece of stiff paper that can be used to contain digital data represented by the presence or absence of holes in predefined positions. Digital data can be used for data processing applications or used to directly control automated machinery. Punched cards were widely used through much of the 20th century in the data processing industry, where specialized and increasingly complex unit record machines, organized into semiautomatic data processing systems, used punched cards for data input, output, and storage. The IBM 12-row/80-column punched card format came to dominate the industry. Many early digital computers used punched cards as the primary medium for input of both computer programs and data. While punched cards are now obsolete as a storage medium, as of 2012, some voting machines still use punched cards to record votes.",
    "Punched_tape": "Punched tape or perforated paper tape is a form of data storage that consists of a long strip of paper in which holes are punched. Now effectively obsolete, it was widely used during much of the 20th century for teleprinter communication, for input to computers of the 1950s and 1960s, and later as a storage medium for minicomputers and CNC machine tools.",
    "Computer_hardware": "Computer hardware includes the physical parts of a computer, such as the case, central processing unit (CPU), monitor, mouse, keyboard, computer data storage, graphics card, sound card, speakers and motherboard. By contrast, software is the set of instructions that can be stored and run by hardware. Hardware is so-termed because it is \"hard\" or rigid with respect to changes, whereas software is \"soft\" because it is easy to change. Hardware is typically directed by the software to execute any command or instruction. A combination of hardware and software forms a usable computing system, although other systems exist with only hardware.",
    "Real-time_computing": "Real-time computing (RTC), or reactive computing is the computer science term for hardware and software systems subject to a \"real-time constraint\", for example from event to system response. Real-time programs must guarantee response within specified time constraints, often referred to as \"deadlines\". Real-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. A system not specified as operating in real time cannot usually guarantee a response within any timeframe, although typical or expected response times may be given. Real-time processing fails if not completed within a specified deadline relative to an event; deadlines must always be met, regardless of system load. A real-time system has been described as one which \"controls an environment by receiving data, processing them, and returning the results sufficiently quickly to affect the environment at that time\". The term \"real-time\" is also used in simulation to mean that the simulation's clock runs at the same speed as a real clock, and in process control and enterprise systems to mean \"without significant delay\". Real-time software may use one or more of the following: synchronous programming languages, real-time operating systems, and real-time networks, each of which provide essential frameworks on which to build a real-time software application. Systems used for many mission critical applications must be real-time, such as for control of fly-by-wire aircraft, or anti-lock brakes, both of which demand immediate and accurate mechanical response.",
    "Rendering_(computer_graphics)": "Rendering or image synthesis is the process of generating a photorealistic or non-photorealistic image from a 2D or 3D model by means of a computer program. The resulting image is referred to as the render. Multiple models can be defined in a scene file containing objects in a strictly defined language or data structure. The scene file contains geometry, viewpoint, texture, lighting, and shading information describing the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The term \"rendering\" is analogous to the concept of an artist's impression of a scene. The term \"rendering\" is also used to describe the process of calculating effects in a video editing program to produce the final video output. Rendering is one of the major sub-topics of 3D computer graphics, and in practice it is always connected to the others. It is the last major step in the graphics pipeline, giving models and animation their final appearance. With the increasing sophistication of computer graphics since the 1970s, it has become a more distinct subject. Rendering has uses in architecture, video games, simulators, movie and TV visual effects, and design visualization, each employing a different balance of features and techniques. A wide variety of renderers are available for use. Some are integrated into larger modeling and animation packages, some are stand-alone, and some are free open-source projects. On the inside, a renderer is a carefully engineered program based on multiple disciplines, including light physics, visual perception, mathematics, and software development. Though the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image on a screen from a 3D representation stored in a scene file are handled by the graphics pipeline in a rendering device such as a GPU. A GPU is a purpose-built device that assists a CPU in performing complex rendering calculations. If a scene is to look relatively realistic and predictable under virtual lighting, the rendering software must solve the rendering equation. The rendering equation doesn't account for all lighting phenomena, but instead acts as a general lighting model for computer-generated imagery. In the case of 3D graphics, scenes can be pre-rendered or generated in realtime. Pre-rendering is a slow, computationally intensive process that is typically used for movie creation, where scenes can be generated ahead of time, while real-time rendering is often done for 3D video games and other applications that must dynamically create scenes. 3D hardware accelerators can improve realtime rendering performance.",
    "Edsger_W._Dijkstra": "Edsger Wybe Dijkstra (; Dutch: [\u02c8\u025btsx\u0259r \u02c8\u028bib\u0259 \u02c8d\u025bikstra] (); 11 May 1930 \u2013 6 August 2002) was a Dutch computer scientist, programmer, software engineer, systems scientist, science essayist, and pioneer in computing science. A theoretical physicist by training, he worked as a programmer at the Mathematisch Centrum (Amsterdam) from 1952 to 1962. A university professor for much of his life, Dijkstra held the Schlumberger Centennial Chair in Computer Sciences at the University of Texas at Austin from 1984 until his retirement in 1999. He was a professor of mathematics at the Eindhoven University of Technology (1962\u20131984) and a research fellow at the Burroughs Corporation (1973\u20131984). One of the most influential figures of computing science's founding generation, Dijkstra helped shape the new discipline from both an engineering and a theoretical perspective. His fundamental contributions cover diverse areas of computing science, including compiler construction, operating systems, distributed systems, sequential and concurrent programming, programming paradigm and methodology, programming language research, program design, program development, program verification, software engineering principles, graph algorithms, and philosophical foundations of computer programming and computer science. Many of his papers are the source of new research areas. Several concepts and problems that are now standard in computer science were first identified by Dijkstra and/or bear names coined by him. As a foremost opponent of the mechanizing view of computing science, he refuted the use of the concepts of 'computer science' and 'software engineering' as umbrella terms for academic disciplines. Until the mid-1960s computer programming was considered more an art (or a craft) than a scientific discipline. In Harlan Mills's words (1986), \"programming [before the 1970s] was regarded as a private, puzzle-solving activity of writing computer instructions to work as a program\". In the late 1960s, computer programming was in a state of crisis. Dijkstra was one of a small group of academics and industrial programmers who advocated a new programming style to improve the quality of programs. Dijkstra, who had a background in mathematics and physics, was one of the driving forces behind the acceptance of computer programming as a scientific discipline. He coined the phrase \"structured programming\" and during the 1970s this became the new programming orthodoxy. His ideas about structured programming helped lay the foundations for the birth and development of the professional discipline of software engineering, enabling programmers to organize and manage increasingly complex software projects. As Bertrand Meyer (2009) noted, \"The revolution in views of programming started by Dijkstra's iconoclasm led to a movement known as structured programming, which advocated a systematic, rational approach to program construction. Structured programming is the basis for all that has been done since in programming methodology, including object-oriented programming.\" The academic study of concurrent computing started in the 1960s, with Dijkstra (1965) credited with being the first paper in this field, identifying and solving the mutual exclusion problem. He was also one of the early pioneers of the research on principles of distributed computing. His foundational work on concurrency, semaphores, mutual exclusion, deadlock (deadly embrace), finding shortest paths in graphs, fault-tolerance, self-stabilization, among many other contributions comprises many of the pillars upon which the field of distributed computing is built. Shortly before his death in 2002, he received the ACM PODC Influential-Paper Award in distributed computing for his work on self-stabilization of program computation. This annual award was renamed the Dijkstra Prize (Edsger W. Dijkstra Prize in Distributed Computing) the following year. As the prize, sponsored jointly by the ACM Symposium on Principles of Distributed Computing (PODC) and the EATCS International Symposium on Distributed Computing (DISC), recognizes that \"No other individual has had a larger influence on research in principles of distributed computing\".",
    "Automated_theorem_proving": "Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs. Automated reasoning over mathematical proof was a major impetus for the development of computer science.",
    "Cryptography": "Cryptography, or cryptology (from Ancient Greek: \u03ba\u03c1\u03c5\u03c0\u03c4\u03cc\u03c2, romanized: krypt\u00f3s \"hidden, secret\"; and \u03b3\u03c1\u03ac\u03c6\u03b5\u03b9\u03bd graphein, \"to write\", or -\u03bb\u03bf\u03b3\u03af\u03b1 -logia, \"study\", respectively), is the practice and study of techniques for secure communication in the presence of third parties called adversaries. More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages; various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, electrical engineering, communication science, and physics. Applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications. Cryptography prior to the modern age was effectively synonymous with encryption, the conversion of information from a readable state to apparent nonsense. The originator of an encrypted message shares the decoding technique only with intended recipients to preclude access from adversaries. The cryptography literature often uses the names Alice (\"A\") for the sender, Bob (\"B\") for the intended recipient, and Eve (\"eavesdropper\") for the adversary. Since the development of rotor cipher machines in World War I and the advent of computers in World War II, the methods used to carry out cryptology have become increasingly complex and its application more widespread. Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these solutions to be continually adapted. There exist information-theoretically secure schemes that provably cannot be broken even with unlimited computing power\u2014an example is the one-time pad\u2014but these schemes are more difficult to use in practice than the best theoretically breakable but computationally secure mechanisms. The growth of cryptographic technology has raised a number of legal issues in the information age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export. In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation. Cryptography also plays a major role in digital rights management and copyright infringement of digital media.",
    "Physics": "Physics (from Ancient Greek: \u03c6\u03c5\u03c3\u03b9\u03ba\u03ae (\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7), romanized: physik\u1e17 (epist\u1e17m\u0113), lit. 'knowledge of nature', from \u03c6\u03cd\u03c3\u03b9\u03c2 ph\u00fdsis 'nature') is the natural science that studies matter, its motion and behavior through space and time, and the related entities of energy and force. Physics is one of the most fundamental scientific disciplines, and its main goal is to understand how the universe behaves. Physics is one of the oldest academic disciplines and, through its inclusion of astronomy, perhaps the oldest. Over much of the past two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the Scientific Revolution in the 17th century these natural sciences emerged as unique research endeavors in their own right. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences and suggest new avenues of research in academic disciplines such as mathematics and philosophy. Advances in physics often enable advances in new technologies. For example, advances in the understanding of electromagnetism, solid-state physics, and nuclear physics led directly to the development of new products that have dramatically transformed modern-day society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus.",
    "List_of_terms_relating_to_algorithms_and_data_structures": "The NIST Dictionary of Algorithms and Data Structures is a reference work maintained by the U.S. National Institute of Standards and Technology.It defines a large number of terms relating to algorithms and data structures. For algorithms and data structures not necessarily mentioned here, see list of algorithms and list of data structures. This list of terms was originally derived from the index of that document, and is in the public domain, as it was compiled by a Federal Government employee as part of a Federal Government work. Some of the terms defined are:",
    "Structured_programming": "Structured programming is a programming paradigm aimed at improving the clarity, quality, and development time of a computer program by making extensive use of the structured control flow constructs of selection (if/then/else) and repetition (while and for), block structures, and subroutines. It emerged in the late 1950s with the appearance of the ALGOL 58 and ALGOL 60 programming languages, with the latter including support for block structures. Contributing factors to its popularity and widespread acceptance, at first in academia and later among practitioners, include the discovery of what is now known as the structured program theorem in 1966, and the publication of the influential \"Go To Statement Considered Harmful\" open letter in 1968 by Dutch computer scientist Edsger W. Dijkstra, who coined the term \"structured programming\". Structured programming is most frequently used with deviations that allow for clearer programs in some particular cases, such as when exception handling has to be performed.",
    "Query_language": "Query languages or data query languages (DQLs) are computer languages used to make queries in databases and information systems.",
    "Error_detection_and_correction": "In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. Many communication channels are subject to channel noise, and thus errors may be introduced during transmission from the source to a receiver. Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases.",
    "Peter_Naur": "Peter Naur (25 October 1928 \u2013 3 January 2016) was a Danish computer science pioneer and Turing award winner. He is best known as a contributor, with John Backus, to the Backus\u2013Naur form (BNF) notation used in describing the syntax for most programming languages. He also contributed to creating the language ALGOL 60.",
    "Computational_cognition": "Computational cognition (sometimes referred to as computational cognitive science or computational psychology) is the study of the computational basis of learning and inference by mathematical modeling, computer simulation, and behavioral experiments. In psychology, it is an approach which develops computational models based on experimental results. It seeks to understand the basis behind the human method of processing of information. Early on computational cognitive scientists sought to bring back and create a scientific form of Brentano's psychology",
    "Numerical_analysis": "Numerical analysis is the study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). Numerical analysis naturally finds application in all fields of engineering and the physical sciences, but in the 21st century also the life sciences, social sciences, medicine, business and even the arts have adopted elements of scientific computations. The growth in computing power has revolutionized the use of realistic mathematical models in science and engineering, and subtle numerical analysis is required to implement these detailed models of the world. For example, ordinary differential equations appear in celestial mechanics (predicting the motions of planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology. Before the advent of modern computers, numerical methods often depended on hand interpolation formulas applied to data from large printed tables. Since the mid 20th century, computers calculate the required functions instead, but many of the same formulas nevertheless continue to be used as part of the software algorithms. The numerical point of view goes back to the earliest mathematical writings. A tablet from the Yale Babylonian Collection (YBC 7289), gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square. Numerical analysis continues this long tradition: rather than exact symbolic answers, which can only be applied to real-world measurements by translation into digits, it gives approximate solutions within specified error bounds.",
    "Information_technology": "Information technology (IT) is the use of computers to store, retrieve, transmit, and manipulate data or information. IT is typically used within the context of business operations as opposed to personal or entertainment technologies. IT is considered to be a subset of information and communications technology (ICT). An information technology system (IT system) is generally an information system, a communications system or, more specifically speaking, a computer system \u2013 including all hardware, software and peripheral equipment \u2013 operated by a limited group of users. Humans have been storing, retrieving, manipulating, and communicating information since the Sumerians in Mesopotamia developed writing in about 3000 BC, but the term information technology in its modern sense first appeared in a 1958 article published in the Harvard Business Review; authors Harold J. Leavitt and Thomas L. Whisler commented that \"the new technology does not yet have a single established name. We shall call it information technology (IT).\" Their definition consists of three categories: techniques for processing, the application of statistical and mathematical methods to decision-making, and the simulation of higher-order thinking through computer programs. The term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several products or services within an economy are associated with information technology, including computer hardware, software, electronics, semiconductors, internet, telecom equipment, and e-commerce. Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of IT development: pre-mechanical (3000 BC \u2013 1450 AD), mechanical (1450\u20131840), electromechanical (1840\u20131940), and electronic (1940\u2013present). This article focuses on the most recent period (electronic).",
    "Algorithmic_trading": "Algorithmic trading is a method of executing orders using automated pre-programmed trading instructions accounting for variables such as time, price, and volume. This type of trading was developed to make use of the speed and data processing advantages that computers have over human traders. Popular \"algos\" include Percentage of Volume, Pegged, VWAP, TWAP, Implementation shortfall and Target close. In the twenty-first century, algorithmic trading has been gaining traction with both retail and institutional traders. It is widely used by investment banks, pension funds, mutual funds, and hedge funds that may need to spread out the execution of a larger order or perform trades too fast for human traders to react to. A study in 2016 showed that over 80% of trading in the FOREX market was performed by trading algorithms rather than humans. The term algorithmic trading is often used synonymously with automated trading system. These encompass trading strategies such as black box trading and Quantitative, or Quant, trading that are heavily reliant on complex mathematical formulas and high-speed computer programs. Such systems run strategies including market making, inter-market spreading, arbitrage, or pure speculation such as trend following. Many fall into the category of high-frequency trading (HFT), which is characterized by high turnover and high order-to-trade ratios. HFT strategies utilize computers that make elaborate decisions to initiate orders based on information that is received electronically, before human traders are capable of processing the information they observe. As a result, in February 2012, the Commodity Futures Trading Commission (CFTC) formed a special working group that included academics and industry experts to advise the CFTC on how best to define HFT. Algorithmic trading and HFT have resulted in a dramatic change of the market microstructure, particularly in the way liquidity is provided.",
    "Corrado_B\u00f6hm": "Corrado B\u00f6hm (17 January 1923 \u2013 23 October 2017) was a Professor Emeritus at the University of Rome \"La Sapienza\" and a computer scientist known especially for his contributions to the theory of structured programming, constructive mathematics, combinatory logic, lambda calculus, and the semantics and implementation of functional programming languages.",
    "List_of_important_publications_in_computer_science": "This is a list of important publications in computer science, organized by field. Some reasons why a particular publication might be regarded as important: \n* Topic creator \u2013 A publication that created a new topic \n* Breakthrough \u2013 A publication that changed scientific knowledge significantly \n* Influence \u2013 A publication which has significantly influenced the world or has had a massive impact on the teaching of computer science.",
    "Goto": "GoTo (goto, GOTO, GO TO or other case combinations, depending on the programming language) is a statement found in many computer programming languages. It performs a one-way transfer of control to another line of code; in contrast a function call normally returns control. The jumped-to locations are usually identified using labels, though some languages use line numbers. At the machine code level, a goto is a form of branch or jump statement, in some cases combined with a stack adjustment. Many languages support the goto statement, and many do not (see ). The structured program theorem proved that the goto statement is not necessary to write programs that can be expressed as flow charts; some combination of the three programming constructs of sequence, selection/choice, and repetition/iteration are sufficient for any computation that can be performed by a Turing machine, with the caveat that code duplication and additional variables may need to be introduced. In the past there was considerable debate in academia and industry on the merits of the use of goto statements. Use of goto was formerly common, but since the advent of structured programming in the 1960s and 1970s its use has declined significantly. The primary  is that code that uses goto statements is harder to understand than alternative constructions. Goto remains in use in certain common usage patterns, but  are generally used if available. Debates over its (more limited) uses continue in academia and software industry circles.",
    "Slavic_languages": "The Slavic languages, also known as the Slavonic languages, are Indo-European languages spoken primarily by the Slavic peoples or their descendants. They are thought to descend from a proto-language called Proto-Slavic, spoken during the Early Middle Ages, which in turn is thought to have descended from the earlier Proto-Balto-Slavic language, linking the Slavic languages to the Baltic languages in a Balto-Slavic group within the Indo-European family. The Slavic languages are conventionally (that is, also on the basis of extralinguistic features) divided intro three subgroups: East, West, and South, which together constitute more than 20 languages. Of these, 10 have at least one million speakers and official status as the national languages of the countries in which they are predominantly spoken: Russian, Belarusian and Ukrainian (of the East group), Polish, Czech and Slovak (of the West group) and Bulgarian and Macedonian (eastern dialects of the South group), and Serbo-Croatian and Slovene (western dialects of the South group). The current geographic distribution of natively spoken Slavic languages includes Southern Europe, Central Europe, the Balkans, Eastern Europe, and all of the territory of Russia, which includes northern and north-central Asia (though many minority languages of Russia are also still spoken). Furthermore, the diasporas of many Slavic peoples have established isolated minorities of speakers of their languages all over the world. The number of speakers of all Slavic languages together was estimated to be 315 million at the turn of the twenty-first century.",
    "List_of_pioneers_in_computer_science": "This article presents a list of individuals who made transformative breakthroughs in the creation, development and imagining of what computers could do.",
    "Cellular_automaton": "A cellular automaton (pl. cellular automata, abbrev. CA) is a discrete model studied in automata theory. Cellular automata are also called cellular spaces, tessellation automata, homogeneous structures, cellular structures, tessellation structures, and iterative arrays. Cellular automata have found application in various areas, including physics, theoretical biology and microstructure modeling. A cellular automaton consists of a regular grid of cells, each in one of a finite number of states, such as on and off (in contrast to a coupled map lattice). The grid can be in any finite number of dimensions. For each cell, a set of cells called its neighborhood is defined relative to the specified cell. An initial state (time t = 0) is selected by assigning a state for each cell. A new generation is created (advancing t by 1), according to some fixed rule (generally, a mathematical function) that determines the new state of each cell in terms of the current state of the cell and the states of the cells in its neighborhood. Typically, the rule for updating the state of cells is the same for each cell and does not change over time, and is applied to the whole grid simultaneously, though exceptions are known, such as the stochastic cellular automaton and asynchronous cellular automaton. The concept was originally discovered in the 1940s by Stanislaw Ulam and John von Neumann while they were contemporaries at Los Alamos National Laboratory. While studied by some throughout the 1950s and 1960s, it was not until the 1970s and Conway's Game of Life, a two-dimensional cellular automaton, that interest in the subject expanded beyond academia. In the 1980s, Stephen Wolfram engaged in a systematic study of one-dimensional cellular automata, or what he calls elementary cellular automata; his research assistant Matthew Cook showed that one of these rules is Turing-complete. Wolfram published A New Kind of Science in 2002, claiming that cellular automata have applications in many fields of science. These include computer processors and cryptography. The primary classifications of cellular automata, as outlined by Wolfram, are numbered one to four. They are, in order, automata in which patterns generally stabilize into homogeneity, automata in which patterns evolve into mostly stable or oscillating structures, automata in which patterns evolve in a seemingly chaotic fashion, and automata in which patterns become extremely complex and may last for a long time, with stable local structures. This last class are thought to be computationally universal, or capable of simulating a Turing machine. Special types of cellular automata are reversible, where only a single configuration leads directly to a subsequent one, and totalistic, in which the future value of individual cells only depends on the total value of a group of neighboring cells. Cellular automata can simulate a variety of real-world systems, including biological and chemical ones.",
    "Programming_language_theory": "Programming language theory (PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, linguistics and even cognitive science. It is a well-recognized branch of computer science, and an active research area, with results published in numerous journals dedicated to PLT, as well as in general computer science and engineering publications.",
    "Mathematical_logic": "Mathematical logic is a subfield of mathematics exploring the applications of formal logic to mathematics. It bears close connections to metamathematics, the foundations of mathematics, and theoretical computer science. The unifying themes in mathematical logic include the study of the expressive power of formal systems and the deductive power of formal proof systems. Mathematical logic is often divided into the fields of set theory, model theory, recursion theory, and proof theory. These areas share basic results on logic, particularly first-order logic, and definability. In computer science (particularly in the ACM Classification) mathematical logic encompasses additional topics not detailed in this article; see Logic in computer science for those. Since its inception, mathematical logic has both contributed to, and has been motivated by, the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt G\u00f6del, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.",
    "Manchester_computers": "The Manchester computers were an innovative series of stored-program electronic computers developed during the 30-year period between 1947 and 1977 by a small team at the University of Manchester, under the leadership of Tom Kilburn. They included the world's first stored-program computer, the world's first transistorised computer, and what was the world's fastest computer at the time of its inauguration in 1962. The project began with two aims: to prove the practicality of the Williams tube, an early form of computer memory based on standard cathode ray tubes (CRTs); and to construct a machine that could be used to investigate how computers might be able to assist in the solution of mathematical problems. The first of the series, the Manchester Baby, ran its first program on 21 June 1948. As the world's first stored-program computer, the Baby, and the Manchester Mark 1 developed from it, quickly attracted the attention of the United Kingdom government, who contracted the electrical engineering firm of Ferranti to produce a commercial version. The resulting machine, the Ferranti Mark 1, was the world's first commercially available general-purpose computer. The collaboration with Ferranti eventually led to an industrial partnership with the computer company ICL, who made use of many of the ideas developed at the university, particularly in the design of their 2900 series of computers during the 1970s.",
    "Category_theory": "Category theory formalizes mathematical structure and its concepts in terms of a labeled directed graph called a category, whose nodes are called objects, and whose labelled directed edges are called arrows (or morphisms). A category has two basic properties: the ability to compose the arrows associatively, and the existence of an identity arrow for each object. The language of category theory has been used to formalize concepts of other high-level abstractions such as sets, rings, and groups. Informally, category theory is a general theory of functions. Several terms used in category theory, including the term \"morphism\", are used differently from their uses in the rest of mathematics. In category theory, morphisms obey conditions specific to category theory itself. Samuel Eilenberg and Saunders Mac Lane introduced the concepts of categories, functors, and natural transformations in 1942\u201345 in their study of algebraic topology, with the goal of understanding the processes that preserve mathematical structure. Category theory has practical applications in programming language theory, for example the usage of monads in functional programming. It may also be used as an axiomatic foundation for mathematics, as an alternative to set theory and other proposed foundations.",
    "Communications_of_the_ACM": "Communications of the ACM is the monthly journal of the Association for Computing Machinery (ACM). It was established in 1958, with Saul Rosen as its first managing editor. It is sent to all ACM members.Articles are intended for readers with backgrounds in all areas of computer science and information systems. The focus is on the practical implications of advances in information technology and associated management issues; ACM also publishes a variety of more theoretical journals. The magazine straddles the boundary of a science magazine, trade magazine, and a scientific journal. While the content is subject to peer review, the articles published are often summaries of research that may also be published elsewhere. Material published must be accessible and relevant to a broad readership. From 1960 onward, CACM also published algorithms, expressed in ALGOL. The collection of algorithms later became known as the Collected Algorithms of the ACM.",
    "Information_theory": "Information theory studies the quantification, storage, and communication of information. It was originally proposed by Claude Shannon in 1948 to find fundamental limits on signal processing and communication operations such as data compression, in a landmark paper titled \"A Mathematical Theory of Communication\". Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the Internet, the study of linguistics and of human perception, the understanding of black holes, and numerous other fields. The field is at the intersection of mathematics, statistics, computer science, physics, neurobiology, information engineering, and electrical engineering. The theory has also found applications in other areas, including statistical inference, natural language processing, cryptography, neurobiology, human vision, the evolution and function of molecular codes (bioinformatics), model selection in statistics, thermal physics, quantum computing, linguistics, plagiarism detection, pattern recognition, and anomaly detection. Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory, information-theoretic security, Grey system theory and measures of information. Applications of fundamental topics of information theory include lossless data compression (e.g. ZIP files), lossy data compression (e.g. MP3s and JPEGs), and channel coding (e.g. for DSL). Information theory is used in information retrieval, intelligence gathering, gambling, and even in musical composition. A key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a die (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy.",
    "Earth_science": "Earth science or geoscience includes all fields of natural science related to the planet Earth. This is a branch of science dealing with the physical and chemical constitution of the Earth and its atmosphere. Earth science can be considered to be a branch of planetary science, but with a much older history. Earth science encompasses four main branches of study, the lithosphere, the hydrosphere, the atmosphere, and the biosphere, each of which is further broken down into more specialized fields. There are both reductionist and holistic approaches to Earth sciences. It is also the study of Earth and its neighbors in space. Some Earth scientists use their knowledge of the planet to locate and develop energy and mineral resources. Others study the impact of human activity on Earth's environment, and design methods to protect the planet. Some use their knowledge about earth processes such as volcanoes, earthquakes, and hurricanes to plan communities that will not expose people to these dangerous events. The Earth sciences can include the study of geology, the lithosphere, and the large-scale structure of the Earth's interior, as well as the atmosphere, hydrosphere, and biosphere. Typically, Earth scientists use tools from geology, chronology, physics, chemistry, geography, biology, and mathematics to build a quantitative understanding of how the Earth works and evolves. Earth science affects our everyday lives. For example, meteorologists study the weather and watch for dangerous storms. Hydrologists study water and warn of floods. Seismologists study earthquakes and try to understand where they will strike. Geologists study rocks and help to locate useful minerals. Earth scientists often work in the field\u2014perhaps climbing mountains, exploring the seabed, crawling through caves, or wading in swamps. They measure and collect samples (such as rocks or river water), then they record their findings on charts and maps.",
    "Very_Large_Scale_Integration": "Very large-scale integration (VLSI) is the process of creating an integrated circuit (IC) by combining millions of MOS transistors onto a single chip. VLSI began in the 1970s when MOS integrated circuit chips were widely adopted, enabling complex semiconductor and telecommunication technologies to be developed. The microprocessor and memory chips are VLSI devices. Before the introduction of VLSI technology, most ICs had a limited set of functions they could perform. An electronic circuit might consist of a CPU, ROM, RAM and other glue logic. VLSI lets IC designers add all of these into one chip.",
    "Complex_system": "A complex system is a system composed of many components which may interact with each other. Examples of complex systems are Earth's global climate, organisms, the human brain, infrastructure such as power grid, transportation or communication systems, social and economic organizations (like cities), an ecosystem, a living cell, and ultimately the entire universe. Complex systems are systems whose behavior is intrinsically difficult to model due to the dependencies, competitions, relationships, or other types of interactions between their parts or between a given system and its environment. Systems that are \"complex\" have distinct properties that arise from these relationships, such as nonlinearity, emergence, spontaneous order, adaptation, and feedback loops, among others. Because such systems appear in a wide variety of fields, the commonalities among them have become the topic of their independent area of research. In many cases, it is useful to represent such a system as a network where the nodes represent the components and links to their interactions.",
    "Supercomputer": "A supercomputer is a computer with a high level of performance as compared to a general-purpose computer. The performance of a supercomputer is commonly measured in floating-point operations per second (FLOPS) instead of million instructions per second (MIPS). Since 2017, there are supercomputers which can perform over a hundred quadrillion FLOPS (100 petaFLOPS, or PFLOPS). Since November 2017, all of the world's fastest 500 supercomputers run Linux-based operating systems. Additional research is being conducted in China, the United States, the European Union, Taiwan and Japan to build faster, more powerful and technologically superior exascale supercomputers. Supercomputers play an important role in the field of computational science, and are used for a wide range of computationally intensive tasks in various fields, including quantum mechanics, weather forecasting, climate research, oil and gas exploration, molecular modeling (computing the structures and properties of chemical compounds, biological macromolecules, polymers, and crystals), and physical simulations (such as simulations of the early moments of the universe, airplane and spacecraft aerodynamics, the detonation of nuclear weapons, and nuclear fusion). They have been essential in the field of cryptanalysis. Supercomputers were introduced in the 1960s, and for several decades the fastest were made by Seymour Cray at Control Data Corporation (CDC), Cray Research and subsequent companies bearing his name or monogram. The first such machines were highly tuned conventional designs that ran faster than their more general-purpose contemporaries. Through the decade, increasing amounts of parallelism were added, with one to four processors being typical. From the 1970s, vector processors operating on large arrays of data came to dominate. A notable example is the highly successful Cray-1 of 1976. Vector computers remained the dominant design into the 1990s. From then until today, massively parallel supercomputers with tens of thousands of off-the-shelf processors became the norm. The US has long been the leader in the supercomputer field, first through Cray's almost uninterrupted dominance of the field, and later through a variety of technology companies. Japan made major strides in the field in the 1980s and 90s, with China becoming increasingly active in the field. As of June 2020, the fastest supercomputer on the TOP500 supercomputer list is Fugaku, in Japan, with a LINPACK benchmark score of 415 PFLOPS, followed by Summit, by around 266.7 PFLOPS. The US has four of the top 10; China and Italy have two each, Switzerland has one. In June 2018, all combined supercomputers on the list broke the 1 exaFLOPS mark.",
    "BBC_Micro": "The British Broadcasting Corporation Microcomputer System, or BBC Micro, is a series of microcomputers and associated peripherals designed and built by the Acorn Computer company in the 1980s for the BBC Computer Literacy Project, operated by the British Broadcasting Corporation. Designed with an emphasis on education, it was notable for its ruggedness, expandability, and the quality of its operating system. An accompanying 1982 television series, The Computer Programme, featuring Chris Serle learning to use the machine, was broadcast on BBC2. After the Literacy Project's call for bids for a computer to accompany the TV programmes and literature, Acorn won the contract with the Proton, a successor of its Atom computer prototyped at short notice. Renamed the BBC Micro, the system was adopted by most schools in the United Kingdom, changing Acorn's fortunes. It was also successful as a home computer in the UK, despite its high cost. Acorn also employed the machine to simulate and develop the ARM architecture which, many years later, has become hugely successful for embedded systems, including tablets and mobile phones. In 2013 ARM was the most widely used 32-bit instruction set architecture. While nine models were eventually produced with the BBC brand, the phrase \"BBC Micro\" is usually used colloquially to refer to the first six (Model A, B, B+64, B+128, Master 128, and Master Compact), excluding the Acorn Electron; subsequent BBC models are considered part of Acorn's Archimedes series.",
    "Applied_science": "Applied science is the application of existing scientific knowledge to practical applications, like technology or inventions. Within natural science, disciplines that are basic science develop basic information to predict and perhaps explain and understand phenomena in the natural world. Applied science is the use of scientific processes and knowledge as the means to achieve a particular practical or useful result. This includes a broad range of applied science related fields, including engineering and medicine. Applied science can also apply formal science, such as statistics and probability theory, as in epidemiology. Genetic epidemiology is an applied science applying both biological and statistical methods.",
    "Howard_H._Aiken": "Howard Hathaway Aiken (March 8, 1900 \u2013 March 14, 1973) was an American physicist and a pioneer in computing, being the original conceptual designer behind IBM's Harvard Mark I computer.",
    "Coding_theory": "Coding theory is the study of the properties of codes and their respective fitness for specific applications. Codes are used for data compression, cryptography, error detection and correction, data transmission and data storage. Codes are studied by various scientific disciplines\u2014such as information theory, electrical engineering, mathematics, linguistics, and computer science\u2014for the purpose of designing efficient and reliable data transmission methods. This typically involves the removal of redundancy and the correction or detection of errors in the transmitted data. There are four types of coding: 1.  \n* Data compression (or source coding) 2.  \n* Error control (or channel coding) 3.  \n* Cryptographic coding 4.  \n* Line coding Data compression attempts to remove redundancy from the data from a source in order to transmit it more efficiently. For example, Zip data compression makes data files smaller, for purposes such as to reduce Internet traffic. Data compression and error correction may be studied in combination. Error correction adds extra data bits to make the transmission of data more robust to disturbances present on the transmission channel. The ordinary user may not be aware of many applications using error correction. A typical music CD uses the Reed-Solomon code to correct for scratches and dust. In this application the transmission channel is the CD itself. Cell phones also use coding techniques to correct for the fading and noise of high frequency radio transmission. Data modems, telephone transmissions, and the NASA Deep Space Network all employ channel coding techniques to get the bits through, for example the turbo code and LDPC codes.",
    "Simulation": "A simulation is an approximate imitation of the operation of a process or system; that represents its operation over time. Simulation is used in many contexts, such as simulation of technology for performance tuning or optimizing, safety engineering, testing, training, education, and video games. Often, computer experiments are used to study simulation models. Simulation is also used with scientific modelling of natural systems or human systems to gain insight into their functioning, as in economics. Simulation can be used to show the eventual real effects of alternative conditions and courses of action. Simulation is also used when the real system cannot be engaged, because it may not be accessible, or it may be dangerous or unacceptable to engage, or it is being designed but not yet built, or it may simply not exist. Key issues in simulation include the acquisition of valid sources of information about the relevant selection of key characteristics and behaviors, the use of simplifying approximations and assumptions within the simulation, and fidelity and validity of the simulation outcomes. Procedures and protocols for model verification and validation are an ongoing field of academic study, refinement, research and development in simulations technology or practice, particularly in the work of computer simulation.",
    "Logic": "Logic (from Greek: \u03bb\u03bf\u03b3\u03b9\u03ba\u03ae, logik\u1e17, 'possessed of reason, intellectual, dialectical, argumentative') is the systematic study of valid rules of inference, i.e. the relations that lead to the acceptance of one proposition (the conclusion) on the basis of a set of other propositions (premises). More broadly, logic is the analysis and appraisal of arguments. There is no universal agreement as to the exact definition and boundaries of logic, hence the issue still remains one of the main subjects of research and debates in the field of philosophy of logic (see ). However, it has traditionally included the classification of arguments; the systematic exposition of the logical forms; the validity and soundness of deductive reasoning; the strength of inductive reasoning; the study of formal proofs and inference (including paradoxes and fallacies); and the study of syntax and semantics. A good argument not only possesses validity and soundness (or strength, in induction), but it also avoids circular dependencies, is clearly stated, relevant, and consistent; otherwise it is useless for reasoning and persuasion, and is classified as a fallacy. In ordinary discourse, inferences may be signified by words such as therefore, thus, hence, ergo, and so on. Historically, logic has been studied in philosophy (since ancient times) and mathematics (since the mid-19th century). More recently, logic has been studied in cognitive science, which draws on computer science, linguistics, philosophy and psychology, among other disciplines. A logician is any person, often a philosopher or mathematician, whose topic of scholarly study is logic.",
    "Tabulating_machine": "The tabulating machine was an electromechanical machine designed to assist in summarizing information stored on punched cards. Invented by Herman Hollerith, the machine was developed to help process data for the 1890 U.S. Census. Later models were widely used for business applications such as accounting and inventory control. It spawned a class of machines, known as unit record equipment, and the data processing industry. The term \"Super Computing\" was used by the New York World newspaper in 1931 to refer to a large custom-built tabulator that IBM made for Columbia University.",
    "Mechanical_calculator": "A mechanical calculator, or calculating machine, is a mechanical device used to perform the basic operations of arithmetic automatically. Most mechanical calculators were comparable in size to small desktop computers and have been rendered obsolete by the advent of the electronic calculator. Surviving notes from Wilhelm Schickard in 1623 reveal that he designed and had built the earliest of the modern attempts at mechanizing calculation. His machine was composed of two sets of technologies: first an abacus made of Napier's bones, to simplify multiplications and divisions first described six years earlier in 1617, and for the mechanical part, it had a dialed pedometer to perform additions and subtractions. A study of the surviving notes shows a machine that would have jammed after a few entries on the same dial, and that it could be damaged if a carry had to be propagated over a few digits (like adding 1 to 999). Schickard abandoned his project in 1624 and never mentioned it again until his death 11 years later in 1635. Two decades after Schickard's supposedly failed attempt, in 1642, Blaise Pascal decisively solved these particular problems with his invention of the mechanical calculator. Co-opted into his father's labour as tax collector in Rouen, Pascal designed the calculator to help in the large amount of tedious arithmetic required; it was called Pascal's Calculator or Pascaline. Thomas' arithmometer, the first commercially successful machine, was manufactured two hundred years later in 1851; it was the first mechanical calculator strong enough and reliable enough to be used daily in an office environment. For forty years the arithmometer was the only type of mechanical calculator available for sale. The comptometer, introduced in 1887, was the first machine to use a keyboard which consisted of columns of nine keys (from 1 to 9) for each digit. The Dalton adding machine, manufactured from 1902, was the first to have a 10 key keyboard. Electric motors were used on some mechanical calculators from 1901. In 1961, a comptometer type machine, the Anita mk7 from Sumlock comptometer Ltd., became the first desktop mechanical calculator to receive an all electronic calculator engine, creating the link in between these two industries and marking the beginning of its decline. The production of mechanical calculators came to a stop in the middle of the 1970s closing an industry that had lasted for 120 years. Charles Babbage designed two new kinds of mechanical calculators, which were so big that they required the power of a steam engine to operate, and that were too sophisticated to be built in his lifetime. The first one was an automatic mechanical calculator, his difference engine, which could automatically compute and print mathematical tables. In 1855, Georg Scheutz became the first of a handful of designers to succeed at building a smaller and simpler model of his difference engine. The second one was a programmable mechanical calculator, his analytical engine, which Babbage started to design in 1834; \"in less than two years he had sketched out many of the salient features of the modern computer. A crucial step was the adoption of a punched card system derived from the Jacquard loom\" making it infinitely programmable. In 1937, Howard Aiken convinced IBM to design and build the ASCC/Mark I, the first machine of its kind, based on the architecture of the analytical engine; when the machine was finished some hailed it as \"Babbage's dream come true\".",
    "Algebra": "Algebra (from Arabic: \u0627\u0644\u062c\u0628\u0631\u200e al-jabr, meaning \"reunion of broken parts\" and \"bonesetting\") is one of the broad parts of mathematics, together with number theory, geometry and analysis. In its most general form, algebra is the study of mathematical symbols and the rules for manipulating these symbols; it is a unifying thread of almost all of mathematics. It includes everything from elementary equation solving to the study of abstractions such as groups, rings, and fields. The more basic parts of algebra are called elementary algebra; the more abstract parts are called abstract algebra or modern algebra. Elementary algebra is generally considered to be essential for any study of mathematics, science, or engineering, as well as such applications as medicine and economics. Abstract algebra is a major area in advanced mathematics, studied primarily by professional mathematicians. Elementary algebra differs from arithmetic in the use of abstractions, such as using letters to stand for numbers that are either unknown or allowed to take on many values. For example, in  the letter  is unknown, but applying additive inverses can reveal its value: . In E = mc2, the letters  and  are variables, and the letter  is a constant, the speed of light in a vacuum. Algebra gives methods for writing formulas and solving equations that are much clearer and easier than the older method of writing everything out in words. The word algebra is also used in certain specialized ways. A special kind of mathematical object in abstract algebra is called an \"algebra\", and the word is used, for example, in the phrases linear algebra and algebraic topology. A mathematician who does research in algebra is called an algebraist.",
    "George_Boole": "George Boole (; 2 November 1815 \u2013 8 December 1864) was a largely self-taught English mathematician, philosopher and logician, most of whose short career was spent as the first professor of mathematics at Queen's College, Cork in Ireland. He worked in the fields of differential equations and algebraic logic, and is best known as the author of The Laws of Thought (1854) which contains Boolean algebra. Boolean logic is credited with laying the foundations for the information age. Boole maintained that: No general method for the solution of questions in the theory of probabilities can be established which does not explicitly recognise, not only the special numerical bases of the science, but also those universal laws of thought which are the basis of all reasoning, and which, whatever they may be as to their essence, are at least mathematical as to their form.",
    "William_Shockley": "William Bradford Shockley Jr. (February 13, 1910 \u2013 August 12, 1989) was an American physicist and inventor. Shockley was the manager of a research group at Bell Labs that included John Bardeen and Walter Brattain. The three scientists were jointly awarded the 1956 Nobel Prize in Physics for \"their researches on semiconductors and their discovery of the transistor effect\". Partly as a result of Shockley's attempts to commercialize a new transistor design in the 1950s and 1960s, California's \"Silicon Valley\" became a hotbed of electronics innovation. In his later life, Shockley was a professor of electrical engineering at Stanford University and became a proponent of eugenics. A 2019 study found him to be the second most (behind Arthur Jensen) controversial intelligence researcher among 55 persons covered.",
    "Process_calculus": "In computer science, the process calculi (or process algebras) are a diverse family of related approaches for formally modelling concurrent systems. Process calculi provide a tool for the high-level description of interactions, communications, and synchronizations between a collection of independent agents or processes. They also provide algebraic laws that allow process descriptions to be manipulated and analyzed, and permit formal reasoning about equivalences between processes (e.g., using bisimulation). Leading examples of process calculi include CSP, CCS, ACP, and LOTOS. More recent additions to the family include the \u03c0-calculus, the ambient calculus, PEPA, the  and the join-calculus.",
    "Computer_engineering": "Computer Engineering (CpE) is a branch of engineering that integrates several fields of computer science and electronic engineering required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration instead of only software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microcontrollers, microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering not only focuses on how computer systems themselves work but also how they integrate into the larger picture. Usual tasks involving computer engineers include writing software and firmware for embedded microcontrollers, designing VLSI chips, designing analog sensors, designing mixed signal circuit boards, and designing operating systems. Computer engineers are also suited for robotics research, which relies heavily on using digital systems to control and monitor electrical systems like motors, communications, and sensors. In many institutions of higher learning, computer engineering students are allowed to choose areas of in-depth study in their junior and senior year because the full breadth of knowledge used in the design and application of computers is beyond the scope of an undergraduate degree. Other institutions may require engineering students to complete one or two years of general engineering before declaring computer engineering as their primary focus.",
    "Harvard_Mark_I": "The IBM Automatic Sequence Controlled Calculator (ASCC), called Mark I by Harvard University\u2019s staff, was a general purpose electromechanical computer that was used in the war effort during the last part of World War II. One of the first programs to run on the Mark I was initiated on 29 March 1944 by John von Neumann. At that time, von Neumann was working on the Manhattan Project, and needed to determine whether implosion was a viable choice to detonate the atomic bomb that would be used a year later. The Mark I also computed and printed mathematical tables, which had been the initial goal of British inventor Charles Babbage for his \"analytical engine\". The Mark I was disassembled in 1959, but portions of it are displayed in the Science Center as part of the Harvard Collection of Historical Scientific Instruments. Other sections of the original machine were transferred to IBM and the Smithsonian Institution.",
    "Computational_science": "Computational science, also known as scientific computing or scientific computation (SC), is a rapidly growing branch of applied computer science and mathematics that uses advanced computing capabilities to understand and solve complex problems. It is an area of science which spans many disciplines, but at its core, it involves the development of models and simulations to understand natural systems. \n* Algorithms (numerical and non-numerical): mathematical models, computational models, and computer simulations developed to solve science (e.g., biological, physical, and social), engineering, and humanities problems \n* Computer hardware that develops and optimizes the advanced system hardware, firmware, networking, and data management components needed to solve computationally demanding problems \n* The computing infrastructure that supports both the science and engineering problem solving and the developmental computer and information science In practical use, it is typically the application of computer simulation and other forms of computation from numerical analysis and theoretical computer science to solve problems in various scientific disciplines. The field is different from theory and laboratory experiment which are the traditional forms of science and engineering. The scientific computing approach is to gain understanding, mainly through the analysis of mathematical models implemented on computers. Scientists and engineers develop computer programs, application software, that model systems being studied and run these programs with various sets of input parameters. The essence of computational science is the application of numerical algorithms and/or computational mathematics. In some cases, these models require massive amounts of calculations (usually floating-point) and are often executed on supercomputers or distributed computing platforms. Actually the science which deals with the Computer Modeling and Simulation of any physical objects and phenomena by high programming language and software and hardware is known as Computer Simulation.",
    "Natural_science": "Natural science is a branch of science concerned with the description, prediction, and understanding of natural phenomena, based on empirical evidence from observation and experimentation. Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances. Natural science can be divided into two main branches: life science and physical science. Life science is alternatively known as biology, and physical science is subdivided into branches: physics, chemistry, astronomy and Earth science. These branches of natural science may be further divided into more specialized branches (also known as fields). As empirical sciences, natural sciences use tools from the formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the \"laws of nature\". Modern natural science succeeded more classical approaches to natural philosophy, usually traced to ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science. Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on. Today, \"natural history\" suggests observational descriptions aimed at popular audiences.",
    "Alonzo_Church": "Alonzo Church (June 14, 1903 \u2013 August 11, 1995) was an American mathematician and logician who made major contributions to mathematical logic and the foundations of theoretical computer science. He is best known for the lambda calculus, Church\u2013Turing thesis, proving the unsolvability of the Entscheidungsproblem, Frege\u2013Church ontology, and the Church\u2013Rosser theorem. He also worked on philosophy of language (see e.g. Church 1970).",
    "Petri_net": "A Petri net, also known as a place/transition (PT) net, is one of several mathematical modeling languages for the description of distributed systems. It is a class of discrete event dynamic system. A Petri net is a directed bipartite graph, in which the nodes represent transitions (i.e. events that may occur, represented by bars) and places (i.e. conditions, represented by circles). The directed arcs describe which places are pre- and/or postconditions for which transitions (signified by arrows). Some sources state that Petri nets were invented in August 1939 by Carl Adam Petri\u2014at the age of 13\u2014for the purpose of describing chemical processes. Like industry standards such as UML activity diagrams, Business Process Model and Notation and event-driven process chains, Petri nets offer a graphical notation for stepwise processes that include choice, iteration, and concurrent execution. Unlike these standards, Petri nets have an exact mathematical definition of their execution semantics, with a well-developed mathematical theory for process analysis.",
    "Ubiquitous_computing": "Ubiquitous computing (or \"ubicomp\") is a concept in software engineering and computer science where computing is made to appear anytime and everywhere. In contrast to desktop computing, ubiquitous computing can occur using any device, in any location, and in any format. A user interacts with the computer, which can exist in many different forms, including laptop computers, tablets and terminals in everyday objects such as a refrigerator or a pair of glasses. The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, computer networks, mobile protocols, location and positioning, and new materials. This paradigm is also described as pervasive computing, ambient intelligence, or \"everyware\". Each term emphasizes slightly different aspects. When primarily concerning the objects involved, it is also known as physical computing, the Internet of Things, haptic computing, and \"things that think\".Rather than propose a single definition for ubiquitous computing and for these related terms, a taxonomy of properties for ubiquitous computing has been proposed, from which different kinds or flavors of ubiquitous systems and applications can be described. Ubiquitous computing touches on distributed computing, mobile computing, location computing, mobile networking, sensor networks, human\u2013computer interaction, context-aware smart home technologies, and artificial intelligence.",
    "Computer_architecture": "In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation. In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation.",
    "IEEE_Computer_Society": "IEEE Computer Society (sometimes abbreviated Computer Society or CS) is a professional society of the Institute of Electrical and Electronics Engineers (IEEE). Its purpose and scope is \"to advance the theory, practice, and application of computer and information processing science and technology\" and the \"professional standing of its members.\" The CS is the largest of 39 technical societies organized under the IEEE Technical Activities Board. The IEEE Computer Society sponsors workshops and conferences, publishes a variety of peer-reviewed literature, operates technical committees, and develops IEEE computing standards. It supports more than 200 chapters worldwide and participates in educational activities at all levels of the profession, including distance learning, accreditation of higher education programs in computer science, and professional certification in software engineering. The IEEE Computer Society is also a member organization of the Federation of Enterprise Architecture Professional Organizations (a worldwide association of professional organizations which have come together to provide a forum to standardize, professionalize, and otherwise advance the discipline of Enterprise Architecture).",
    "Peter_Wegner": "Peter A. Wegner (August 20, 1932 \u2013 July 27, 2017) was a computer scientist who made significant contributions to both the theory of object-oriented programming during the 1980s and to the relevance of the Church\u2013Turing thesis for empirical aspects of computer science during the 1990s and present. In 2016, Wegner wrote a brief autobiography for Conduit, the annual Brown University Computer Science department magazine.",
    "Functional_programming": "In computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions. It is a declarative programming paradigm in which function definitions are trees of expressions that each return a value, rather than a sequence of imperative statements which change the state of the program. In functional programming, functions are treated as first-class citizens, meaning that they can be bound to names (including local identifiers), passed as arguments, and returned from other functions, just as any other data type can. This allows programs to be written in a declarative and composable style, where small functions are combined in a modular manner. Functional programming is sometimes treated as synonymous with purely functional programming, a subset of functional programming which treats all functions as deterministic mathematical functions, or pure functions. When a pure function is called with some given arguments, it will always return the same result, and cannot be affected by any mutable state or other side effects. This is in contrast with impure procedures, common in imperative programming, which can have side effects (such as modifying the program's state or taking input from a user). Proponents of purely functional programming claim that by restricting side effects, programs can have fewer bugs, be easier to debug and test, and be more suited to formal verification. Functional programming has its roots in academia, evolving from the lambda calculus, a formal system of computation based only on functions. Functional programming has historically been less popular than imperative programming, but many functional languages are seeing use today in industry and education, including Common Lisp, Scheme, Clojure, Wolfram Language, Racket, Erlang, OCaml, Haskell, and F#. Functional programming is also key to some languages that have found success in specific domains, like R in statistics, J, K and Q in financial analysis, and XQuery/XSLT for XML. Domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, such as not allowing mutable values. In addition, many other programming languages support programming in a functional style or have implemented features from functional programming, such as C++11, Kotlin, Perl, PHP, Python, and Scala.",
    "Integrated_circuit": "An integrated circuit or monolithic integrated circuit (also referred to as an IC, a chip, or a microchip) is a set of electronic circuits on one small flat piece (or \"chip\") of semiconductor material that is normally silicon. The integration of large numbers of tiny MOS transistors into a small chip results in circuits that are orders of magnitude smaller, faster, and less expensive than those constructed of discrete electronic components. The IC's mass production capability, reliability, and building-block approach to integrated circuit design has ensured the rapid adoption of standardized ICs in place of designs using discrete transistors. ICs are now used in virtually all electronic equipment and have revolutionized the world of electronics. Computers, mobile phones, and other digital home appliances are now inextricable parts of the structure of modern societies, made possible by the small size and low cost of ICs. Integrated circuits were made practical by technological advancements in metal\u2013oxide\u2013silicon (MOS) semiconductor device fabrication. Since their origins in the 1960s, the size, speed, and capacity of chips have progressed enormously, driven by technical advances that fit more and more MOS transistors on chips of the same size \u2013 a modern chip may have many billions of MOS transistors in an area the size of a human fingernail. These advances, roughly following Moore's law, make computer chips of today possess millions of times the capacity and thousands of times the speed of the computer chips of the early 1970s. ICs have two main advantages over discrete circuits: cost and performance. Cost is low because the chips, with all their components, are printed as a unit by photolithography rather than being constructed one transistor at a time. Furthermore, packaged ICs use much less material than discrete circuits. Performance is high because the IC's components switch quickly and consume comparatively little power because of their small size and proximity. The main disadvantage of ICs is the high cost to design them and fabricate the required photomasks. This high initial cost means ICs are only commercially viable when high production volumes are anticipated.",
    "John_Bardeen": "John Bardeen (; May 23, 1908 \u2013 January 30, 1991) was an American physicist. He is the only person to be awarded the Nobel Prize in Physics twice: first in 1956 with William Shockley and Walter Brattain for the invention of the transistor; and again in 1972 with Leon N Cooper and John Robert Schrieffer for a fundamental theory of conventional superconductivity known as the BCS theory. The transistor revolutionized the electronics industry, making possible the development of almost every modern electronic device, from telephones to computers, and ushering in the Information Age. Bardeen's developments in superconductivity\u2014for which he was awarded his second Nobel Prize\u2014are used in nuclear magnetic resonance spectroscopy (NMR) and medical magnetic resonance imaging (MRI). In 1990, Bardeen appeared on LIFE Magazine's list of \"100 Most Influential Americans of the Century.\"",
    "Swarm_intelligence": "Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems. SI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the emergence of \"intelligent\" global behavior, unknown to the individual agents. Examples of swarm intelligence in natural systems include ant colonies, bird flocking, hawks hunting, animal herding, bacterial growth, fish schooling and microbial intelligence. The application of swarm principles to robots is called swarm robotics, while 'swarm intelligence' refers to the more general set of algorithms. 'Swarm prediction' has been used in the context of forecasting problems. Similar approaches to those proposed for swarm robotics are considered for genetically modified organisms in synthetic collective intelligence.",
    "Omnipresence": "Omnipresence or ubiquity is the property of being present anywhere and everywhere. The term omnipresence is most often used in a religious context as an attribute of a deity or supreme being, while the term ubiquity is generally used to describe something \"existing or being everywhere at the same time, constantly encountered, widespread, common\". Ubiquitous can also be used as a synonym for words like worldwide, universal, global, pervasive, all over the place. Being Omnipresent relates to the fact of having the presence everywhere and IF God exists, he can be called as Omnipresent. The omnipresence of a supreme being is conceived differently by different religious systems. In monotheistic beliefs like Christianity, Judaism, and Islam the divine and the universe are separate, but the divine is present everywhere. In pantheistic beliefs the divine and the universe are identical. In panentheistic beliefs the divine interpenetrates the universe, but extends beyond it in time and space.",
    "Data_transmission": "Data transmission (also data communication or digital communications) is the transfer of data (a digital bitstream or a digitized analog signal) over a point-to-point or point-to-multipoint communication channel. Examples of such channels are copper wires, optical fibers, wireless communication channels, storage media and computer buses. The data are represented as an electromagnetic signal, such as an electrical voltage, radiowave, microwave, or infrared signal. Analog or analogue transmission is a transmission method of conveying voice, data, image, signal or video information using a continuous signal which varies in amplitude, phase, or some other property in proportion to that of a variable. The messages are either represented by a sequence of pulses by means of a line code (baseband transmission), or by a limited set of continuously varying waveforms (passband transmission), using a digital modulation method. The passband modulation and corresponding demodulation (also known as detection) is carried out by modem equipment. According to the most common definition of digital signal, both baseband and passband signals representing bit-streams are considered as digital transmission, while an alternative definition only considers the baseband signal as digital, and passband transmission of digital data as a form of digital-to-analog conversion. Data transmitted may be digital messages originating from a data source, for example a computer or a keyboard. It may also be an analog signal such as a phone call or a video signal, digitized into a bit-stream, for example, using pulse-code modulation (PCM) or more advanced source coding (analog-to-digital conversion and data compression) schemes. This source coding and decoding is carried out by codec equipment.",
    "Kurt_G\u00f6del": "Kurt Friedrich G\u00f6del (; German: [\u02c8k\u028a\u0250\u032ft \u02c8\u0261\u00f8\u02d0dl\u0329] (); April 28, 1906 \u2013 January 14, 1978) was a logician, mathematician, and analytic philosopher. Considered along with Aristotle and Gottlob Frege to be one of the most significant logicians in history, G\u00f6del had an immense effect upon scientific and philosophical thinking in the 20th century, a time when others such as Bertrand Russell, Alfred North Whitehead, and David Hilbert were analyzing the use of logic and set theory to understand the foundations of mathematics pioneered by Georg Cantor. G\u00f6del published his two incompleteness theorems in 1931 when he was 25 years old, one year after finishing his doctorate at the University of Vienna. The first incompleteness theorem states that for any self-consistent recursive axiomatic system powerful enough to describe the arithmetic of the natural numbers (for example Peano arithmetic), there are true propositions about the natural numbers that cannot be proved from the axioms. To prove this theorem, G\u00f6del developed a technique now known as G\u00f6del numbering, which codes formal expressions as natural numbers. He also showed that neither the axiom of choice nor the continuum hypothesis can be disproved from the accepted axioms of set theory, assuming these axioms are consistent. The former result opened the door for mathematicians to assume the axiom of choice in their proofs. He also made important contributions to proof theory by clarifying the connections between classical logic, intuitionistic logic, and modal logic.",
    "Electrical_engineering": "Electrical engineering is an engineering discipline concerned with the study, design and application of equipment, devices and systems which use electricity, electronics, and electromagnetism. It emerged as an identifiable occupation in the latter half of the 19th century after commercialization of the electric telegraph, the telephone, and electrical power generation, distribution and use. Electrical engineering is now divided into a wide range of fields, including computer engineering, power engineering, telecommunications, radio-frequency engineering, signal processing, instrumentation, and electronics. Many of these disciplines overlap with other engineering branches, spanning a huge number of specializations including hardware engineering, power electronics, electromagnetics and waves, microwave engineering, nanotechnology, electrochemistry, renewable energies, mechatronics, and electrical materials science. See glossary of electrical and electronics engineering. Electrical engineers typically hold a degree in electrical engineering or electronic engineering. Practising engineers may have professional certification and be members of a professional body or an international standards organization. These include the International Electrotechnical Commission (IEC), the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET) (formerly the IEE). The IEC prepares international standards for electrical engineering, developed through consensus, thanks to the work of 20,000 electrotechnical experts, coming from 172 countries worldwide. Electrical engineers work in a very wide range of industries and the skills required are likewise variable. These range from circuit theory to the management skills of a project manager. The tools and equipment that an individual engineer may need are similarly variable, ranging from a simple voltmeter to a top end analyzer to sophisticated design and manufacturing software.",
    "Philosophy_of_mind": "Philosophy of mind is a branch of philosophy that studies the ontology and nature of the mind and its relationship with the body. The mind\u2013body problem is a paradigmatic issue in philosophy of mind, although a number of other issues are addressed, such as the hard problem of consciousness and the nature of particular mental states. Aspects of the mind that are studied include mental events, mental functions, mental properties, consciousness, the ontology of the mind, the nature of thought, and the relationship of the mind to the body. Dualism and monism are the two central schools of thought on the mind\u2013body problem, although nuanced views have arisen that do not fit one or the other category neatly. \n* Dualism finds its entry into Western philosophy thanks to Ren\u00e9 Descartes in the 17th century. Substance dualists like Descartes argue that the mind is an independently existing substance, whereas property dualists maintain that the mind is a group of independent properties that emerge from and cannot be reduced to the brain, but that it is not a distinct substance. \n* Monism is the position that mind and body are not ontologically distinct entities (not dependent substances). This view was first advocated in Western philosophy by Parmenides in the 5th century BCE and was later espoused by the 17th-century rationalist Baruch Spinoza. Physicalists argue that only entities postulated by physical theory exist, and that mental processes will eventually be explained in terms of these entities as physical theory continues to evolve. Physicalists maintain various positions on the prospects of reducing mental properties to physical properties (many of whom adopt compatible forms of property dualism), and the ontological status of such mental properties remains unclear. Idealists maintain that the mind is all that exists and that the external world is either mental itself, or an illusion created by the mind. Neutral monists such as Ernst Mach and William James argue that events in the world can be thought of as either mental (psychological) or physical depending on the network of relationships into which they enter, and dual-aspect monists such as Spinoza adhere to the position that there is some other, neutral substance, and that both matter and mind are properties of this unknown substance. The most common monisms in the 20th and 21st centuries have all been variations of physicalism; these positions include behaviorism, the type identity theory, anomalous monism and functionalism. Most modern philosophers of mind adopt either a reductive physicalist or non-reductive physicalist position, maintaining in their different ways that the mind is not something separate from the body. These approaches have been particularly influential in the sciences, especially in the fields of sociobiology, computer science (specifically, artificial intelligence), evolutionary psychology and the various neurosciences. Reductive physicalists assert that all mental states and properties will eventually be explained by scientific accounts of physiological processes and states. Non-reductive physicalists argue that although the mind is not a separate substance, mental properties supervene on physical properties, or that the predicates and vocabulary used in mental descriptions and explanations are indispensable, and cannot be reduced to the language and lower-level explanations of physical science. Continued neuroscientific progress has helped to clarify some of these issues; however, they are far from being resolved. Modern philosophers of mind continue to ask how the subjective qualities and the intentionality of mental states and properties can be explained in naturalistic terms. However, a number of issues have been recognized with non-reductive physicalism. First, it is irreconcilable with self-identity over time. Secondly, intentional states of consciousness do not make sense on non-reductive physicalism. Thirdly, free will is impossible to reconcile with either reductive or non-reductive physicalism. Fourthly, it fails to properly explain the phenomenon of mental causation.",
    "Bertrand_Meyer": "Bertrand Meyer (; French: [m\u025bj\u025b\u0281]; born 21 November 1950) is a French academic, author, and consultant in the field of computer languages. He created the Eiffel programming language and the idea of design by contract.",
    "Walter_Houser_Brattain": "Walter Houser Brattain (; February 10, 1902 \u2013 October 13, 1987) was an American physicist at Bell Labs who, along with fellow scientists John Bardeen and William Shockley, invented the point-contact transistor in December 1947. They shared the 1956 Nobel Prize in Physics for their invention. Brattain devoted much of his life to research on surface states.",
    "Solid_modeling": "Solid modeling (or modelling) is a consistent set of principles for mathematical and computer modeling of three-dimensional solids. Solid modeling is distinguished from related areas of geometric modeling and computer graphics by its emphasis on physical fidelity. Together, the principles of geometric and solid modeling form the foundation of 3D-computer-aided design and in general support the creation, exchange, visualization, animation, interrogation, and annotation of digital models of physical objects.",
    "Columbia_University": "Columbia University (also known as Columbia, and officially as Columbia University in the City of New York) is a private Ivy League research university in New York City. Established in 1754 on the grounds of Trinity Church in Manhattan, Columbia is the oldest institution of higher education in New York and the fifth-oldest institution of higher learning in the United States. It is one of nine colonial colleges founded prior to the Declaration of Independence, seven of which belong to the Ivy League. Columbia was established as King's College by royal charter of George II of Great Britain in reaction to the founding of Princeton College. It was renamed Columbia College in 1784 following the American Revolution, and in 1787 was placed under a private board of trustees headed by former students Alexander Hamilton and John Jay. In 1896, the campus was moved to its current location in Morningside Heights and renamed Columbia University. Columbia scientists and scholars have played an important role in scientific breakthroughs including: brain-computer interface; the laser and maser; nuclear magnetic resonance; the first nuclear pile; the first nuclear fission reaction in the Americas; the first evidence for plate tectonics and continental drift; and much of the initial research and planning for the Manhattan Project during World War II. Columbia is organized into twenty schools, including three undergraduate schools and numerous graduate schools. The university's research efforts include the Lamont\u2013Doherty Earth Observatory, the Goddard Institute for Space Studies, and accelerator laboratories with major technology firms such as IBM. Columbia is a founding member of the Association of American Universities and was the first school in the United States to grant the M.D. degree. With over 14 million volumes, Columbia University Library is the third largest private research library in the United States. In 2020, Columbia's undergraduate acceptance rate was 6.1%, making it the fourth most selective college in the United States. The university's endowment stood at $10.9 billion in 2019, among the largest of any academic institution. As of 2018, Columbia's alumni and affiliates include: five Founding Fathers of the United States\u2014among them an author of the United States Constitution and a co-author of the Declaration of Independence; three U.S. presidents; 29 foreign heads of state; ten justices of the United States Supreme Court, two of whom currently serve; 96 Nobel laureates; 101 National Academy members; 53 living billionaires; eleven Olympic medalists; 33 Academy Award winners; and 125 Pulitzer Prize recipients.",
    "Probability": "Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur or how likely it is that a proposition is true. The probability of an event is a number between 0 and 1, where, roughly speaking, 0 indicates impossibility of the event and 1 indicates certainty. The higher the probability of an event, the more likely it is that the event will occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (\"heads\" and \"tails\") are both equally probable; the probability of \"heads\" equals the probability of \"tails\"; and since no other outcomes are possible, the probability of either \"heads\" or \"tails\" is 1/2 (which could also be written as 0.5 or 50%). These concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in such areas of study as mathematics, statistics, finance, gambling, science (in particular physics), artificial intelligence/machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.",
    "Epistemology": "Epistemology ( (); from Greek  \u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7, epist\u0113m\u0113, meaning 'knowledge', and  -logy) is the branch of philosophy concerned with knowledge. Epistemologists study the nature of knowledge, epistemic justification, the rationality of belief, and various related issues. Epistemology is considered one of the four main branches of philosophy, along with ethics, logic, and metaphysics. Debates in epistemology are generally clustered around four core areas: 1.  \n* The philosophical analysis of the nature of knowledge and the conditions required for a belief to constitute knowledge, such as truth and justification 2.  \n* Potential sources of knowledge and justified belief, such as perception, reason, memory, and testimony 3.  \n* The structure of a body of knowledge or justified belief, including whether all justified beliefs must be derived from justified foundational beliefs or whether justification requires only a coherent set of beliefs 4.  \n* Philosophical skepticism, which questions the possibility of knowledge, and related problems, such as whether skepticism pose a threat to our ordinary knowledge claims and whether it is possible to refute skeptical arguments In these debates and others, epistemology aims to answer questions such as \"What do we know?\", \"What does it mean to say that we know something?\", \"What makes justified beliefs justified?\", and \"How do we know that we know?\".",
    "Information_system": "An information system (IS) is a formal, sociotechnical, organizational system designed to collect, process, store, and distribute information. In a sociotechnical perspective, information systems are composed by four components: task, people, structure (or roles), and technology. A computer information system is a system composed of people and computers that processes or interprets information. The term is also sometimes used to simply refer to a computer system with software installed. Information Systems is an academic study of systems with a specific reference to information and the complementary networks of hardware and software that people and organizations use to collect, filter, process, create and also distribute data. An emphasis is placed on an information system having a definitive boundary, users, processors, storage, inputs, outputs and the aforementioned communication networks. Any specific information system aims to support operations, management and decision-making. An information system is the information and communication technology (ICT) that an organization uses, and also the way in which people interact with this technology in support of business processes. Some authors make a clear distinction between information systems, computer systems, and business processes. Information systems typically include an ICT component but are not purely concerned with ICT, focusing instead on the end use of information technology. Information systems are also different from business processes. Information systems help to control the performance of business processes. Alter argues for advantages of viewing an information system as a special type of work system. A work system is a system in which humans or machines perform processes and activities using resources to produce specific products or services for customers. An information system is a work system whose activities are devoted to capturing, transmitting, storing, retrieving, manipulating and displaying information. As such, information systems inter-relate with data systems on the one hand and activity systems on the other. An information system is a form of communication system in which data represent and are processed as a form of social memory. An information system can also be considered a semi-formal language which supports human decision making and action. Information systems are the primary focus of study for organizational informatics.",
    "Millennium_Prize_Problems": "The Millennium Prize Problems are seven problems in mathematics that were stated by the Clay Mathematics Institute on May 24, 2000. The problems are the Birch and Swinnerton-Dyer conjecture, Hodge conjecture, Navier\u2013Stokes existence and smoothness, P versus NP problem, Poincar\u00e9 conjecture, Riemann hypothesis, and Yang\u2013Mills existence and mass gap. A correct solution to any of the problems results in a US$1 million prize being awarded by the institute to the discoverer(s). To date, the only Millennium Prize problem to have been solved is the Poincar\u00e9 conjecture, which was solved in 2003 by the Russian mathematician Grigori Perelman. He declined the prize money.",
    "Digital_Revolution": "The Digital Revolution (also known as the Third Industrial Revolution) is the shift from mechanical and analogue electronic technology to digital electronics which began in the latter half of the 20th century, with the adoption and proliferation of digital computers and digital record-keeping, that continues to the present day. Implicitly, the term also refers to the sweeping changes brought about by digital computing and communication technology during this period. Analogous to the Agricultural Revolution and Industrial Revolution, the Digital Revolution marked the beginning of the Information Age. Central to this revolution is the mass production and widespread use of digital logic, MOSFETs (MOS transistors), and integrated circuit (IC) chips, and their derived technologies, including computers, microprocessors, digital cellular phones, and the Internet. These technological innovations have transformed traditional production and business techniques.",
    "Digital_signal_processing": "Digital signal processing (DSP) is the use of digital processing, such as by computers or more specialized digital signal processors, to perform a wide variety of signal processing operations. The digital signals processed in this manner are a sequence of numbers that represent samples of a continuous variable in a domain such as time, space, or frequency. In digital electronics, a digital signal is represented as a pulse train, which is typically generated by the switching of a transistor. Digital signal processing and analog signal processing are subfields of signal processing. DSP applications include audio and speech processing, sonar, radar and other sensor array processing, spectral density estimation, statistical signal processing, digital image processing, data compression, video coding, audio coding, image compression, signal processing for telecommunications, control systems, biomedical engineering, and seismology, among others. DSP can involve linear or nonlinear operations. Nonlinear signal processing is closely related to nonlinear system identification and can be implemented in the time, frequency, and spatio-temporal domains. The application of digital computation to signal processing allows for many advantages over analog processing in many applications, such as error detection and correction in transmission as well as data compression. Digital signal processing is also fundamental to digital technology, such as digital telecommunication and wireless communications. DSP is applicable to both streaming data and static (stored) data.",
    "Transistor": "A transistor is a semiconductor device used to amplify or switch electronic signals and electrical power. It is composed of semiconductor material usually with at least three terminals for connection to an external circuit. A voltage or current applied to one pair of the transistor's terminals controls the current through another pair of terminals. Because the controlled (output) power can be higher than the controlling (input) power, a transistor can amplify a signal. Today, some transistors are packaged individually, but many more are found embedded in integrated circuits. Austro-Hungarian physicist Julius Edgar Lilienfeld proposed the concept of a field-effect transistor in 1926, but it was not possible to actually construct a working device at that time. The first working device to be built was a point-contact transistor invented in 1947 by American physicists John Bardeen and Walter Brattain while working under William Shockley at Bell Labs. They shared the 1956 Nobel Prize in Physics for their achievement. The most widely used transistor is the MOSFET (metal\u2013oxide\u2013semiconductor field-effect transistor), also known as the MOS transistor, which was invented by Mohamed Atalla with Dawon Kahng at Bell Labs in 1959. The MOSFET was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses. Transistors revolutionized the field of electronics, and paved the way for smaller and cheaper radios, calculators, and computers, among other things. The first transistor and the MOSFET are on the list of IEEE milestones in electronics. The MOSFET is the fundamental building block of modern electronic devices, and is ubiquitous in modern electronic systems. An estimated total of 13 sextillion MOSFETs have been manufactured between 1960 and 2018 (at least 99.9% of all transistors), making the MOSFET the most widely manufactured device in history. Most transistors are made from very pure silicon, and some from germanium, but certain other semiconductor materials are sometimes used. A transistor may have only one kind of charge carrier, in a field-effect transistor, or may have two kinds of charge carriers in bipolar junction transistor devices. Compared with the vacuum tube, transistors are generally smaller, and require less power to operate. Certain vacuum tubes have advantages over transistors at very high operating frequencies or high operating voltages. Many types of transistors are made to standardized specifications by multiple manufacturers.",
    "ACM_SIGACT": "ACM SIGACT or SIGACT is the Association for Computing Machinery Special Interest Group on Algorithms and Computation Theory, whose purpose is support of research in theoretical computer science. It was founded in 1968 by Patrick C. Fischer.",
    "Information_processing": "Information processing is the change (processing) of information in any manner detectable by an observer. As such, it is a process that describes everything that happens (changes) in the universe, from the falling of a rock (a change in position) to the printing of a text file from a digital computer system. In the latter case, an information processor (the printer) is changing the form of presentation of that text file (from bytes to glyphs).The computers up to this period function on the basis of programmes saved in the memory, they have no intelligence of their own.",
    "Formal_language": "In mathematics, computer science, and linguistics, a formal language consists of words whose letters are taken from an alphabet and are well-formed according to a specific set of rules. The alphabet of a formal language consist of symbols, letters, or tokens that concatenate into strings of the language. Each string concatenated from symbols of this alphabet is called a word, and the words that belong to a particular formal language are sometimes called well-formed words or well-formed formulas. A formal language is often defined by means of a formal grammar such as a regular grammar or context-free grammar, which consists of its formation rules. The field of formal language theory studies primarily the purely syntactical aspects of such languages\u2014that is, their internal structural patterns. Formal language theory sprang out of linguistics, as a way of understanding the syntactic regularities of natural languages.In computer science, formal languages are used among others as the basis for defining the grammar of programming languages and formalized versions of subsets of natural languages in which the words of the language represent concepts that are associated with particular meanings or semantics. In computational complexity theory, decision problems are typically defined as formal languages, and complexity classes are defined as the sets of the formal languages that can be parsed by machines with limited computational power. In logic and the foundations of mathematics, formal languages are used to represent the syntax of axiomatic systems, and mathematical formalism is the philosophy that all of mathematics can be reduced to the syntactic manipulation of formal languages in this way.",
    "Abacus": "The abacus (plural abaci or abacuses), also called a counting frame, is a calculating tool that was in use in the ancient Near East, Europe, China, and Russia, centuries before the adoption of the written Arabic numeral system. The exact origin of the abacus is still unknown. Today, abacuses are often constructed as a bamboo frame with beads sliding on wires, but originally they were beans or stones moved in grooves of sand or on tablets of wood, stone, or metal. Abacuses come in different designs. Some designs, like the bead frame consisting of beads divided into tens, are used mainly to teach arithmetic, although they remain popular in the post-Soviet states as a tool. Other designs, such as the Japanese soroban, have been used for practical calculations even involving several digits. For any particular abacus design, there are usually numerous different methods to perform a certain type of calculation, which may include basic operations like addition and multiplication, or even more complex ones, such as calculating square roots. Some of these methods may work with non-natural numbers (numbers such as 1.5 and \u200b3\u20444). Although today many use calculators and computers instead of abacuses to calculate, abacuses still remain in common use in some countries. Merchants, traders and clerks in some parts of Eastern Europe, Russia, China and Africa use abacuses, and they are still used to teach arithmetic to children. Some people who are unable to use a calculator because of visual impairment may use an abacus.",
    "John_von_Neumann": "John von Neumann (; Hungarian: Neumann J\u00e1nos Lajos, pronounced [\u02c8n\u0252jm\u0252n \u02c8ja\u02d0no\u0283 \u02c8l\u0252jo\u0283]; December 28, 1903 \u2013 February 8, 1957) was a Hungarian-American mathematician, physicist, computer scientist, engineer and polymath. Von Neumann was generally regarded as the foremost mathematician of his time and said to be \"the last representative of the great mathematicians\"; who integrated both pure and applied sciences. He made major contributions to a number of fields, including mathematics (foundations of mathematics, functional analysis, ergodic theory, representation theory, operator algebras, geometry, topology, and numerical analysis), physics (quantum mechanics, hydrodynamics, and quantum statistical mechanics), economics (game theory), computing (Von Neumann architecture, linear programming, self-replicating machines, stochastic computing), and statistics. He was a pioneer of the application of operator theory to quantum mechanics in the development of functional analysis, and a key figure in the development of game theory and the concepts of cellular automata, the universal constructor and the digital computer. He published over 150 papers in his life: about 60 in pure mathematics, 60 in applied mathematics, 20 in physics, and the remainder on special mathematical subjects or non-mathematical ones. His last work, an unfinished manuscript written while he was in the hospital, was later published in book form as The Computer and the Brain. His analysis of the structure of self-replication preceded the discovery of the structure of DNA. In a short list of facts about his life he submitted to the National Academy of Sciences, he stated, \"The part of my work I consider most essential is that on quantum mechanics, which developed in G\u00f6ttingen in 1926, and subsequently in Berlin in 1927\u20131929. Also, my work on various forms of operator theory, Berlin 1930 and Princeton 1935\u20131939; on the ergodic theorem, Princeton, 1931\u20131932.\" During World War II, von Neumann worked on the Manhattan Project with theoretical physicist Edward Teller, mathematician Stanis\u0142aw Ulam and others, problem solving key steps in the nuclear physics involved in thermonuclear reactions and the hydrogen bomb. He developed the mathematical models behind the explosive lenses used in the implosion-type nuclear weapon, and coined the term \"kiloton\" (of TNT), as a measure of the explosive force generated. After the war, he served on the General Advisory Committee of the United States Atomic Energy Commission, and consulted for a number of organizations, including the United States Air Force, the Army's Ballistic Research Laboratory, the Armed Forces Special Weapons Project, and the Lawrence Livermore National Laboratory. As a Hungarian \u00e9migr\u00e9, concerned that the Soviets would achieve nuclear superiority, he designed and promoted the policy of mutually assured destruction to limit the arms race.",
    "Mixed_reality": "Mixed reality (MR) is the merging of real and virtual worlds to produce new environments and visualizations, where physical and digital objects co-exist and interact in real time. Mixed reality does not exclusively take place in either the physical or virtual world, but is a hybrid of reality and virtual reality, encompassing both augmented reality and augmented virtuality via immersive technology. The first immersive mixed reality system that provided enveloping sight, sound, and touch was the Virtual Fixtures platform, which was developed in 1992 at the Armstrong Laboratories of the United States Air Force. The project demonstrated that human performance could be significantly amplified, by overlaying spatially registered virtual objects on top of a person's direct view of a real physical environment.",
    "Applied_mathematics": "Applied mathematics is the application of mathematical methods by different fields such as physics, engineering, medicine, biology, business, computer science, and industry. Thus, applied mathematics is a combination of mathematical science and specialized knowledge. The term \"applied mathematics\" also describes the professional specialty in which mathematicians work on practical problems by formulating and studying mathematical models. In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics where abstract concepts are studied for their own sake. The activity of applied mathematics is thus intimately connected with research in pure mathematics.",
    "Interpreter_(computing)": "In computer science, an interpreter is a computer program that directly executes instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program. An interpreter generally uses one of the following strategies for program execution: 1.  \n* Parse the source code and perform its behavior directly; 2.  \n* Translate source code into some efficient intermediate representation and immediately execute this; 3.  \n* Explicitly execute stored precompiled code made by a compiler which is part of the interpreter system. Early versions of Lisp programming language and Dartmouth BASIC would be examples of the first type. Perl, Python, MATLAB, and Ruby are examples of the second, while UCSD Pascal is an example of the third type. Source programs are compiled ahead of time and stored as machine independent code, which is then linked at run-time and executed by an interpreter and/or compiler (for JIT systems). Some systems, such as Smalltalk and contemporary versions of BASIC and Java may also combine two and three. Interpreters of various types have also been constructed for many languages traditionally associated with compilation, such as Algol, Fortran, Cobol, C and C++. While interpretation and compilation are the two main means by which programming languages are implemented, they are not mutually exclusive, as most interpreting systems also perform some translation work, just like compilers. The terms \"interpreted language\" or \"compiled language\" signify that the canonical implementation of that language is an interpreter or a compiler, respectively. A high level language is ideally an abstraction independent of particular implementations.",
    "List_of_unsolved_problems_in_computer_science": "This article is a list of notable unsolved problems in computer science. A problem in computer science is considered unsolved when no solution is known, or when experts in the field disagree about proposed solutions.",
    "Audio_signal_processing": "Audio signal processing is a subfield of signal processing that is concerned with the electronic manipulation of audio signals. Audio signals are electronic representations of sound waves\u2014longitudinal waves which travel through air, consisting of compressions and rarefactions. The energy contained in audio signals is typically measured in decibels. As audio signals may be represented in either digital or analog format, processing may occur in either domain. Analog processors operate directly on the electrical signal, while digital processors operate mathematically on its digital representation.",
    "Linear_network_coding": "Network coding is a field of research founded in a series of papers from the late 1990s to the early 2000s. However, the concept of network coding, in particular linear network coding, appeared much earlier. In a 1978 paper, a scheme for improving the throughput of a two-way communication through a satellite was proposed. In this scheme, two users trying to communicate with each other transmit their data streams to a satellite, which combines the two streams by summing them modulo 2 and then broadcasts the combined stream. Each of the two users, upon receiving the broadcast stream, can decode the other stream by using the information of their own stream. The 2000 paper  gave the butterfly network example (discussed below) that illustrates how linear network coding can outperform routing. This example is equivalent to the scheme for satellite communication described above. The same paper gave an optimal coding scheme for a network with one source node and three destination nodes. This is the first example illustrating the optimality of convolutional network coding (a more general form of linear network coding) over a cyclic network. Linear network coding may be used to improve a network's throughput, efficiency and scalability, as well as resilience to attacks and eavesdropping. Instead of simply relaying the packets of information they receive, the nodes of a network take several packets and combine them together for transmission. This may be used to attain the maximum possible information flow in a network. It has been mathematically proven that in theory linear coding is enough to achieve the upper bound in multicast problems with one source. However linear coding is not sufficient in general (e.g. multisource, multisink with arbitrary demands), even for more general versions of linearity such as convolutional coding and . Finding optimal coding solutions for general network problems with arbitrary demands remains an open problem.",
    "Information": "Information can be thought of as the resolution of uncertainty; it is that which answers the question of \"what an entity is\" and thus defines both its essence and nature of its characteristics. The concept of information has different meanings in different contexts. Thus the concept becomes related to notions of constraint, communication, control, data, form, education, knowledge, meaning, understanding, mental stimuli, pattern, perception, representation, and entropy. Information is associated with data, as data represents values attributed to parameters, and information is data in context and with meaning attached. Information also relates to knowledge, as knowledge signifies understanding of an abstract or concrete concept. In terms of communication, information is expressed either as the content of a message or through direct or indirect observation. That which is perceived can be construed as a message in its own right, and in that sense, information is always conveyed as the content of a message. Information can be encoded into various forms for transmission and interpretation (for example, information may be encoded into a sequence of signs, or transmitted via a signal). It can also be encrypted for safe storage and communication. The uncertainty of an event is measured by its probability of occurrence and is inversely proportional to that. The more uncertain an event, the more information is required to resolve uncertainty of that event. The bit is a typical unit of information, but other units such as the nat may be used. For example, the information encoded in one \"fair\" coin flip is log2(2/1) = 1 bit, and in two fair coin flips islog2(4/1) = 2 bits.",
    "Algorithm": "In mathematics and computer science, an algorithm ( ()) is a finite sequence of well-defined, computer-implementable instructions, typically to solve a class of problems or to perform a computation. Algorithms are always unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks. As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input. The concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, was used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC. Greek mathematicians later used algorithms in the sieve of Eratosthenes for finding prime numbers, and the Euclidean algorithm for finding the greatest common divisor of two numbers. Arabic mathematicians such as al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis. The word algorithm itself is derived from the 9th-century mathematician Mu\u1e25ammad ibn M\u016bs\u0101 al-Khw\u0101rizm\u012b, Latinized Algoritmi. A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem  (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define \"effective calculability\" or \"effective method\". Those formalizations included the G\u00f6del\u2013Herbrand\u2013Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936\u201337 and 1939.",
    "Field-effect_transistor": "The field-effect transistor (FET) is a type of transistor which uses an electric field to control the flow of current. FETs are devices with three terminals: source, gate, and drain. FETs control the flow of current by the application of a voltage to the gate, which in turn alters the conductivity between the drain and source. FETs are also known as unipolar transistors since they involve single-carrier-type operation. That is, FETs use electrons or holes as charge carriers in their operation, but not both. Many different types of field effect transistors exist. Field effect transistors generally display very high input impedance at low frequencies. The most widely used field-effect transistor is the MOSFET (metal-oxide-semiconductor field-effect transistor).",
    "Safety-critical_system": "A safety-critical system (SCS) or life-critical system is a system whose failure or malfunction may result in one (or more) of the following outcomes: \n* death or serious injury to people \n* loss or severe damage to equipment/property \n* environmental harm A safety-related system (or sometimes safety-involved system) comprises everything (hardware, software, and human aspects) needed to perform one or more safety functions, in which failure would cause a significant increase in the safety risk for the people or environment involved. Safety-related systems are those that do not have full responsibility for controlling hazards such as loss of life, severe injury or severe environmental damage. The malfunction of a safety-involved system would only be that hazardous in conjunction with the failure of other systems or human error. Some safety organizations provide guidance on safety-related systems, for example the Health and Safety Executive (HSE) in the United Kingdom. Risks of this sort are usually managed with the methods and tools of safety engineering. A safety-critical system is designed to lose less than one life per billion (109) hours of operation. Typical design methods include probabilistic risk assessment, a method that combines failure mode and effects analysis (FMEA) with fault tree analysis. Safety-critical systems are increasingly computer-based.",
    "Microcontroller": "A microcontroller (MCU for microcontroller unit) is a small computer on a single metal-oxide-semiconductor (MOS) integrated circuit (IC) chip. In modern terminology, it is similar to, but less sophisticated than, a system on a chip (SoC); a SoC may include a microcontroller as one of its components. A microcontroller contains one or more CPUs (processor cores) along with memory and programmable input/output peripherals. Program memory in the form of ferroelectric RAM, NOR flash or OTP ROM is also often included on chip, as well as a small amount of RAM. Microcontrollers are designed for embedded applications, in contrast to the microprocessors used in personal computers or other general purpose applications consisting of various discrete chips. Microcontrollers are used in automatically controlled products and devices, such as automobile engine control systems, implantable medical devices, remote controls, office machines, appliances, power tools, toys and other embedded systems. By reducing the size and cost compared to a design that uses a separate microprocessor, memory, and input/output devices, microcontrollers make it economical to digitally control even more devices and processes. Mixed signal microcontrollers are common, integrating analog components needed to control non-digital electronic systems. In the context of the internet of things, microcontrollers are an economical and popular means of data collection, sensing and actuating the physical world as edge devices. Some microcontrollers may use four-bit words and operate at frequencies as low as 4 kHz, for low power consumption (single-digit milliwatts or microwatts). They generally have the ability to retain functionality while waiting for an event such as a button press or other interrupt; power consumption while sleeping (CPU clock and most peripherals off) may be just nanowatts, making many of them well suited for long lasting battery applications. Other microcontrollers may serve performance-critical roles, where they may need to act more like a digital signal processor (DSP), with higher clock speeds and power consumption.",
    "Signal_processing": "Signal processing is an electrical engineering subfield that focuses on analysing, modifying, and synthesizing signals such as sound, images, and scientific measurements. Signal processing techniques can be used to improve transmission, storage efficiency and subjective quality and to also emphasize or detect components of interest in a measured signal.",
    "Logic_in_computer_science": "Logic in computer science covers the overlap between the field of logic and that of computer science. The topic can essentially be divided into three main areas: \n* Theoretical foundations and analysis \n* Use of computer technology to aid logicians \n* Use of concepts from logic for computer applications",
    "Dawon_Kahng": "Dawon Kahng (May 4, 1931 \u2013 May 13, 1992) was a Korean-American electrical engineer and inventor, known for his work in solid-state electronics. He is best known for inventing the MOSFET (metal-oxide-semiconductor field-effect transistor), also known as the MOS transistor, with Mohamed Atalla in 1959. Atalla and Kahng developed both the PMOS and NMOS processes for MOSFET semiconductor device fabrication. The MOSFET is the most widely used type of transistor, and the basic element in most modern electronic equipment. Atalla and Kahng later proposed the concept of the MOS integrated circuit, and they did pioneering work on Schottky diodes and nanolayer-base transistors in the early 1960s. Kahng then invented the floating-gate MOSFET (FGMOS) with Simon Sze in 1967. Kahng and Sze proposed that FGMOS could be used as floating-gate memory cells for non-volatile memory (NVM) and reprogrammable read-only memory (ROM), which became the basis for EPROM (erasable programmable ROM), EEPROM (electrically erasable programmable ROM) and flash memory technologies. Kahng was inducted into the National Inventors Hall of Fame in 2009.",
    "Bit": "The bit is a basic unit of information in computing and digital communications. The name is a portmanteau of binary digit. The bit represents a logical state with one of two possible values. These values are most commonly represented as either 0or1, but other representations such as true/false, yes/no, +/\u2212, or on/off are common. The correspondence between these values and the physical states of the underlying storage or device is a matter of convention, and different assignments may be used even within the same device or program. It may be physically implemented with a two-state device. The symbol for the binary digit is either bit per recommendation by the IEC 80000-13:2008 standard, or the lowercase character b, as recommended by the IEEE 1541-2002 and IEEE Std 260.1-2004 standards. A group of eight binary digits is commonly called one byte, but historically the size of the byte is not strictly defined. In information theory, one bit is the information entropy of a binary random variable that is 0 or 1 with equal probability, or the information that is gained when the value of such a variable becomes known. As a unit of information, the bit is also known as a shannon, named after Claude E. Shannon.",
    "Julius_Edgar_Lilienfeld": "Julius Edgar Lilienfeld (April 18, 1882 \u2013 August 28, 1963) an Austro-Hungarian, and later American (where he moved in 1921) physicist and electrical engineer, credited with the first patents on the field-effect transistor (FET) (1925) and electrolytic capacitor (1931). Because of his failure to publish articles in learned journals and because high-purity semiconductor materials were not available yet, his FET patent never achieved fame, causing confusion for later inventors.",
    "Percy_Ludgate": "Percy Edwin Ludgate (2 August 1883 \u2013 16 October 1922) was an Irish amateur scientist who designed the second analytical engine (general-purpose Turing-complete computer) in history. Ludgate was born in Skibbereen, Cork, to Michael Ludgate and Mary McMahon. In the 1901 census, he is listed as Civil Servant National Education (Boy Copyist) in Dublin. In the 1911 census he's also in Dubiln, as a Commercial Clerk (Corn Merchant). He studied accountancy at Rathmines College of Commerce, earning a gold medal based on the results his final examinations in 1917. At some date before or after then he joined Kevans & Son, accountants. He died in Dublin aged 39, of pneumonia, his death certificate listing him as an accountant. It seems that Ludgate worked as a clerk for an unknown corn merchants, in Dublin, and pursued his interest in calculating machines at night. Charles Babbage in 1843 and Ludgate in 1909 designed the only two mechanical analytical engines before the electromechanical analytical engine of Leonardo Torres y Quevedo of 1920 and its few successors, and the six first-generation electronic analytical engines of 1949. Working alone, Ludgate designed an analytical engine while unaware of Babbage's designs, although he later went on to write about Babbage's machine. Ludgate's engine used multiplication as its base mechanism (unlike Babbage's which used addition). It incorporated the first multiplier-accumulator (MAC), and was the first to exploit a MAC to perform division (using multiplication seeded by reciprocal, via the convergent series (1 + x)\u22121). Ludgate's engine used a mechanism similar to slide rules, but employed his unique discrete Logarithmic Indexes (now known as Irish logarithms (Boys, 1909)), and provided a very novel memory using concentric cylinders, storing numbers as displacements of rods in shuttles. His design featured several other novel features, including for program control (e.g. preemption and subroutines \u2013 or microcode, depending on viewpoint). The design is so different from Babbage's as to be a second type of analytical engine, preceding the third (electromechanical) and fourth (electronic) types. The engine's precise mechanism is unknown as the only written accounts of the engine which survive do not detail its workings, although he stated in 1914 that \"[c]omplete descriptive drawings of the machine exist, as well as a description in manuscript\" \u2013 these have never been found. He was one of a few independent workers in the field of science and mathematics. His inventions were worked on outside a lab. He worked on the inventions only part-time, often until the early hours of the morning. Many publications refer to him as an accountant, but that came eight years after his 1909 analytical engine paper. Little is known about his personal life, as his only records are his scientific writings. The best source of information about Ludgate and his significance lie in the work of Professor Brian Randell. As from 2016, a further investigation is underway at Trinity College, Dublin under the auspices of The John Gabriel Byrne Computer Science Collection. He died of pneumonia in 1922, and is buried in Mount Jerome Cemetery in Dublin. In 1991, a prize for the best final year project in the Moderatorship in computer science course at Trinity College, Dublin \u2013 the Ludgate Prize \u2013 was instituted in his honor, and in 2016 the Ludgate Hub e-business incubation centre was opened in Skibbereen, where he was born.",
    "Charles_Xavier_Thomas": "Charles Xavier Thomas de Colmar (May 5, 1785 \u2013 March 12, 1870) was a French inventor and entrepreneur best known for designing, patenting and manufacturing the first commercially successful mechanical calculator, the Arithmometer, and for founding the insurance companies Le Soleil and L'aigle which, under his leadership, became the number one insurance group in France at the beginning of the Second Empire.",
    "Computational_learning_theory": "In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.",
    "Claude_Shannon": "Claude Elwood Shannon (April 30, 1916 \u2013 February 24, 2001) was an American mathematician, electrical engineer, and cryptographer known as \"the father of information theory\". Shannon is noted for having founded information theory with a landmark paper, \"A Mathematical Theory of Communication\", that he published in 1948. He is also well known for founding digital circuit design theory in 1937, when\u2014as a 21-year-old master's degree student at the Massachusetts Institute of Technology (MIT)\u2014he wrote his thesis demonstrating that electrical applications of Boolean algebra could construct any logical numerical relationship. Shannon contributed to the field of cryptanalysis for national defense during World War II, including his fundamental work on codebreaking and secure telecommunications.",
    "David_Parnas": "David Lorge Parnas (born February 10, 1941) is a Canadian early pioneer of software engineering, who developed the concept of information hiding in modular programming, which is an important element of object-oriented programming today. He is also noted for his advocacy of precise documentation.",
    "Imperative_programming": "In computer science, imperative programming is a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates. The term is often used in contrast to declarative programming, which focuses on what the program should accomplish without specifying how the program should achieve the result.",
    "Computer_Science_Teachers_Association": "The Computer Science Teachers Association (CSTA) is a professional association whose mission to \u201cempower, engage and advocate for K-12 CS teachers worldwide.\u201d It supports and encourages education in the field of computer science and related areas. Started in 2004, CSTA supports computer science education in elementary schools, middle schools, high schools, higher education, and industry.",
    "Transistor_computer": "A transistor computer, now often called a second generation computer, is a computer which uses discrete transistors instead of vacuum tubes. The first generation of electronic computers used vacuum tubes, which generated large amounts of heat, were bulky and unreliable. A second generation of computers, through the late 1950s and 1960s featured circuit boards filled with individual transistors and magnetic core memory. These machines remained the mainstream design into the late 1960s, when integrated circuits started appearing and led to the third generation computer.",
    "Computational_problem": "In theoretical computer science, a computational problem is a problem that a computer might be able to solve, or a question that a computer may be able to answer. For example, the problem of factoring \"Given a positive integer n, find a nontrivial prime factor of n.\" is a computational problem. A computational problem can be viewed as an infinite collection of instances together with a, possibly empty, set of solutions for every instance. For example, in the factoring problem, the instances are the integers n, and solutions are prime numbers p that describe nontrivial prime factors of n. Computational problems are one of the main objects of study in theoretical computer science. The field of computational complexity theory attempts to determine the amount of resources (computational complexity) solving a given problem will require and explain why some problems are intractable or undecidable. Computational problems belong to complexity classes that define broadly the time it takes to compute (solve) them with various abstract machines (e.g. classical or quantum machines). It is typical of many problems to represent both instances and solutions by binary strings, namely elements of {0, 1}* (see regular expressions for the notation used). For example, numbers can be represented as binary strings using the binary encoding. For readability, we sometimes identify numbers with their binary encodings in the examples below.",
    "Department_of_Computer_Science_and_Technology,_University_of_Cambridge": "The Department of Computer Science and Technology, formerly the Computer Laboratory, is the computer science department of the University of Cambridge. As of 2007 it employed 35 academic staff, 25 support staff, 35 affiliated research staff, and about 155 research students. The current Head of Department is Professor Ann Copestake.",
    "MOSFET": "The metal\u2013oxide\u2013semiconductor field-effect transistor (MOSFET, MOS-FET, or MOS FET), also known as the metal\u2013oxide\u2013silicon transistor (MOS transistor, or MOS), is a type of insulated-gate field-effect transistor (IGFET) that is fabricated by the controlled oxidation of a semiconductor, typically silicon. The voltage of the covered gate determines the electrical conductivity of the device; this ability to change conductivity with the amount of applied voltage can be used for amplifying or switching electronic signals. The MOSFET was invented by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959, and first presented in June 1960. It is the basic building block of modern electronics, and the most frequently manufactured device in history, with an estimated total of 13 sextillion (1.3\u00d71022) MOSFETs manufactured between 1960 and 2018. It is the dominant semiconductor device in digital and analog integrated circuits (ICs), and the most common power device. It is a compact transistor that has been miniaturised and mass-produced for a wide range of applications, revolutionizing the electronics industry and the world economy, and being central to the digital revolution, silicon age and information age. MOSFET scaling and miniaturization has been driving the rapid exponential growth of electronic semiconductor technology since the 1960s, and enables high-density ICs such as memory chips and microprocessors. The MOSFET is considered the \"workhorse\" of the electronics industry. A key advantage of a MOSFET is that it requires almost no input current to control the load current, when compared with bipolar junction transistors (BJTs). In an enhancement mode MOSFET, voltage applied to the gate terminal can increase the conductivity from the \"normally off\" state. In a depletion mode MOSFET, voltage applied at the gate can reduce the conductivity from the \"normally on\" state. MOSFETs are also capable of high scalability, with increasing miniaturization, and can be easily scaled down to smaller dimensions. They also have faster switching speed (ideal for digital signals), much smaller size, consume significantly less power, and allow much higher density (ideal for large-scale integration), compared to BJTs. MOSFETs are also cheaper and have relatively simple processing steps, resulting in high manufacturing yield. MOSFETs can either be manufactured as part of MOS integrated circuit (MOS IC) chips or as discrete MOSFET devices (such as a power MOSFET), and can take the form of single-gate or multi-gate transistors. Since MOSFETs can be made with either p-type or n-type semiconductors (PMOS or NMOS logic, respectively), complementary pairs of MOSFETs can be used to make switching circuits with very low power consumption, in the form of complementary MOS (CMOS) logic. The name \"metal\u2013oxide\u2013semiconductor\" (MOS) typically refers to a metal gate, oxide insulation, and semiconductor (typically silicon). However, the \"metal\" in the name MOSFET is sometimes a misnomer, because the gate material can also be a layer of polysilicon (polycrystalline silicon). Along with oxide, different dielectric materials can also be used with the aim of obtaining strong channels with smaller applied voltages. The MOS capacitor is also part of the MOSFET structure.",
    "List_of_computer_science_awards": "This list of computer science awards is an index to articles on notable awards related to computer science. It includes lists of awards by the Association for Computing Machinery, the Institute of Electrical and Electronics Engineers, other computer science and information science awards, and a list of computer science competitions. The top computer science award is the ACM Turing Award, generally regarded as the Nobel Prize equivalent for Computer Science.  Other highly regarded top computer science awards include IEEE John von Neumann Medal awarded by the IEEE Board of Directors, and the Japan Kyoto Prize for Information Science.",
    "Purdue_University": "Purdue University is a public research university in West Lafayette, Indiana, and the flagship campus of the Purdue University system. The university was founded in 1869 after Lafayette businessman John Purdue donated land and money to establish a college of science, technology, and agriculture in his name. The first classes were held on September 16, 1874, with six instructors and 39 students. The main campus in West Lafayette offers more than 200 majors for undergraduates, over 69 masters and doctoral programs, and professional degrees in pharmacy and veterinary medicine. In addition, Purdue has 18 intercollegiate sports teams and more than 900 student organizations. Purdue is a member of the Big Ten Conference and enrolls the second largest student body of any university in Indiana, as well as the fourth largest foreign student population of any university in the United States. Purdue University is a member of the Association of American Universities and is classified among \"R1: Doctoral Universities \u2013 Very high research activity\". Purdue has 25 American astronauts as alumni and as of April 2019, the university has been associated with 13 Nobel Prizes.",
    "Conference_proceeding": "In academia and librarianship, conference proceeding is a collection of academic papers published in the context of an academic conference or workshop. Conference proceedings typically contain the contributions made by researchers at the conference. They are the written record of the work that is presented to fellow researchers. In many fields, they are published as supplements to academic journals; in some, they are considered the main dissemination route; in others they may be considered grey literature. They are usually distributed in printed or electronic volumes, either before the conference opens or after it has closed. A less common, broader meaning of proceedings are the acts and happenings of an academic field, a learned society. For example, the title of the Acta Crystallographica journals is New Latin for \"Proceedings in Crystallography\"; the Proceedings of the National Academy of Sciences of the United States of America is the main journal of that academy. Scientific journals whose ISO 4 title abbreviations start with Proc, Acta, or Trans are journals of the proceedings (transactions) of a field or of an organization concerned with it, in that secondary meaning of the word.",
    "Computer": "A computer is a machine that can be instructed to carry out sequences of arithmetic or logical operations automatically via computer programming. Modern computers have the ability to follow generalized sets of operations, called programs. These programs enable computers to perform an extremely wide range of tasks. A \"complete\" computer including the hardware, the operating system (main software), and peripheral equipment required and used for \"full\" operation can be referred to as a computer system. This term may as well be used for a group of computers that are connected and work together, in particular a computer network or computer cluster. Computers are used as control systems for a wide variety of industrial and consumer devices. This includes simple special purpose devices like microwave ovens and remote controls, factory devices such as industrial robots and computer-aided design, and also general purpose devices like personal computers and mobile devices such as smartphones. The Internet is run on computers and it connects hundreds of millions of other computers and their users. Early computers were only conceived as calculating devices. Since ancient times, simple manual devices like the abacus aided people in doing calculations. Early in the Industrial Revolution, some mechanical devices were built to automate long tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit (IC) chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s. The speed, power and versatility of computers have been increasing dramatically ever since then, with MOS transistor counts increasing at a rapid pace (as predicted by Moore's law), leading to the Digital Revolution during the late 20th to early 21st centuries. Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU) in the form of a metal-oxide-semiconductor (MOS) microprocessor, along with some type of computer memory, typically MOS semiconductor memory chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved.",
    "Evolutionary_computation": "In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and  studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character. In evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. Each new generation is produced by stochastically removing less desired solutions, and introducing small random changes. In biological terminology, a population of solutions is subjected to natural selection (or artificial selection) and mutation. As a result, the population will gradually evolve to increase in fitness, in this case the chosen fitness function of the algorithm. Evolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. Many variants and extensions exist, suited to more specific families of problems and data structures. Evolutionary computation is also sometimes used in evolutionary biology as an in silico experimental procedure to study common aspects of general evolutionary processes.",
    "Harvard_Business_School": "Harvard Business School (HBS) is the graduate business school of Harvard University. Located in Boston, Massachusetts, it is consistently ranked among the top business schools in the world and offers a large full-time MBA program, management-related doctoral programs, and many executive education programs. It owns Harvard Business Publishing, which publishes business books, leadership articles, case studies, and the monthly Harvard Business Review. It is also home to the Baker Library/Bloomberg Center.",
    "Automata_theory": "Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science. The word automata (the plural of automaton) comes from the Greek word \u03b1\u1f50\u03c4\u03cc\u03bc\u03b1\u03c4\u03b1, which means \"self-making\". The figure at right illustrates a finite-state machine, which belongs to a well-known type of automaton. This automaton consists of states (represented in the figure by circles) and transitions (represented by arrows). As the automaton sees a symbol of input, it makes a transition (or jump) to another state, according to its transition function, which takes the current state and the recent symbol as its inputs. Automata theory is closely related to formal language theory. An automaton is a finite representation of a formal language that may be an infinite set. Automata are often classified by the class of formal languages they can recognize, typically illustrated by the Chomsky hierarchy, which describes the relations between various languages and kinds of formalized logics. Automata play a major role in theory of computation, compiler construction, artificial intelligence, parsing and formal verification.",
    "ENIAC": "ENIAC (; Electronic Numerical Integrator and Computer) was the first electronic general-purpose digital computer. It was Turing-complete, and able to solve \"a large class of numerical problems\" through reprogramming. Although ENIAC was designed and primarily used to calculate artillery firing tables for the United States Army's Ballistic Research Laboratory (which later became a part of the Army Research Laboratory), its first program was a study of the feasibility of the thermonuclear weapon. ENIAC was completed in 1945 and first put to work for practical purposes on December 10, 1945. ENIAC was formally dedicated at the University of Pennsylvania on February 15, 1946 and was heralded as a \"Giant Brain\" by the press. It had a speed on the order of one thousand times faster than that of electro-mechanical machines; this computational power, coupled with general-purpose programmability, excited scientists and industrialists alike. The combination of speed and programmability allowed for thousands more calculations for problems, as ENIAC calculated a trajectory in 30 seconds that took a human 20 hours (allowing one ENIAC hour to displace 2,400 human hours). The completed machine was announced to the public the evening of February 14, 1946 and formally dedicated the next day at the University of Pennsylvania, having cost almost $500,000 (approximately $6,300,000 today). It was formally accepted by the U.S. Army Ordnance Corps in July 1946. ENIAC was shut down on November 9, 1946 for a refurbishment and a memory upgrade, and was transferred to Aberdeen Proving Ground, Maryland in 1947. There, on July 29, 1947, it was turned on and was in continuous operation until 11:45 p.m. on October 2, 1955.",
    "Information_engineering_(field)": "Information engineering is the engineering discipline that deals with the generation, distribution, analysis, and use of information, data, and knowledge in systems. The field first became identifiable in the early 21st century. The components of information engineering include more theoretical fields such as machine learning, artificial intelligence, control theory, signal processing, and information theory, and more applied fields such as computer vision, natural language processing, bioinformatics, medical image computing, cheminformatics, autonomous robotics, mobile robotics, and telecommunications. Many of these originate from computer science, as well as other branches of engineering such as computer engineering, electrical engineering, and bioengineering. The field of information engineering is based heavily on mathematics, particularly probability, statistics, calculus, linear algebra, optimization, differential equations, variational calculus, and complex analysis. Information engineers often hold a degree in information engineering or a related area, and are often part of a professional body such as the Institution of Engineering and Technology or Institute of Measurement and Control. They are employed in almost all industries due to the widespread use of information engineering.",
    "Natural_language_processing": "Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.",
    "Virtual_reality": "Virtual reality (VR) is a simulated experience that can be similar to or completely different from the real world. Applications of virtual reality can include entertainment (i.e. video games) and educational purposes (i.e. medical or military training). Other, distinct types of VR style technology include augmented reality and mixed reality. Currently standard virtual reality systems use either virtual reality headsets or multi-projected environments to generate realistic images, sounds and other sensations that simulate a user's physical presence in a virtual environment. A person using virtual reality equipment is able to look around the artificial world, move around in it, and interact with virtual features or items. The effect is commonly created by VR headsets consisting of a head-mounted display with a small screen in front of the eyes, but can also be created through specially designed rooms with multiple large screens. Virtual reality typically incorporates auditory and video feedback, but may also allow other types of sensory and force feedback through haptic technology.",
    "Greek_language": "Greek (: \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac, romanized: Ellinik\u00e1) is an independent branch of the Indo-European family of languages, native to Greece, Cyprus, Albania and other parts of the Eastern Mediterranean and the Black Sea. It has the longest documented history of any living Indo-European language, spanning at least 3,500 years of written records. Its writing system has been the Greek alphabet for the major part of its history; other systems, such as Linear B and the Cypriot syllabary, were used previously. The alphabet arose from the Phoenician script and was in turn the basis of the Latin, Cyrillic, Armenian, Coptic, Gothic, and many other writing systems. The Greek language holds an important place in the history of the Western world and Christianity; the canon of ancient Greek literature includes works in the Western canon such as the epic poems Iliad and Odyssey. Greek is also the language in which many of the foundational texts in science, especially astronomy, mathematics and logic and Western philosophy, such as the Platonic dialogues and the works of Aristotle, are composed; the New Testament of the Christian Bible was written in Koin\u00e9 Greek. Together with the Latin texts and traditions of the Roman world, the study of the Greek texts and society of antiquity constitutes the discipline of Classics. During antiquity, Greek was a widely spoken lingua franca in the Mediterranean world, West Asia and many places beyond. It would eventually become the official parlance of the Byzantine Empire and develop into Medieval Greek. In its modern form, Greek is the official language in two countries, Greece and Cyprus, a recognized minority language in seven other countries, and is one of the 24 official languages of the European Union. The language is spoken by at least 13.4 million people today in Greece, Cyprus, Italy, Albania, and Turkey and by the Greek diaspora. Greek roots are often used to coin new words for other languages; Greek and Latin are the predominant sources of international scientific vocabulary.",
    "Systems_architecture": "A system architecture is the conceptual model that defines the structure, behavior, and more views of a system. An architecture description is a formal description and representation of a system, organized in a way that supports reasoning about the structures and behaviors of the system. A system architecture can consist of system components and the sub-systems developed, that will work together to implement the overall system. There have been efforts to formalize languages to describe system architecture, collectively these are called architecture description languages (ADLs).",
    "Computational_physics": "Computational physics is the study and implementation of numerical analysis to solve problems in physics for which a quantitative theory already exists. Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science. It is sometimes regarded as a subdiscipline (or offshoot) of theoretical physics, but others consider it an intermediate branch between theoretical and experimental physics - an area of study which supplements both theory and experiment.",
    "Point-contact_transistor": "The point-contact transistor was the first type of transistor to be successfully demonstrated. It was developed by research scientists John Bardeen and Walter Brattain at Bell Laboratories in December 1947. They worked in a group led by physicist William Shockley. The group had been working together on experiments and theories of electric field effects in solid state materials, with the aim of replacing vacuum tubes with a smaller device that consumed less power. The critical experiment, carried out on December 16, 1947, consisted of a block of germanium, a semiconductor, with two very closely spaced gold contacts held against it by a spring. Brattain attached a small strip of gold foil over the point of a plastic triangle \u2014 a configuration which is essentially a point-contact diode. He then carefully sliced through the gold at the tip of the triangle. This produced two electrically isolated gold contacts very close to each other. The piece of germanium used had a surface layer with an excess of electrons. When an electric signal traveled in through the gold foil, it injected holes (points which lack electrons). This created a thin layer which had a scarcity of electrons. A small positive current applied to one of the two contacts had an influence on the current which flowed between the other contact and the base upon which the block of germanium was mounted. In fact, a small change in the first contact current caused a greater change in the second contact current, thus it was an amplifier. The first contact is the \"emitter\" and the second contact is the \"collector\". The low-current input terminal into the point-contact transistor is the emitter, while the output high current terminals are the base and collector. This differs from the later type of bipolar junction transistor invented in 1951 that operates as transistors still do, with the low current input terminal as the base and the two high current output terminals are the emitter and collector. The point-contact transistor was commercialized and sold by Western Electric and others but was soon superseded by the bipolar junction transistor, which was easier to manufacture and more rugged.",
    "Computational_geometry": "Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with history stretching back to antiquity. Computational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. For such sets, the difference between O(n2) and O(n log n) may be the difference between days and seconds of computation. The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization. Other important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), computer vision (3D reconstruction). The main branches of computational geometry are: \n* Combinatorial computational geometry, also called algorithmic geometry, which deals with geometric objects as discrete entities. A groundlaying book in the subject by Preparata and Shamos dates the first use of the term \"computational geometry\" in this sense by 1975. \n* Numerical computational geometry, also called machine geometry, computer-aided geometric design (CAGD), or geometric modeling, which deals primarily with representing real-world objects in forms suitable for computer computations in CAD/CAM systems. This branch may be seen as a further development of descriptive geometry and is often considered a branch of computer graphics or CAD. The term \"computational geometry\" in this meaning has been in use since 1971.",
    "Social_intelligence": "Social intelligence is the capacity to know oneself and to know others. Social Intelligence develops from experience with people and learning from success and failures in social settings. It is more commonly referred to as \"tact,\" \"Common sense,\" or \"street smarts.\" Social scientist Ross Honeywill believes social intelligence is an aggregated measure of self- and social-awareness, evolved social beliefs and attitudes, and a capacity and appetite to manage complex social change. Psychologist, Nicholas Humphrey believes that it is social intelligence, rather than quantitative intelligence, that defines who we are as humans. The original definition by Edward Thorndike in 1920 is \"the ability to understand and manage men and women and boys and girls, to act wisely in human relations\". It is equivalent to interpersonal intelligence, one of the types of intelligence identified in Howard Gardner's theory of multiple intelligences, and closely related to theory of mind. Some authors have restricted the definition to deal only with knowledge of social situations, perhaps more properly called social cognition or social marketing intelligence, as it pertains to trending socio-psychological advertising and marketing strategies and tactics. According to Sean Foleno, social intelligence is a person's competence to optimally understand one's environment and react appropriately for socially successful conduct. It is important to note the multiple definitions listed above, as there is yet to be a complete consensus on the operational definition of social intelligence.Babu M (2013) pointed out the major four dimensions of social intelligence viz., Me-Identity, Web-ironment, Social Inputting, and Empathetic Co-operation. These factors together form the social intelligence of an organism.",
    "Processor_design": "Processor design is the design engineering task of creating a processor, a key component of computer hardware. It is a subfield of computer engineering (design, development and implementation) and electronics engineering (fabrication). The design process involves choosing an instruction set and a certain execution paradigm (e.g. VLIW or RISC) and results in a microarchitecture, which might be described in e.g. VHDL or Verilog. For microprocessor design, this description is then manufactured employing some of the various semiconductor device fabrication processes, resulting in a die which is bonded onto a chip carrier. This chip carrier is then soldered onto, or inserted into a socket on, a printed circuit board (PCB). The mode of operation of any processor is the execution of lists of instructions. Instructions typically include those to compute or manipulate data values using registers, change or retrieve values in read/write memory, perform relational tests between data values and to control program flow.",
    "Neurophysiology": "Neurophysiology (from Greek \u03bd\u03b5\u1fe6\u03c1\u03bf\u03bd, neuron, \"nerve\"; \u03c6\u03cd\u03c3\u03b9\u03c2, physis, \"nature, origin\"; and -\u03bb\u03bf\u03b3\u03af\u03b1, -logia, \"knowledge\") is a branch of physiology and neuroscience that is concerned with the study of the functioning of the nervous system. The primary tools of basic neurophysiological research include electrophysiological recordings, such as patch clamp, voltage clamp, extracellular single-unit recording and recording of local field potentials, as well as some of the methods of calcium imaging, optogenetics, and molecular biology. Neurophysiology is related to electrophysiology, neuroanatomy, neurochemistry, biophysics, and mathematical neuroscience. It also has medical applications in clinical neurophysiology and clinical neuroscience.",
    "Jacquard_machine": "The Jacquard machine (French: [\u0292aka\u0281]) is a device fitted to a loom that simplifies the process of manufacturing textiles with such complex patterns as brocade, damask and matelass\u00e9. It was invented by Joseph Marie Jacquard in 1804, based on earlier inventions by the Frenchmen Basile Bouchon (1725), Jean Baptiste Falcon (1728), and Jacques Vaucanson (1740). The machine was controlled by a \"chain of cards\"; a number of punched cards laced together into a continuous sequence. Multiple rows of holes were punched on each card, with one complete card corresponding to one row of the design. Several such paper cards, generally white in color, can be seen in the images below. Chains, like Bouchon's earlier use of paper tape, allowed sequences of any length to be constructed, not limited by the size of a card. Both the Jacquard process and the necessary loom attachment are named after their inventor. This mechanism is probably one of the most important weaving inventions as Jacquard shedding made possible the automatic production of unlimited varieties of pattern weaving. The term \"Jacquard\" is not specific or limited to any particular loom, but rather refers to the added control mechanism that automates the patterning. The process can also be used for patterned knitwear and machine-knitted textiles, such as jerseys. This use of replaceable punched cards to control a sequence of operations is considered an important step in the history of computing hardware.",
    "Semiotics": "Semiotics (also called semiotic studies) is the study of sign process (semiosis), which is any form of activity, conduct, or any process that involves signs, including the production of meaning. A sign is anything that communicates a meaning, that is not the sign itself, to the interpreter of the sign. The meaning can be intentional such as a word uttered with a specific meaning, or unintentional, such as a symptom being a sign of a particular medical condition. Signs can communicate through any of the senses, visual, auditory, tactile, olfactory, or gustatory. The semiotic tradition explores the study of signs and symbols as a significant part of communications. Unlike linguistics, semiotics also studies non-linguistic sign systems. Semiotics includes the study of signs and sign processes, indication, designation, likeness, analogy, allegory, metonymy, metaphor, symbolism, signification, and communication. Semiotics is frequently seen as having important anthropological and sociological dimensions; for example, the Italian semiotician and novelist Umberto Eco proposed that every cultural phenomenon may be studied as communication. Some semioticians focus on the logical dimensions of the science, however. They examine areas belonging also to the life sciences\u2014such as how organisms make predictions about, and adapt to, their semiotic niche in the world (see semiosis). In general, semiotic theories take signs or sign systems as their object of study: the communication of information in living organisms is covered in biosemiotics (including zoosemiotics and phytosemiotics). Semiotics is not to be confused with the Saussurean tradition called semiology, which is a subset of semiotics.",
    "Speech_synthesis": "Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech. Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely \"synthetic\" voice output. The quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with visual impairments or reading disabilities to listen to written words on a home computer. Many computer operating systems have included speech synthesizers since the early 1990s. A text-to-speech system (or \"engine\") is composed of two parts: a front-end and a back-end. The front-end has two major tasks. First, it converts raw text containing symbols like numbers and abbreviations into the equivalent of written-out words. This process is often called text normalization, pre-processing, or tokenization. The front-end then assigns phonetic transcriptions to each word, and divides and marks the text into prosodic units, like phrases, clauses, and sentences. The process of assigning phonetic transcriptions to words is called text-to-phoneme or grapheme-to-phoneme conversion. Phonetic transcriptions and prosody information together make up the symbolic linguistic representation that is output by the front-end. The back-end\u2014often referred to as the synthesizer\u2014then converts the symbolic linguistic representation into sound. In certain systems, this part includes the computation of the target prosody (pitch contour, phoneme durations), which is then imposed on the output speech.",
    "Multiprocessing": "Multiprocessing is the use of two or more central processing units (CPUs) within a single computer system. The term also refers to the ability of a system to support more than one processor or the ability to allocate tasks between them. There are many variations on this basic theme, and the definition of multiprocessing can vary with context, mostly as a function of how CPUs are defined (multiple cores on one die, multiple dies in one package, multiple packages in one system unit, etc.). According to some on-line dictionaries, a multiprocessor is a computer system having two or more processing units (multiple processors) each sharing main memory and peripherals, in order to simultaneously process programs. A 2009 textbook defined multiprocessor system similarly, but noting that the processors may share \"some or all of the system\u2019s memory and I/O facilities\"; it also gave tightly coupled system as a synonymous term. At the operating system level, multiprocessing is sometimes used to refer to the execution of multiple concurrent processes in a system, with each process running on a separate CPU or core, as opposed to a single process at any one instant. When used with this definition, multiprocessing is sometimes contrasted with multitasking, which may use just a single processor but switch it in time slices between tasks (i.e. a time-sharing system). Multiprocessing however means true parallel execution of multiple processes using more than one processor. Multiprocessing doesn't necessarily mean that a single process or task uses more than one processor simultaneously; the term parallel processing is generally used to denote that scenario. Other authors prefer to refer to the operating system techniques as multiprogramming and reserve the term multiprocessing for the hardware aspect of having more than one processor. The remainder of this article discusses multiprocessing only in this hardware sense. In Flynn's taxonomy, multiprocessors as defined above are MIMD machines. As the term \"multiprocessor\" normally refers to tightly coupled systems in which all processors share memory, multiprocessors are not the entire class of MIMD machines, which also contains message passing multicomputer systems.",
    "Microarchitecture": "In computer engineering, microarchitecture, also called computer organization and sometimes abbreviated as \u00b5arch or uarch, is the way a given instruction set architecture (ISA) is implemented in a particular processor. A given ISA may be implemented with different microarchitectures; implementations may vary due to different goals of a given design or due to shifts in technology. Computer architecture is the combination of microarchitecture and instruction set architecture.",
    "Computation": "A computation is any type of calculation that includes both arithmetical and non-arithmetical steps and which follows a well-defined model (e.g. an algorithm). Mechanical or electronic devices (or, historically, people) that perform computations are known as computers. An especially well-known discipline of the study of computation is computer science.",
    "Computational_neuroscience": "Computational neuroscience (also known as theoretical neuroscience or mathematical neuroscience) is a branch of neuroscience which employs mathematical models, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, physiology and cognitive abilities of the nervous system. In theory, computational neuroscience would be a sub-field of theoretical neuroscience which employs computational simulations to validate and solve the mathematical models. However, since the biologically plausible mathematical models formulated in neuroscience are in most cases too complex to be solved analytically, the two terms are essentially synonyms and are used interchangeably. The term mathematical neuroscience is also used sometimes, to stress the quantitative nature of the field. Computational neuroscience focuses on the description of biologically plausible neurons (and neural systems) and their physiology and dynamics, and it is therefore not directly concerned with biologically unrealistic models used in connectionism, control theory, cybernetics, quantitative psychology, machine learning, artificial neural networks, artificial intelligence and computational learning theory; although mutual inspiration exists and sometimes there is no strict limit between fields, with model abstraction in computational neuroscience depending on research scope and the granularity at which biological entities are analyzed. Models in theoretical neuroscience are aimed at capturing the essential features of the biological system at multiple spatial-temporal scales, from membrane currents, and chemical coupling via network oscillations, columnar and topographic architecture, nuclei, all the way up to psychological faculties like memory, learning and behavior. These computational models frame hypotheses that can be directly tested by biological or psychological experiments.",
    "Models_of_neural_computation": "Models of neural computation are attempts to elucidate, in an abstract and mathematical fashion, the core principles that underlie information processing in biological nervous systems, or functional components thereof. This article aims to provide an overview of the most definitive models of neuro-biological computation as well as the tools commonly used to construct and analyze them.",
    "Formal_specification": "In computer science, formal specifications are mathematically based techniques whose purpose are to help with the implementation of systems and software. They are used to describe a system, to analyze its behavior, and to aid in its design by verifying key properties of interest through rigorous and effective reasoning tools. These specifications are formal in the sense that they have a syntax, their semantics fall within one domain, and they are able to be used to infer useful information.",
    "Formal_verification": "In the context of hardware and software systems, formal verification is the act of proving or disproving the correctness of intended algorithms underlying a system with respect to a certain formal specification or property, using formal methods of mathematics. Formal verification can be helpful in proving the correctness of systems such as: cryptographic protocols, combinational circuits, digital circuits with internal memory, and software expressed as source code. The verification of these systems is done by providing a formal proof on an abstract mathematical model of the system, the correspondence between the mathematical model and the nature of the system being otherwise known by construction. Examples of mathematical objects often used to model systems are: finite state machines, labelled transition systems, Petri nets, vector addition systems, timed automata, hybrid automata, process algebra, formal semantics of programming languages such as operational semantics, denotational semantics, axiomatic semantics and Hoare logic.",
    "Cambridge_University_Press": "Cambridge University Press is the publishing business of the University of Cambridge. Granted letters patent by King Henry VIII in 1534, it is the oldest university press in the world. It is also the Queen's Printer. Cambridge University Press is a department of the University of Cambridge and is both an academic and educational publisher. With a global sales presence, publishing hubs, and offices in more than 40 countries, it publishes over 50,000 titles by authors from over 100 countries. Its publishing includes more than 380 academic journals, monographs, reference works, school and university textbooks, and English language teaching and learning publications. It also publishes Bibles, runs a bookshop in Cambridge, sells through Amazon, and has small conference venues business in Cambridge, with facilities at the Pitt Building and the Sir Geoffrey Cass Sports and Social Centre. Being part of the University of Cambridge gives the Press a non-profit status for most of its activities, thereby not having to pay corporation tax. Cambridge University Press transfers a minimum of 30% of any annual surplus back to the University of Cambridge.",
    "Informatics": "Informatics applies the principles of information science to solve problems using data. It involves the practice of information processing and the engineering of information systems. The field considers the interaction between humans and information alongside the construction of interfaces, organisations, technologies and systems. As such, informatics encompasses many academic disciplines, including computer science, information systems, information technology and statistics. Since the advent of computers, individuals and organizations increasingly process information digitally. This has led to the study of informatics with computational, mathematical, biological, cognitive and social aspects, including study of the social impact of information technologies.",
    "Computer_graphics_(computer_science)": "Computer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Although the term often refers to the study of three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing.",
    "Turing_test": "The Turing test, developed by Alan Turing in 1950, is a test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human. Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses. The evaluator would be aware that one of the two partners in conversation is a machine, and all participants would be separated from one another. The conversation would be limited to a text-only channel such as a computer keyboard and screen so the result would not depend on the machine's ability to render words as speech. If the evaluator cannot reliably tell the machine from the human, the machine is said to have passed the test. The test results do not depend on the machine's ability to give correct answers to questions, only how closely its answers resemble those a human would give. The test was introduced by Turing in his 1950 paper, \"Computing Machinery and Intelligence\", while working at the University of Manchester (Turing, 1950; p. 460). It opens with the words: \"I propose to consider the question, 'Can machines think?'\" Because \"thinking\" is difficult to define, Turing chooses to \"replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.\" Turing describes the new form of the problem in terms of a three-person game called the \"imitation game\", in which an interrogator asks questions of a man and a woman in another room in order to determine the correct sex of the two players. Turing's new question is: \"Are there imaginable digital computers which would do well in the imitation game?\" This question, Turing believed, is one that can actually be answered. In the remainder of the paper, he argued against all the major objections to the proposition that \"machines can think\". Since Turing first introduced his test, it has proven to be both highly influential and widely criticised, and it has become an important concept in the philosophy of artificial intelligence. Some of these criticisms, such as John Searle's Chinese room, are controversial in their own right.",
    "New_York_City": "New York City, often called simply New York and abbreviated as NYC, is the most populous city in the United States. With an estimated 2019 population of 8,336,817 distributed over about 302.6 square miles (784 km2), New York is also the most densely populated major city in the United States. Located at the southern tip of the U.S. state of New York, the city is the center of the New York metropolitan area, the largest metropolitan area in the world by urban landmass. With almost 20 million people in its metropolitan statistical area and approximately 23 million in its combined statistical area, it is one of the world's most populous megacities. New York City has been described as the cultural, financial, and media capital of the world, significantly influencing commerce, entertainment, research, technology, education, politics, tourism, art, fashion, and sports. Home to the headquarters of the United Nations, New York is an important center for international diplomacy. Situated on one of the world's largest natural harbors, New York City is composed of five boroughs, each of which is a county of the State of New York. The five boroughs\u2014Brooklyn, Queens, Manhattan, the Bronx, and Staten Island\u2014were consolidated into a single city in 1898. The city and its metropolitan area constitute the premier gateway for legal immigration to the United States. As many as 800 languages are spoken in New York, making it the most linguistically diverse city in the world. New York is home to more than 3.2 million residents born outside the United States, the largest foreign-born population of any city in the world as of 2016. As of 2019, the New York metropolitan area is estimated to produce a gross metropolitan product (GMP) of $2.0 trillion. If the New York metropolitan area were a sovereign state, it would have the eighth-largest economy in the world. New York is home to the highest number of billionaires of any city in the world. New York City traces its origins to a trading post founded by colonists from the Dutch Republic in 1624 on Lower Manhattan; the post was named New Amsterdam in 1626. The city and its surroundings came under English control in 1664 and were renamed New York after King Charles II of England granted the lands to his brother, the Duke of York. New York was the capital of the United States from 1785 until 1790, and has been the largest U.S. city since 1790. The Statue of Liberty greeted millions of immigrants as they came to the U.S. by ship in the late 19th and early 20th centuries, and is a symbol of the U.S. and its ideals of liberty and peace. In the 21st century, New York has emerged as a global node of creativity, entrepreneurship, and environmental sustainability, and as a symbol of freedom and cultural diversity. In 2019, New York was voted the greatest city in the world per a survey of over 30,000 people from 48 cities worldwide, citing its cultural diversity. Many districts and landmarks in New York City are well known, including three of the world's ten most visited tourist attractions in 2013. A record 62.8 million tourists visited New York City in 2017. Times Square is the brightly illuminated hub of the Broadway Theater District, one of the world's busiest pedestrian intersections, and a major center of the world's entertainment industry. Many of the city's landmarks, skyscrapers, and parks are known around the world. Manhattan's real estate market is among the most expensive in the world. New York is home to the largest ethnic Chinese population outside of Asia, with multiple distinct Chinatowns across the city. Providing continuous 24/7 service and contributing to the nickname The City that Never Sleeps, the New York City Subway is the largest single-operator rapid transit system worldwide, with 472 rail stations. The city has over 120 colleges and universities, including Columbia University, New York University, Rockefeller University, and the City University of New York system, which is the largest urban public university system in the United States. Anchored by Wall Street in the Financial District of Lower Manhattan, New York City has been called both the world\u2019s leading financial center and the most financially powerful city in the world, and is home to the world's two largest stock exchanges by total market capitalization, the New York Stock Exchange and NASDAQ.",
    "Difference_engine": "The historical difficulty in producing error-free tables by teams of mathematicians and human \"computers\" spurred Charles Babbage's desire to build a mechanism to automate the process.",
    "Quantum_computing": "Quantum computing is the use of quantum-mechanical phenomena such as superposition and entanglement to perform computation. Computers that perform quantum computations are known as quantum computers. Quantum computers are believed to be able to solve certain computational problems, such as integer factorization (which underlies RSA encryption), substantially faster than classical computers. The study of quantum computing is a subfield of quantum information science. Quantum computing began in the early 1980s, when physicist Paul Benioff proposed a quantum mechanical model of the Turing machine. Richard Feynman and Yuri Manin later suggested that a quantum computer had the potential to simulate things that a classical computer could not. In 1994, Peter Shor developed a quantum algorithm for factoring integers that had the potential to decrypt RSA-encrypted communications. Despite ongoing experimental progress since the late 1990s, most researchers believe that \"fault-tolerant quantum computing [is] still a rather distant dream.\" In recent years, investment into quantum computing research has increased in both the public and private sector. On 23 October 2019, Google AI, in partnership with the U.S. National Aeronautics and Space Administration (NASA), claimed to have performed a quantum computation that is infeasible on any classical computer. There are several models of quantum computing, including the quantum circuit model, quantum Turing machine, adiabatic quantum computer, one-way quantum computer, and various quantum cellular automata. The most widely used model is the quantum circuit. Quantum circuits are based on the quantum bit, or \"qubit\", which is somewhat analogous to the bit in classical computation. Qubits can be in a 1 or 0 quantum state, or they can be in a superposition of the 1 and 0 states. However, when qubits are measured the result of the measurement is always either a 0 or a 1; the probabilities of these two outcomes depend on the quantum state that the qubits were in immediately prior to the measurement. Computation is performed by manipulating qubits with quantum logic gates, which are somewhat analogous to classical logic gates. There are currently two main approaches to physically implementing a quantum computer: analog and digital. Analog approaches are further divided into quantum simulation, quantum annealing, and adiabatic quantum computation. Digital quantum computers use quantum logic gates to do computation. Both approaches use quantum bits or qubits. There are currently a number of significant obstacles in the way of constructing useful quantum computers. In particular, it is difficult to maintain the quantum states of qubits as they are prone to quantum decoherence, and quantum computers require significant error correction as they are far more prone to errors than classical computers. Any computational problem that can be solved by a classical computer can also, in principle, be solved by a quantum computer. Conversely, quantum computers obey the Church\u2013Turing thesis; that is, any computational problem that can be solved by a quantum computer can also be solved by a classical computer. While this means that quantum computers provide no additional advantages over classical computers in terms of computability, they do in theory enable the design of algorithms for certain problems that have significantly lower time complexities than known classical algorithms. Notably, quantum computers are believed to be able to quickly solve certain problems that no classical computer could solve in any feasible amount of time\u2014a feat known as \"quantum supremacy.\" The study of the computational complexity of problems with respect to quantum computers is known as quantum complexity theory.",
    "Operating_system": "An operating system (OS) is system software that manages computer hardware, software resources, and provides common services for computer programs. Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources. For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer \u2013 from cellular phones and video game consoles to web servers and supercomputers. The dominant desktop operating system is Microsoft Windows with a market share of around 82.74%. macOS by Apple Inc. is in second place (13.23%), and the varieties of Linux are collectively in third place (1.57%). In the mobile sector (including smartphones and tablets), Android's share is up to 70% in the year 2017. According to third quarter 2016 data, Android's share on smartphones is dominant with 87.5 percent with also a growth rate of 10.3 percent per year, followed by Apple's iOS with 12.1 percent with per year decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent. Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems, such as embedded and real-time systems, exist for many applications.",
    "Analytical_Engine": "The Analytical Engine was a proposed mechanical general-purpose computer designed by English mathematician and computer pioneer Charles Babbage. It was first described in 1837 as the successor to Babbage's difference engine, a design for a simpler mechanical computer. The Analytical Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete. In other words, the logical structure of the Analytical Engine was essentially the same as that which has dominated computer design in the electronic era. The Analytical Engine is one of the most successful achievements of Charles Babbage. Babbage was never able to complete construction of any of his machines due to conflicts with his chief engineer and inadequate funding. It was not until 1941 that the first general-purpose computer, Z3, was built, more than a century after Babbage had proposed the pioneering Analytical Engine in 1837.",
    "Theoretical_computer_science": "Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on more mathematical topics of computing, and includes the theory of computation. It is difficult to circumscribe the theoretical areas precisely. The ACM's Special Interest Group on Algorithms and Computation Theory (SIGACT) provides the following description: TCS covers a wide variety of topics including algorithms, data structures, computational complexity, parallel and distributed computation, probabilistic computation, quantum computation, automata theory, information theory, cryptography, program semantics and verification, machine learning, computational biology, computational economics, computational geometry, and computational number theory and algebra. Work in this field is often distinguished by its emphasis on mathematical technique and rigor.",
    "Computational_logic": "Computational logic is the use of logic to perform or reason about computation. It bears a similar relationship to computer science and engineering as mathematical logic bears to mathematics and as philosophical logic bears to philosophy. It is synonymous with \"logic in computer science\". The term \u201cComputational Logic\u201d came to prominence with the founding of the ACM Transactions on Computational Logic. However, the term was apparently introduced by J.A. Robinson in a 1970 paper in the Proceedings of the Sixth Annual Machine Intelligence Workshop, Edinburgh, 1970, entitled \"Computational Logic: The Unification Computation\" (Machine Intelligence 6:63-72, Edinburgh University Press, 1971). The expression is used in the second paragraph with a footnote claiming that *computational logic* (the emphasis is in the paper) is \"surely a better phrase than 'theorem proving', for the branch of artificial intelligence which deals with how to make machines do deduction efficiently\". This sounds like coining the term; no reference to a previous use is mentioned. In 1972 the Metamathematics Unit at the University of Edinburgh was renamed \u201cThe Department of Computational Logic\u201d in the School of Artificial Intelligence. The term was then used by Robert S. Boyer and J Strother Moore, who worked in the Department in the early 1970s, to describe their work on program verification and automated reasoning. They also founded Computational Logic Inc. The term \u201cComputational Logic\u201d has also come to be associated with logic programming, because much of the early work in logic programming in the early 1970s also took place in the Department of Computational Logic in Edinburgh. It was reused in the early 1990s to describe work on extensions of logic programming in the EU Basic Research Project \"Compulog\" and in the associated Network of Excellence. Krzysztof Apt, who was the co-ordinator of the Basic Research Project Compulog-II, reused and generalized the term when he founded the ACM Transactions on Computational Logic in 2000 and became its first Editor-in-Chief.",
    "Charles_Babbage": "Charles Babbage  (; 26 December 1791 \u2013 18 October 1871) was an English polymath. A mathematician, philosopher, inventor and mechanical engineer, Babbage originated the concept of a digital programmable computer. Considered by some to be \"father of the computer\", Babbage is credited with inventing the first mechanical computer that eventually led to more complex electronic designs, though all the essential ideas of modern computers are to be found in Babbage's Analytical Engine. His varied work in other fields has led him to be described as \"pre-eminent\" among the many polymaths of his century. Parts of Babbage's incomplete mechanisms are on display in the Science Museum in London. In 1991, a functioning difference engine was constructed from Babbage's original plans. Built to tolerances achievable in the 19th century, the success of the finished engine indicated that Babbage's machine would have worked.",
    "Alan_Turing": "Alan Mathison Turing  (; 23 June 1912 \u2013 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer. Turing is widely considered to be the father of theoretical computer science and artificial intelligence. Despite these accomplishments, he was never fully recognised in his home country during his lifetime due to his homosexuality and because much of his work was covered by the Official Secrets Act. During the Second World War, Turing worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the section that was responsible for German naval cryptanalysis. Here, he devised a number of techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bombe method, an electromechanical machine that could find settings for the Enigma machine. Turing played a crucial role in cracking intercepted coded messages that enabled the Allies to defeat the Nazis in many crucial engagements, including the Battle of the Atlantic, and in so doing helped win the war. Due to the problems of counterfactual history, it is hard to estimate the precise effect Ultra intelligence had on the war, but at the upper end it has been estimated that this work shortened the war in Europe by more than two years and saved over 14 million lives. After the war Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine. The Automatic Computing Engine was one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory, at the Victoria University of Manchester, where he helped develop the Manchester computers and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis and predicted oscillating chemical reactions such as the Belousov\u2013Zhabotinsky reaction, first observed in the 1960s. Turing was prosecuted in 1952 for homosexual acts; the Labouchere Amendment of 1885 had mandated that \"gross indecency\" was a criminal offence in the UK. He accepted chemical castration treatment, with DES, as an alternative to prison. Turing died in 1954, 16 days before his 42nd birthday, from cyanide poisoning. An inquest determined his death as a suicide, but it has been noted that the known evidence is also consistent with accidental poisoning. In 2009, following an Internet campaign, British Prime Minister Gordon Brown made an  on behalf of the British government for \"the appalling way he was treated\". Queen Elizabeth II granted Turing a posthumous pardon in 2013. The \"Alan Turing law\" is now an informal term for a 2017 law in the United Kingdom that retroactively pardoned men cautioned or convicted under historical legislation that outlawed homosexual acts.",
    "Atanasoff\u2013Berry_computer": "The Atanasoff\u2013Berry computer (ABC) was the first automatic electronic digital computer. Limited by the technology of the day, and execution, the device has remained somewhat obscure. The ABC's priority is debated among historians of computer technology, because it was neither programmable, nor Turing-complete, unlike the widely famous ENIAC machine of 1947 in part derived from it.",
    "History_of_personal_computers": "The history of the personal computer as a mass-market consumer electronic device began with the microcomputer revolution of the 1970s. A personal computer is one intended for interactive individual use, as opposed to a mainframe computer where the end user's requests are filtered through operating staff, or a time-sharing system in which one large processor is shared by many individuals. After the development of the microprocessor, individual personal computers were low enough in cost that they eventually became affordable consumer goods. Early personal computers \u2013 generally called microcomputers \u2013 were sold often in electronic kit form and in limited numbers, and were of interest mostly to hobbyists and technicians.",
    "Cognitive_science": "Cognitive science is the interdisciplinary, scientific study of the mind and its processes. It examines the nature, the tasks, and the functions of cognition (in a broad sense). Cognitive scientists study intelligence and behavior, with a focus on how nervous systems represent, process, and transform information. Mental faculties of concern to cognitive scientists include language, perception, memory, attention, reasoning, and emotion; to understand these faculties, cognitive scientists borrow from fields such as linguistics, psychology, artificial intelligence, philosophy, neuroscience, and anthropology. The typical analysis of cognitive science spans many levels of organization, from learning and decision to logic and planning; from neural circuitry to modular brain organization. One of the fundamental concepts of cognitive science is that \"thinking can best be understood in terms of representational structures in the mind and computational procedures that operate on those structures.\" Cognitive science is the interdisciplinary study of cognition in humans, animals, and machines. It encompasses the traditional disciplines of psychology, computer science, neuroscience, anthropology, linguistics and philosophy. The goal of cognitive science is to understand the principles of intelligence with the hope that this will lead to better comprehension of the mind and of learning and to develop intelligent devices.The cognitive sciences began as an intellectual movement in the 1950s often referred to as the cognitive revolution.",
    "2D_computer_graphics": "2D computer graphics is the computer-based generation of digital images\u2014mostly from two-dimensional models (such as 2D geometric models, text, and digital images) and by techniques specific to them. The word may stand for the branch of computer science that comprises such techniques or for the models themselves. 2D computer graphics are mainly used in applications that were originally developed upon traditional printing and drawing technologies, such as typography, cartography, technical drawing, advertising, etc. In those applications, the two-dimensional image is not just a representation of a real-world object, but an independent artifact with added semantic value; two-dimensional models are therefore preferred, because they give more direct control of the image than 3D computer graphics (whose approach is more akin to photography than to typography). In many domains, such as desktop publishing, engineering, and business, a description of a document based on 2D computer graphics techniques can be much smaller than the corresponding digital image\u2014often by a factor of 1/1000 or more. This representation is also more flexible since it can be rendered at different resolutions to suit different output devices. For these reasons, documents and illustrations are often stored or transmitted as 2D graphic files. 2D computer graphics started in the 1950s, based on vector graphics devices. These were largely supplanted by raster-based devices in the following decades. The PostScript language and the X Window System protocol were landmark developments in the field.",
    "Interdisciplinarity": "Interdisciplinarity or interdisciplinary studies involves the combining of two or more academic disciplines into one activity (e.g., a research project). It draws knowledge from several other fields like sociology, anthropology, psychology, economics etc. It is about creating something by thinking across boundaries. It is related to an interdiscipline or an interdisciplinary field, which is an organizational unit that crosses traditional boundaries between academic disciplines or schools of thought, as new needs and professions emerge. Large engineering teams are usually interdisciplinary, as a power station or mobile phone or other project requires the melding of several specialties. However, the term \"interdisciplinary\" is sometimes confined to academic settings. The term interdisciplinary is applied within education and training pedagogies to describe studies that use methods and insights of several established disciplines or traditional fields of study. Interdisciplinarity involves researchers, students, and teachers in the goals of connecting and integrating several academic schools of thought, professions, or technologies\u2014along with their specific perspectives\u2014in the pursuit of a common task. The epidemiology of HIV/AIDS or global warming requires understanding of diverse disciplines to solve complex problems. Interdisciplinary may be applied where the subject is felt to have been neglected or even misrepresented in the traditional disciplinary structure of research institutions, for example, women's studies or ethnic area studies. Interdisciplinarity can likewise be applied to complex subjects that can only be understood by combining the perspectives of two or more fields. The adjective interdisciplinary is most often used in educational circles when researchers from two or more disciplines pool their approaches and modify them so that they are better suited to the problem at hand, including the case of the team-taught course where students are required to understand a given subject in terms of multiple traditional disciplines. For example, the subject of land use may appear differently when examined by different disciplines, for instance, biology, chemistry, economics, geography, and politics.",
    "Computer_security": "Computer security, cybersecurity or information technology security (IT security) is the protection of computer systems and networks from the theft of or damage to their hardware, software, or electronic data, as well as from the disruption or misdirection of the services they provide. The field is becoming more important due to increased reliance on computer systems, the Internet and wireless network standards such as Bluetooth and Wi-Fi, and due to the growth of \"smart\" devices, including smartphones, televisions, and the various devices that constitute the \"Internet of things\". Owing to its complexity, both in terms of politics and technology, cybersecurity is also one of the major challenges in the contemporary world.",
    "Software_development": "Software development is the process of conceiving, specifying, designing, programming, documenting, testing, and bug fixing involved in creating and maintaining applications, frameworks, or other software components. Software development is a process of writing and maintaining the source code, but in a broader sense, it includes all that is involved between the conception of the desired software through to the final manifestation of the software, sometimes in a planned and structured process. Therefore, software development may include research, new development, prototyping, modification, reuse, re-engineering, maintenance, or any other activities that result in software products. The software can be developed for a variety of purposes, the three most common being to meet specific needs of a specific client/business (the case with custom software), to meet a perceived need of some set of potential users (the case with commercial and open source software), or for personal use (e.g. a scientist may write software to automate a mundane task). Embedded software development, that is, the development of embedded software, such as used for controlling consumer products, requires the development process to be integrated with the development of the controlled physical product. System software underlies applications and the programming process itself, and is often developed separately. The need for better quality control of the software development process has given rise to the discipline of software engineering, which aims to apply the systematic approach exemplified in the engineering paradigm to the process of software development. There are many approaches to software project management, known as software development life cycle models, methodologies, processes, or models. The waterfall model is a traditional version, contrasted with the more recent innovation of agile software development.",
    "Embedded_system": "An embedded system is a computer system\u2014a combination of a computer processor, computer memory, and input/output peripheral devices\u2014that has a dedicated function within a larger mechanical or electrical system. It is embedded as part of a complete device often including electrical or electronic hardware and mechanical parts. Because an embedded system typically controls physical operations of the machine that it is embedded within, it often has real-time computing constraints. Embedded systems control many devices in common use today. Ninety-eight percent of all microprocessors manufactured are used in embedded systems. Modern embedded systems are often based on microcontrollers (i.e. microprocessors with integrated memory and peripheral interfaces), but ordinary microprocessors (using external chips for memory and peripheral interface circuits) are also common, especially in more complex systems. In either case, the processor(s) used may be types ranging from general purpose to those specialized in a certain class of computations, or even custom designed for the application at hand. A common standard class of dedicated processors is the digital signal processor (DSP). Since the embedded system is dedicated to specific tasks, design engineers can optimize it to reduce the size and cost of the product and increase the reliability and performance. Some embedded systems are mass-produced, benefiting from economies of scale. Embedded systems range from portable devices such as digital watches and MP3 players, to large stationary installations like traffic light controllers, programmable logic controllers, and large complex systems like hybrid vehicles, medical imaging systems, and avionics. Complexity varies from low, with a single microcontroller chip, to very high with multiple units, peripherals and networks mounted inside a large equipment rack.",
    "DBLP": "DBLP is a computer science bibliography website. Starting in 1993 at the University of Trier, Germany, it grew from a small collection of HTML files and became an organization hosting a database and logic programming bibliography site. Since November 2018, DBLP is a branch of Schloss Dagstuhl \u2013 Leibniz-Zentrum f\u00fcr Informatik (LZI). DBLP listed more than 4.86 million journal articles, conference papers, and other publications on computer science in December 2019, up from about 14,000 in 1995 and 3.66 million in July 2016. All important journals on computer science are tracked. Proceedings papers of many conferences are also tracked. It is mirrored at three sites across the Internet. For his work on maintaining DBLP, Michael Ley received an award from the Association for Computing Machinery and the VLDB Endowment Special Recognition Award in 1997. DBLP originally stood for DataBase systems and Logic Programming. As a backronym, it has been taken to stand for Digital Bibliography & Library Project; however, it is now preferred that the acronym be simply a name, hence the new title \"The DBLP Computer Science Bibliography\".",
    "Association_for_Computing_Machinery": "The Association for Computing Machinery (ACM) is a US-based international learned society for computing. It was founded in 1947, and is the world's largest scientific and educational computing society. The ACM is a non-profit professional membership group, claiming nearly 100,000 student and professional members as of 2019. Its headquarters are in New York City. The ACM is an umbrella organization for academic and scholarly interests in computer science (informatics). Its motto is \"Advancing Computing as a Science & Profession\".",
    "Computational_complexity_theory": "Computational complexity theory focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm. A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying their computational complexity, i.e., the amount of resources needed to solve them, such as time and storage. Other measures of complexity are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. The P versus NP problem, one of the seven Millennium Prize Problems, is dedicated to the field of computational complexity. Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kinds of problems can, in principle, be solved algorithmically.",
    "CiteSeerX": "CiteSeerx (originally called CiteSeer) is a public search engine and digital library for scientific and academic papers, primarily in the fields of computer and information science. CiteSeer is considered as a predecessor of academic search tools such as Google Scholar and Microsoft Academic Search. CiteSeer-like engines and archives usually only harvest documents from publicly available websites and do not crawl publisher websites. For this reason, authors whose documents are freely available are more likely to be represented in the index. CiteSeer's goal is to improve the dissemination and access of academic and scientific literature. As a non-profit service that can be freely used by anyone, it has been considered as part of the open access movement that is attempting to change academic and scientific publishing to allow greater access to scientific literature. CiteSeer freely provided Open Archives Initiative metadata of all indexed documents and links indexed documents when possible to other sources of metadata such as DBLP and the ACM Portal. To promote open data, CiteSeerx shares its data for non-commercial purposes under a Creative Commons license. CiteSeer changed its name to ResearchIndex at one point and then changed it back.",
    "P_versus_NP_problem": "The P versus NP problem is a major unsolved problem in computer science. It asks whether every problem whose solution can be quickly verified can also be solved quickly. It is one of the seven Millennium Prize Problems selected by the Clay Mathematics Institute, each of which carries a US$1,000,000 prize for the first correct solution. The informal term quickly, used above, means the existence of an algorithm solving the task that runs in polynomial time, such that the time to complete the task varies as a polynomial function on the size of the input to the algorithm (as opposed to, say, exponential time). The general class of questions for which some algorithm can provide an answer in polynomial time is called \"class P\" or just \"P\". For some questions, there is no known way to find an answer quickly, but if one is provided with information showing what the answer is, it is possible to verify the answer quickly. The class of questions for which an answer can be verified in polynomial time is called NP, which stands for \"nondeterministic polynomial time\". An answer to the P = NP question would determine whether problems that can be verified in polynomial time can also be solved in polynomial time. If it turned out that P \u2260 NP, which is widely believed, it would mean that there are problems in NP that are harder to compute than to verify: they could not be solved in polynomial time, but the answer could be verified in polynomial time. Aside from being an important problem in computational theory, a proof either way would have profound implications for mathematics, cryptography, algorithm research, artificial intelligence, game theory, multimedia processing, philosophy, economics and many other fields.",
    "Computing": "Computing is any activity that uses computers to manage, process, and communicate information. It includes development of both hardware and software. Computing is a critical, integral component of modern industrial technology. Major computing disciplines include computer engineering, software engineering, computer science, information systems, and information technology.",
    "Computational_linguistics": "Computational linguistics is an interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective, as well as the study of appropriate computational approaches to linguistic questions. Traditionally, computational linguistics was performed by computer scientists who had specialized in the application of computers to the processing of a natural language. Today, computational linguists often work as members of interdisciplinary teams, which can include regular linguists, experts in the target language, and computer scientists. In general, computational linguistics draws upon the involvement of linguists, computer scientists, experts in artificial intelligence, mathematicians, logicians, philosophers, cognitive scientists, cognitive psychologists, psycholinguists, anthropologists and neuroscientists, among others. Computational linguistics has both theoretical and applied components. Theoretical computational linguistics focuses on issues in theoretical linguistics and cognitive science and applied computational linguistics focuses on the practical outcome of modeling human language use. The Association for Computational Linguistics defines computational linguistics as: ...the scientific study of language from a computational perspective. Computational linguists are interested in providing computational models of various kinds of linguistic phenomena.",
    "Data_structure": "In computer science, a data structure is a data organization, management, and storage format that enables efficient access and modification. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data.",
    "Digital_image_processing": "In computer science, digital image processing is the use of a digital computer to process digital images through an algorithm. As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and distortion during processing. Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems. The generation and development of digital image processing are mainly affected by three factors: first, the development of computers; second, the development of mathematics (especially the creation and improvement of discrete mathematics theory); third, the demand for a wide range of applications in environment, agriculture, military, industry and medical science has increased.",
    "Deductive_reasoning": "Deductive reasoning, also deductive logic, is the process of reasoning from one or more statements (premises) to reach a logically certain conclusion. Deductive reasoning goes in the same direction as that of the conditionals, and links premises with conclusions. If all premises are true, the terms are clear, and the rules of deductive logic are followed, then the conclusion reached is necessarily true. Deductive reasoning (\"top-down logic\") contrasts with inductive reasoning (\"bottom-up logic\") in the following way; in deductive reasoning, a conclusion is reached reductively by applying general rules which hold over the entirety of a closed domain of discourse, narrowing the range under consideration until only the conclusion(s) is left (there is no epistemic uncertainty; i.e. unrecognized parts of the currently available set; all parts of the currently available set are available and recognized). In inductive reasoning, the conclusion is reached by generalizing or extrapolating from specific cases to general rules, i.e., there is epistemic uncertainty (unrecognized parts of the currently available set). However, the inductive reasoning mentioned here is not the same as induction used in mathematical proofs \u2013 mathematical induction is actually a form of deductive reasoning. Deductive reasoning differs from abductive reasoning by the direction of the reasoning relative to the conditionals. Deductive reasoning goes in the same direction as that of the conditionals, whereas abductive reasoning goes in the opposite direction to that of the conditionals.",
    "Channel_capacity": "Channel capacity, in electrical engineering, computer science, and information theory, is the tight upper bound on the rate at which information can be reliably transmitted over a communication channel. Following the terms of the noisy-channel coding theorem, the channel capacity of a given channel is the highest information rate (in units of information per unit time) that can be achieved with arbitrarily small error probability. Information theory, developed by Claude E. Shannon in 1948, defines the notion of channel capacity and provides a mathematical model by which one can compute it. The key result states that the capacity of the channel, as defined above, is given by the maximum of the mutual information between the input and output of the channel, where the maximization is with respect to the input distribution. The notion of channel capacity has been central to the development of modern wireline and wireless communication systems, with the advent of novel error correction coding mechanisms that have resulted in achieving performance very close to the limits promised by channel capacity.",
    "Arithmometer": "The Arithmometer or Arithmom\u00e8tre was the first digital mechanical calculator strong enough and reliable enough to be used daily in an office environment. This calculator could add and subtract two numbers directly and could perform long multiplications and divisions effectively by using a movable accumulator for the result. Patented in France by Thomas de Colmar in 1820 and manufactured from 1851 to 1915, it became the first commercially successful mechanical calculator. Its sturdy design gave it a strong reputation for reliability and accuracy and made it a key player in the move from human computers to calculating machines that took place during the second half of the 19th century. Its production debut of 1851 launched the mechanical calculator industry which ultimately built millions of machines well into the 1970s. For forty years, from 1851 to 1890, the arithmometer was the only type of mechanical calculator in commercial production, and it was sold all over the world. During the later part of that period two companies started manufacturing clones of the arithmometer: Burkhardt, from Germany, which started in 1878, and Layton of the UK, which started in 1883. Eventually about twenty European companies built clones of the arithmometer until the beginning of World War I.",
    "Bipolar_junction_transistor": "A bipolar junction transistor (BJT) is a type of transistor that uses both electrons and holes as charge carriers. Unipolar transistors, such as field-effect transistors, use only one kind of charge carrier. A bipolar transistor allows a small current injected at one of its terminals to control a much larger current flowing between two other terminals, making the device capable of amplification or switching. BJTs use two junctions between two semiconductor types, n-type and p-type, which are regions in a single crystal of material. The junctions can be made in several different ways, such as changing the Doping of the semiconductor material as it is grown, by depositing metal pellets to form alloy junctions, or by such methods as diffusion of n -type and p-type doping substances into the crystal. The superior predictability and performance of junction transistors soon displaced the original point-contact transistor. Diffused transistors, along with other components, are elements of integrated circuits for analog and digital functions. Hundreds of bipolar junction transistors can be made in one circuit at very low cost. Bipolar transistor integrated circuits were the main active devices of a generation of mainframe and mini computers, but most computer systems now use integrated circuits relying on field effect transistors. Bipolar transistors are still used for amplification of signals, switching, and in digital circuits. Specialized types are used for high voltage switches, for radio-frequency amplifiers, or for switching heavy currents.",
    "Computational_chemistry": "Computational chemistry is a branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into efficient computer programs, to calculate the structures and properties of molecules and solids. It is necessary because, apart from relatively recent results concerning the hydrogen molecular ion (dihydrogen cation, see references therein for more details), the quantum many-body problem cannot be solved analytically, much less in closed form. While computational results normally complement the information obtained by chemical experiments, it can in some cases predict hitherto unobserved chemical phenomena. It is widely used in the design of new drugs and materials. Examples of such properties are structure (i.e., the expected positions of the constituent atoms), absolute and relative (interaction) energies, electronic charge density distributions, dipoles and higher multipole moments, vibrational frequencies, reactivity, or other spectroscopic quantities, and cross sections for collision with other particles. The methods used cover both static and dynamic situations. In all cases, the computer time and other resources (such as memory and disk space) increase rapidly with the size of the system being studied. That system can be one molecule, a group of molecules, or a solid. Computational chemistry methods range from very approximate to highly accurate; the latter are usually feasible for small systems only. Ab initio methods are based entirely on quantum mechanics and basic physical constants. Other methods are called empirical or semi-empirical because they use additional empirical parameters. Both ab initio and semi-empirical approaches involve approximations. These range from simplified forms of the first-principles equations that are easier or faster to solve, to approximations limiting the size of the system (for example, periodic boundary conditions), to fundamental approximations to the underlying equations that are required to achieve any solution to them at all. For example, most ab initio calculations make the Born\u2013Oppenheimer approximation, which greatly simplifies the underlying Schr\u00f6dinger equation by assuming that the nuclei remain in place during the calculation. In principle, ab initio methods eventually converge to the exact solution of the underlying equations as the number of approximations is reduced. In practice, however, it is impossible to eliminate all approximations, and residual error inevitably remains. The goal of computational chemistry is to minimize this residual error while keeping the calculations tractable. In some cases, the details of electronic structure are less important than the long-time phase space behavior of molecules. This is the case in conformational studies of proteins and protein-ligand binding thermodynamics. Classical approximations to the potential energy surface are used, typically with molecular mechanics force fields, as they are computationally less intensive than electronic calculations, to enable longer simulations of molecular dynamics. Furthermore, cheminformatics uses even more empirical (and computationally cheaper) methods like machine learning based on physicochemical properties. One typical problem in cheminformatics is to predict the binding affinity of drug molecules to a given target. Other problems include predicting binding specificity, off-target effects, toxicity, and pharmacokinetic properties.",
    "Processor_(computing)": "In computing, a processor or processing unit is an electronic circuit which performs operations on some external data source, usually memory or some other data stream. It typically takes the form of a microprocessor, which is fabricated on a single metal\u2013oxide\u2013semiconductor (MOS) integrated circuit (IC) chip. The term is frequently used to refer to the central processing unit in a system. However, it can also refer to other co-processors.",
    "Fluid_dynamics": "In physics and engineering, fluid dynamics is a subdiscipline of fluid mechanics that describes the flow of fluids\u2014liquids and gases. It has several subdisciplines, including aerodynamics (the study of air and other gases in motion) and hydrodynamics (the study of liquids in motion). Fluid dynamics has a wide range of applications, including calculating forces and moments on aircraft, determining the mass flow rate of petroleum through pipelines, predicting weather patterns, understanding nebulae in interstellar space and modelling fission weapon detonation. Fluid dynamics offers a systematic structure\u2014which underlies these practical disciplines\u2014that embraces empirical and semi-empirical laws derived from flow measurement and used to solve practical problems. The solution to a fluid dynamics problem typically involves the calculation of various properties of the fluid, such as flow velocity, pressure, density, and temperature, as functions of space and time. Before the twentieth century, hydrodynamics was synonymous with fluid dynamics. This is still reflected in names of some fluid dynamics topics, like magnetohydrodynamics and hydrodynamic stability, both of which can also be applied to gases.",
    "Control_system": "A control system manages, commands, directs, or regulates the behavior of other devices or systems using control loops. It can range from a single home heating controller using a thermostat controlling a domestic boiler to large Industrial control systems which are used for controlling processes or machines. For continuously modulated control, a feedback controller is used to automatically control a process or operation. The control system compares the value or status of the process variable (PV) being controlled with the desired value or setpoint (SP), and applies the difference as a control signal to bring the process variable output of the plant to the same value as the setpoint. For sequential and combinational logic, software logic, such as in a programmable logic controller, is used.",
    "Computational_statistics": "Computational statistics, or statistical computing, is the interface between statistics and computer science. It is the area of computational science (or scientific computing) specific to the mathematical science of statistics. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general statistical education. As in traditional statistics the goal is to transform raw data into knowledge, but the focus lies on computer intensive statistical methods, such as cases with very large sample size and non-homogeneous data sets. The terms 'computational statistics' and 'statistical computing' are often used interchangeably, although Carlo Lauro (a former president of the International Association for Statistical Computing) proposed making a distinction, defining 'statistical computing' as \"the application of computer science to statistics\",and 'computational statistics' as \"aiming at the design of algorithm for implementingstatistical methods on computers, including the ones unthinkable before the computerage (e.g. bootstrap, simulation), as well as to cope with analytically intractable problems\" [sic]. The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models.",
    "History_of_artificial_intelligence": "The history of Artificial Intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by classical philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain. The field of AI research was founded at a workshop held on the campus of Dartmouth College during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation and they were given millions of dollars to make this vision come true. Eventually, it became obvious that they had grossly underestimated the difficulty of the project. In 1973, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an \"AI winter\". Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 80s the investors became disillusioned and withdrew funding again. Investment and interest in AI boomed in the first decades of the 21st century, when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets.",
    "Bioinformatics": "Bioinformatics  () is an interdisciplinary field that develops methods and software tools for understanding biological data, in particular when the data sets are large and complex. As an interdisciplinary field of science, bioinformatics combines biology, computer science, information engineering, mathematics and statistics to analyze and interpret the biological data. Bioinformatics has been used for in silico analyses of biological queries using mathematical and statistical techniques. Bioinformatics includes biological studies that use computer programming as part of their methodology, as well as a specific analysis \"pipelines\" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidates genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organizational principles within nucleic acid and protein sequences, called proteomics.",
    "Video_game": "A video game is an electronic game that involves interaction with a user interface to generate visual feedback on a two- or three-dimensional video display device such as a touchscreen, virtual reality headset or monitor/TV set. Since the 1980s, video games have become an increasingly important part of the entertainment industry, and whether they are also a form of art is a matter of dispute. The electronic systems used to play video games are called platforms. Video games are developed and released for one or several platforms and may not be available on others. Specialized platforms such as arcade games, which present the game in a large, typically coin-operated chassis, were common in the 1980s in video arcades, but declined in popularity as other, more affordable platforms became available. These include dedicated devices such as video game consoles, as well as general-purpose computers like a laptop, desktop or handheld computing devices. The input device used for games, the game controller, varies across platforms. Common controllers include gamepads, joysticks, mouse devices, keyboards, the touchscreens of mobile devices, or even a person's body, using a Kinect sensor. Players view the game on a display device such as a television or computer monitor or sometimes on virtual reality head-mounted display goggles. There are often game sound effects, music and voice actor lines which come from loudspeakers or headphones. Some games in the 2000s include haptic, vibration-creating effects, force feedback peripherals and virtual reality headsets. Since the 2010s, the commercial importance of the video game industry has been increasing. The emerging Asian markets and mobile games on smartphones in particular are driving the growth of the industry. As of 2018, video games generated sales of US$134.9 billion annually worldwide, and were the third-largest segment in the U.S. entertainment market, behind broadcast and cable TV.",
    "Pattern_recognition": "Pattern recognition is the automated recognition of patterns and regularities in data. It has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. However, these activities can be viewed as two facets of the same field of application, and together they have undergone substantial development over the past few decades. A modern definition of pattern recognition is: The field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories. This article focuses on machine learning approaches to pattern recognition. Pattern recognition systems are in many cases trained from labeled \"training\" data (supervised learning), but when no labeled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning). Machine learning is strongly related to pattern recognition and originates from artificial intelligence. KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition focuses more on the signal and also takes acquisition and Signal Processing into consideration. It originated in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. In pattern recognition, there may be a higher interest to formalize, explain and visualize the pattern, while machine learning traditionally focuses on maximizing the recognition rates. Yet, all of these domains have evolved substantially from their roots in artificial intelligence, engineering and statistics, and they've become increasingly similar by integrating developments and ideas from each other. In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is \"spam\" or \"non-spam\"). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence. Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform \"most likely\" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors. In contrast to pattern recognition, pattern matching is not generally a type of machine learning, although pattern-matching algorithms (especially with fairly general, carefully tailored patterns) can sometimes succeed in providing similar-quality output of the sort provided by pattern-recognition algorithms.",
    "Compiler": "In computing, a compiler is a computer program that translates computer code written in one programming language (the source language) into another language (the target language). The name \"compiler\" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program. However, there are many different types of compilers. If the compiled program can run on a computer whose CPU or operating system is different from the one on which the compiler runs, the compiler is a cross-compiler. A bootstrap compiler is written in the language that it intends to compile. A program that translates from a low-level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transcompiler. A language rewriter is usually a program that translates the form of expressions without a change of language. The term compiler-compiler refers to tools used to create parsers that perform syntax analysis. A compiler is likely to perform many or all of the following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness. Compilers are not the only language processor used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations. The translation process influences the design of computer languages, which leads to a preference of compilation or interpretation. In practice, an interpreter can be implemented for compiled languages and compilers can be implemented for interpreted languages.",
    "R\u00f3zsa_P\u00e9ter": "R\u00f3zsa P\u00e9ter, born R\u00f3zsa Politzer, (17 February 1905 \u2013 16 February 1977) was a Hungarian mathematician and logician. She is best known as the \"founding mother of recursion theory\".",
    "Information_and_communications_technology": "Information and communications technology (ICT) is an extensional term for information technology (IT) that stresses the role of unified communications and the integration of telecommunications (telephone lines and wireless signals) and computers, as well as necessary enterprise software, middleware, storage, and audiovisual systems, that enable users to access, store, transmit, and manipulate information. The term ICT is also used to refer to the convergence of audiovisual and telephone networks with computer networks through a single cabling or link system. There are large economic incentives to merge the telephone network with the computer network system using a single unified system of cabling, signal distribution, and management. ICT is an umbrella term that includes any communication device, encompassing radio, television, cell phones, computer and network hardware, satellite systems and so on, as well as the various services and appliance with them such as video conferencing and distance learning. ICT is a broad subject and the concepts are evolving. It covers any product that will store, retrieve, manipulate, transmit, or receive information electronically in a digital form (e.g., personal computers, digital television, email, or robots). Theoretical differences between interpersonal-communication technologies and mass-communication technologies have been identified by the philosopher Piyush Mathur. Skills Framework for the Information Age is one of many models for describing and managing competencies for ICT professionals for the 21st century.",
    "Moore's_law": "Moore's law is the observation that the number of transistors in a dense integrated circuit (IC) doubles about every two years. Moore's law is an observation and projection of a historical trend. Rather than a law of physics, it is an empirical relationship linked to gains from experience in production. The observation is named after Gordon Moore, the co-founder of Fairchild Semiconductor and CEO and co-founder of Intel, who in 1965 posited a doubling every year in the number of components per integrated circuit, and projected this rate of growth would continue for at least another decade. In 1975, looking forward to the next decade, he revised the forecast to doubling every two years, a compound annual growth rate (CAGR) of 40%. While Moore did not use empirical evidence in forecasting that the historical trend would continue, his prediction held since 1975 and has since become known as a \"law.\" Moore's prediction has been used in the semiconductor industry to guide long-term planning and to set targets for research and development. Advancements in digital electronics, such as the reduction in quality-adjusted microprocessor prices, the increase in memory capacity (RAM and flash), the improvement of sensors, and even the number and size of pixels in digital cameras, are strongly linked to Moore's law. These step changes in digital electronics have been a driving force of technological and social change, productivity, and economic growth. Industry experts have not reached a consensus on exactly when Moore's law will cease to apply. Microprocessor architects report that semiconductor advancement has slowed industry-wide since around 2010, below the pace predicted by Moore's law. However, as of 2018, leading semiconductor manufacturers have developed IC fabrication processes in mass production which are claimed to keep pace with Moore's law.",
    "Data_science": "Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, deep learning and big data. Data science is a \"concept to unify statistics, data analysis, machine learning, domain knowledge and their related methods\" in order to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, domain knowledge and information science. Turing award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.",
    "Gottfried_Wilhelm_Leibniz": "Gottfried Wilhelm (von) Leibniz (sometimes spelled Leibnitz) (; German: [\u02c8\u0261\u0254tf\u0281i\u02d0t \u02c8v\u026alh\u025blm f\u0254n \u02c8la\u026abn\u026ats] or [\u02c8la\u026apn\u026ats]; French: Godefroi Guillaume Leibnitz; 1 July 1646 [O.S. 21 June] \u2013 14 November 1716) was a prominent German polymath and one of the most important logicians, mathematicians and natural philosophers of the Enlightenment. As a representative of the seventeenth-century tradition of rationalism, Leibniz developed, as his most prominent accomplishment, the ideas of differential and integral calculus, independently of Isaac Newton's contemporaneous developments. Mathematical works have consistently favored Leibniz's notation as the conventional expression of calculus. It was only in the 20th century that Leibniz's law of continuity and transcendental law of homogeneity found mathematical implementation (by means of non-standard analysis). He became one of the most prolific inventors in the field of mechanical calculators. While working on adding automatic multiplication and division to Pascal's calculator, he was the first to describe a pinwheel calculator in 1685 and invented the Leibniz wheel, used in the arithmometer, the first mass-produced mechanical calculator. He also refined the binary number system, which is the foundation of nearly all digital (electronic, solid-state, discrete logic) computers, including \"the Von Neumann machine\", which is the standard design paradigm, or \"computer architecture\", followed from the second half of the 20th Century, and into the 21st. In philosophy, Leibniz is most noted for his optimism, i.e. his conclusion that our universe is, in a restricted sense, the best possible one that God could have created, an idea that was often lampooned by others such as Voltaire. Leibniz, along with Ren\u00e9 Descartes and Baruch Spinoza, was one of the three great 17th-century advocates of rationalism. The work of Leibniz anticipated modern logic and analytic philosophy, but his philosophy also assimilates elements of the scholastic tradition, notably that conclusions are produced by applying reason to first principles or prior definitions rather than to empirical evidence. Leibniz made major contributions to physics and technology, and anticipated notions that surfaced much later in philosophy, probability theory, biology, medicine, geology, psychology, linguistics, and computer science. He wrote works on philosophy, politics, law, ethics, theology, history, and philology. Leibniz also contributed to the field of library science. While serving as overseer of the Wolfenb\u00fcttel library in Germany, he devised a cataloging system that would serve as a guide for many of Europe's largest libraries. Leibniz's contributions to this vast array of subjects were scattered in various learned journals, in tens of thousands of letters, and in unpublished manuscripts. He wrote in several languages, primarily in Latin, French and German but also in English, Italian and Dutch. There is no complete gathering of the writings of Leibniz translated into English.",
    "Telecommunication": "Telecommunication is the transmission of information by various types of technologies over wire, radio, optical or other electromagnetic systems. It has its origin in the desire of humans for communication over a distance greater than that feasible with the human voice, but with a similar scale of expediency. This excludes systems such as postal mail from the field of telecommunication. The Latin term communicatio is considered the social process of information exchange, and the Greek prefix tele expresses distance. The transmission media in telecommunication have evolved through numerous stages of technology from beacons and other visual signals, such as smoke signals, semaphore telegraphs, signal flags, and optical heliographs, to electrical cable and electromagnetic radiation, including light. Such transmission paths are often divided into communication channels which afford the advantages of multiplexing multiple concurrent communication sessions. Telecommunication is often used in its plural form, because it involves many different technologies. Other examples of pre-modern long-distance communication included audio messages such as coded drumbeats, lung-blown horns, and loud whistles. 20th- and 21st-century technologies for long-distance communication usually involve electrical and electromagnetic technologies, such as telegraph, telephone, television and teleprinter, networks, radio, microwave transmission, optical fiber, and communications satellites. A revolution in wireless communication began in the first decade of the 20th century with the pioneering developments in radio communications by Guglielmo Marconi, who won the Nobel Prize in Physics in 1909, and other notable pioneering inventors and developers in the field of electrical and electronic telecommunications. These included Charles Wheatstone and Samuel Morse (inventors of the telegraph), Alexander Graham Bell (inventor of the telephone), Edwin Armstrong and Lee de Forest (inventors of radio), as well as Vladimir K. Zworykin, John Logie Baird and Philo Farnsworth (some of the inventors of television).",
    "United_States": "The United States of America (USA), commonly known as the United States (U.S. or US) or America, is a country mostly located in central North America, between Canada and Mexico. It consists of 50 states, a federal district, five major self-governing territories, and various possessions. At 3.8 million square miles (9.8 million km2), it is the world's third- or fourth-largest country by total area. With a 2019 estimated population of over 328 million, the U.S. is the third most populous country in the world. The Americans are a racially and ethnically diverse population that has been shaped through centuries of immigration. The capital is Washington, D.C., and the most populous city is New York City. Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago, and European colonization began in the 16th century. The United States emerged from the thirteen British colonies established along the East Coast. Numerous disputes between Great Britain and the colonies led to the American Revolutionary War lasting between 1775 and 1783, leading to independence. Beginning in the late 18th century, the United States vigorously expanded across North America, gradually acquiring new territories, killing and displacing Native Americans, and admitting new states. By 1848, the United States spanned the continent.Slavery was legal in much of the United States until the second half of the 19th century, when the American Civil War led to its abolition. The Spanish\u2013American War and World War I entrenched the U.S. as a world power, a status confirmed by the outcome of World War II. It was the first country to develop nuclear weapons and is the only country to have used them in warfare. During the Cold War, the United States and the Soviet Union competed in the Space Race, culminating with the 1969 Apollo 11 mission, the spaceflight that first landed humans on the Moon. The end of the Cold War and collapse of the Soviet Union in 1991 left the United States as the world's sole superpower. The United States is a federal republic and a representative democracy. It is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States (OAS), NATO, and other international organizations. It is a permanent member of the United Nations Security Council. A highly developed country, the United States is the world's largest economy and accounts for approximately a quarter of global gross domestic product (GDP). The United States is the world's largest importer and the second-largest exporter of goods, by value. Although its population is only 4.3% of the world total, it holds 29.4% of the total wealth in the world, the largest share held by any country. Despite income and wealth disparities, the United States continues to rank very high in measures of socioeconomic performance, including average wage, median income, median wealth, human development, per capita GDP, and worker productivity. It is the foremost military power in the world, making up more than a third of global military spending, and is a leading political, cultural, and scientific force internationally.",
    "Medical_image_computing": "Medical image computing (MIC) is an interdisciplinary field at the intersection of computer science, information engineering, electrical engineering, physics, mathematics and medicine. This field develops computational and mathematical methods for solving problems pertaining to medical images and their use for biomedical research and clinical care. The main goal of MIC is to extract clinically relevant information or knowledge from medical images. While closely related to the field of medical imaging, MIC focuses on the computational analysis of the images, not their acquisition. The methods can be grouped into several broad categories: , , , and others.",
    "IBM": "International Business Machines Corporation (IBM) is an American multinational technology company headquartered in Armonk, New York, with operations in over 170 countries. The company began in 1911, founded in Endicott, New York, as the Computing-Tabulating-Recording Company (CTR) and was renamed \"International Business Machines\" in 1924. IBM is incorporated in New York. IBM produces and sells computer hardware, middleware and software, and provides hosting and consulting services in areas ranging from mainframe computers to nanotechnology. IBM is also a major research organization, holding the record for most U.S. patents generated by a business (as of 2020) for 27 consecutive years. Inventions by IBM include the automated teller machine (ATM), the floppy disk, the hard disk drive, the magnetic stripe card, the relational database, the SQL programming language, the UPC barcode, and dynamic random-access memory (DRAM). The IBM mainframe, exemplified by the System/360, was the dominant computing platform during the 1960s and 1970s. IBM has continually shifted business operations by focusing on higher-value, more profitable markets. This includes spinning off printer manufacturer Lexmark in 1991 and the sale of personal computer (ThinkPad/ThinkCentre) and x86-based server businesses to Lenovo (in 2005 and 2014, respectively), and acquiring companies such as PwC Consulting (2002), SPSS (2009), The Weather Company (2016), and Red Hat (2019). Also in 2015, IBM announced that it would go \"fabless\", continuing to design semiconductors, but offloading manufacturing to GlobalFoundries. Nicknamed Big Blue, IBM is one of 30 companies included in the Dow Jones Industrial Average and one of the world's largest employers, with (as of 2018) over 352,600 employees, known as \"IBMers\". At least 70% of IBMers are based outside the United States, and the country with the largest number of IBMers is India. IBM employees have been awarded five Nobel Prizes, six Turing Awards, ten National Medals of Technology (USA) and five National Medals of Science (USA).",
    "Hungarian_language": "Hungarian () is a Uralic language spoken in Hungary and parts of several neighbouring countries. It is the official language of Hungary and one of the 24 official languages of the European Union. Outside Hungary it is also spoken by communities of Hungarians in the countries that today make up Slovakia, western Ukraine (Subcarpathia), central and western Romania (Transylvania), northern Serbia (Vojvodina), northern Croatia and northeastern Slovenia (Mur region). It is also spoken by Hungarian diaspora communities worldwide, especially in North America (particularly the United States and Canada) and Israel. With 13 million speakers, it is the Uralic family's largest member by number of speakers.",
    "Management_science": "Management science (MS) is the broad interdisciplinary study of problem solving and decision making in human organizations, with strong links to management, economics, business, engineering, management consulting, and other fields. It uses various scientific research-based principles, strategies, and analytical methods including mathematical modeling, statistics and numerical algorithms to improve an organization's ability to enact rational and accurate management decisions by arriving at optimal or near optimal solutions to complex decision problems. Management science helps businesses to achieve goals using various scientific methods. The field was initially an outgrowth of applied mathematics, where early challenges were problems relating to the optimization of systems which could be modeled linearly, i.e., determining the optima (maximum value of profit, assembly line performance, crop yield, bandwidth, etc. or minimum of loss, risk, costs, etc.) of some objective function. Today, management science encompasses any organizational activity for which a problem is structured in mathematical form to generate managerially relevant insights.",
    "Algorithmics": "Algorithmics is the systematic study of the design and analysis of algorithms. It is fundamental and one of the oldest fields of computer science. It includes algorithm design, the art of building a procedure which can solve efficiently a specific problem or a class of problem, algorithmic complexity theory, the study of estimating the hardness of problems by studying the properties of algorithm that solves them, or algorithm analysis, the science of studying the properties of a problem, such as quantifying resources in time and memory space needed by this algorithm to solve this problem.",
    "Dependability": "In systems engineering, dependability is a measure of a system's availability, reliability, and its maintainability, and maintenance support performance, and, in some cases, other characteristics such as durability, safety and security. In software engineering, dependability is the ability to provide services that can defensibly be trusted within a time-period. This may also encompass mechanisms designed to increase and maintain the dependability of a system or software. The International Electrotechnical Commission (IEC), via its Technical Committee TC 56 develops and maintains international standards that provide systematic methods and tools for dependability assessment and management of equipment, services, and systems throughout their life cycles. Dependability can be broken down into three elements: \n* Attributes - a way to assess the dependability of a system \n* Threats - an understanding of the things that can affect the dependability of a system \n* Means - ways to increase a system's dependability",
    "Data_compression": "In signal processing, data compression, source coding, or bit-rate reduction is the process of encoding information using fewer bits than the original representation. Any particular compression is either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information. Typically, a device that performs data compression is referred to as an encoder, and one that performs the reversal of the process (decompression) as a decoder. The process of reducing the size of a data file is often referred to as data compression. In the context of data transmission, it is called source coding; encoding done at the source of the data before it is stored or transmitted. Source coding should not be confused with channel coding, for error detection and correction or line coding, the means for mapping data onto a signal. Compression is useful because it reduces resources required to store and transmit data. Computational resources are consumed in the compression and decompression processes. Data compression is subject to a space\u2013time complexity trade-off. For instance,  may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data.",
    "Computability_theory": "Computability theory, also known as recursion theory, is a branch of mathematical logic, of computer science, and of the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability. In these areas, recursion theory overlaps with proof theory and effective descriptive set theory. Basic questions addressed by recursion theory include: \n* What does it mean for a function on the natural numbers to be computable? \n* How can noncomputable functions be classified into a hierarchy based on their level of noncomputability? Although there is considerable overlap in terms of knowledge and methods, mathematical recursion theorists study the theory of relative computability, reducibility notions, and degree structures; those in the computer science field focus on the theory of subrecursive hierarchies, formal methods, and formal languages.",
    "Personal_computer": "A personal computer (PC) is a multi-purpose computer whose size, capabilities, and price make it feasible for individual use. Personal computers are intended to be operated directly by an end user, rather than by a computer expert or technician. Unlike large, costly minicomputers and mainframes, time-sharing by many people at the same time is not used with personal computers. Institutional or corporate computer owners in the 1960s had to write their own programs to do any useful work with the machines. While personal computer users may develop their own applications, usually these systems run commercial software, free-of-charge software (\"freeware\"), which is most often proprietary, or free and open-source software, which is provided in \"ready-to-run\", or binary, form. Software for personal computers is typically developed and distributed independently from the hardware or operating system manufacturers. Many personal computer users no longer need to write their own programs to make any use of a personal computer, although end-user programming is still feasible. This contrasts with mobile systems, where software is often only available through a manufacturer-supported channel, and end-user program development may be discouraged by lack of support by the manufacturer. Since the early 1990s, Microsoft operating systems and Intel hardware dominated much of the personal computer market, first with MS-DOS and then with Microsoft Windows. Alternatives to Microsoft's Windows operating systems occupy a minority share of the industry. These include Apple's macOS and free and open-source Unix-like operating systems, such as Linux. The advent of personal computers and the concurrent Digital Revolution have significantly affected the lives of people in all countries.",
    "Association_for_Information_Systems": "The Association for Information Systems (AIS) is an international, not-for-profit, professional association. Membership is made up primarily of academic educators, researchers and institutions that specialize in information systems (IS) development, implementation and evaluation. The association has members in more than 90 countries, and is led by a president who is annually elected from one of three world regions\u2014the Americas, Europe and Africa and Asia-Pacific\u2014on a rotating basis. The governing Council is made up of elected functional vice-presidents and other officers and council members who are elected in the three world regions. The association organizes two annual conferences for IS researchers, educators and students: The International Conference on Information Systems (I.C.I.S.), which alternates between the three world regions and the Americas Conference For Information Systems (AMCIS), which is located at different sites in North, Central and South America. The Association publishes academic journals including: \n* Journal of the Association for Information Systems (JAIS) \n* Scandinavian Journal of Information Systems (SJIS) \n* Revista Latinoamericana Y Del Caribe De La Associacion De Sistemas De Informacion (RELCASI) \n* Pacific Asia Journal of the Association for Information Systems (PAJAIS) \n* Journal of the Midwest Association for Information Systems (JMWAIS) \n* Journal of Information Technology Theory and Application (JITTA) \n* Communications of the Association for Information Systems (CAIS) \n* AIS Transactions on Replication Research (TRR) \n* AIS Transactions on Human-Computer Interaction (THCI) Affiliated journals include: \n* Business & Information Systems Engineering (BISE) \n* Management Information Systems Quarterly (MISQ) \n* MIS Quarterly Executive (MISQe) \n* Information Systems Journal (ISJ) \n* Syst\u00e8mes d'Information et Management (SIM) \n* Foundations and Trends in Information Systems (FnTIS) Both AIS published titles and affiliated journals are included in the AIS eLibrary, which is accessible as a benefit of membership.",
    "GCE_Ordinary_Level": "The O Level (Ordinary Level; official title: General Certificate of Education: Ordinary Level) is a subject-based qualification conferred as part of the General Certificate of Education. It was introduced in place of the School Certificate in 1951 as part of an educational reform alongside the more in-depth and academically rigorous A-level (official title of qualification: General Certificate of Education \u2013 Advanced Level) in England, Wales and Northern Ireland. Those three jurisdictions replaced O Levels gradually with General Certificate of Secondary Education (GCSE) and International General Certificate of Secondary Education (IGCSE) exams over time. The Scottish equivalent was the O-grade (replaced by the Standard Grade). The O Level qualification is still awarded by CIE Cambridge International Examinations, the international counterpart of the British examination Board OCR (Oxford, Cambridge & Royal Society of Arts), in select locations, instead of or alongside the International General Certificate of Secondary Education qualifications. Both CIE and OCR have Cambridge Assessment as their parent organisation. The Cambridge O Level has already been phased out, however, and is no longer available in certain administrative regions.",
    "George_Forsythe": "George Elmer Forsythe (January 8, 1917 \u2013 April 9, 1972) was the founder and head of Stanford University's Computer Science Department. George came to Stanford in the Mathematics Department in 1959, and served as professor and chairman of the Computer Science department from 1965 until his death. Forsythe served as the president of the Association for Computing Machinery (ACM), and also co-authored four books on computer science and a fifth on meteorology, and edited more than 75 other books on computer science. Forsythe married Alexandra I. Forsythe, who wrote the first published textbook in computer science and actively participated in her husband's work, while promoting a more active role for women than was common at the time. Between 1950 and 1958 both of them programmed using the SWAC at the National Bureau of Standards (NBS) in Los Angeles and later at UCLA after the western division of NBS was closed due to political pressures (see Oral History cited below). With his wife, Forsythe had a daughter and a son. According to Donald Knuth, Forsythe's greatest contributions were helping to establish computer science as its own academic discipline and starting the field of refereeing and editing algorithms as scholarly work.Professor Forsythe supervised 17 PhD graduates; many of them went into academic careers. He won a Lester R. Ford Award in 1969 and again in 1971.",
    "List_of_computer_scientists": "This is a list of computer scientists, people who do work in computer science, in particular researchers and authors. Some persons notable as programmers are included here because they work in research as well as program. A few of these people pre-date the invention of the digital computer; they are now regarded as computer scientists because their work can be seen as leading to the invention of the computer. Others are mathematicians whose work falls within what would now be called theoretical computer science, such as complexity theory and algorithmic information theory.",
    "Collection_of_Computer_Science_Bibliographies": "The Collection of Computer Science Bibliographies (founded 1993) is one of the oldest (if not the oldest) bibliography collections freely accessible on the Internet. It is a collection of bibliographies of scientific literature in computer science and (computational) mathematics from various sources, covering most aspects of computer science. The bibliographies are updated weekly from their original locations. As of 2009 the collection contains more than 2.8 million unique references (mostly to journal articles, conference papers and technical reports), clustered in about 1700 bibliographies, and consists of more than 4.4 Gb (950 Mb gzipped) of BibTeX entries. More than 600,000 references contain cross-references to citing or cited publications. More than 1 million references contain URLs to online versions of the papers. Abstracts are available for more than 1 million entries. There are more than 2,000 links to other sites carrying bibliographic information."
}