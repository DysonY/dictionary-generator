{
    "Natural_language_processing": "Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.",
    "PARRY": "PARRY was an early example of a chatterbot, implemented in 1972 by psychiatrist Kenneth Colby.",
    "Noam_Chomsky": "Avram Noam Chomsky (born December 7, 1928) is an American linguist, philosopher, cognitive scientist, historian, social critic, and political activist. Sometimes called \"the father of modern linguistics\", Chomsky is also a major figure in analytic philosophy and one of the founders of the field of cognitive science. He holds a joint appointment as Institute Professor Emeritus at the Massachusetts Institute of Technology (MIT) and Laureate Professor at the University of Arizona, and is the author of more than 100 books on topics such as linguistics, war, politics, and mass media. Ideologically, he aligns with anarcho-syndicalism and libertarian socialism. Born to Ashkenazi Jewish immigrants in Philadelphia, Chomsky developed an early interest in anarchism from alternative bookstores in New York City. He studied at the University of Pennsylvania. During his postgraduate work in the Harvard Society of Fellows, Chomsky developed the theory of transformational grammar for which he earned his doctorate in 1955. That year he began teaching at MIT, and in 1957 emerged as a significant figure in linguistics with his landmark work Syntactic Structures, which played a major role in remodeling the study of language. From 1958 to 1959 Chomsky was a National Science Foundation fellow at the Institute for Advanced Study. He created or co-created the universal grammar theory, the generative grammar theory, the Chomsky hierarchy, and the minimalist program. Chomsky also played a pivotal role in the decline of linguistic behaviorism, and was particularly critical of the work of B. F. Skinner. An outspoken opponent of U.S. involvement in the Vietnam War, which he saw as an act of American imperialism, in 1967 Chomsky rose to national attention for his antiwar essay \"The Responsibility of Intellectuals\". Associated with the New Left, he was arrested multiple times for his activism and placed on President Richard Nixon's Enemies List. While expanding his work in linguistics over subsequent decades, he also became involved in the linguistics wars. In collaboration with Edward S. Herman, Chomsky later articulated the propaganda model of media criticism in Manufacturing Consent and worked to expose the Indonesian occupation of East Timor. His defense of freedom of speech, including Holocaust denial, generated significant controversy in the Faurisson affair of the 1980s. Since retiring from MIT, he has continued his vocal political activism, including opposing the 2003 invasion of Iraq and supporting the Occupy movement. Chomsky began teaching at the University of Arizona in 2017. One of the most cited scholars alive, Chomsky has influenced a broad array of academic fields. He is widely recognized as having helped to spark the cognitive revolution in the human sciences, contributing to the development of a new cognitivistic framework for the study of language and the mind. In addition to his continued scholarship, he remains a leading critic of U.S. foreign policy, neoliberalism and contemporary state capitalism, the Israeli\u2013Palestinian conflict, and mainstream news media. His ideas are highly influential in the anti-capitalist and anti-imperialist movements, but have also drawn criticism, with some accusing Chomsky of anti-Americanism.",
    "Comparative": "In general linguistics, the comparative is a syntactic construction that serves to express a comparison between two (or more) entities or groups of entities in quality or degree - see also comparison (grammar) for an overview of comparison, as well as positive and superlative degrees of comparison. The syntax of comparative constructions is poorly understood due to the complexity of the data. In particular, the comparative frequently occurs with independent mechanisms of syntax such as coordination and forms of ellipsis (gapping, pseudogapping, null complement anaphora, stripping, verb phrase ellipsis). The interaction of the various mechanisms complicates the analysis.",
    "Pro-drop_language": "A pro-drop language (from \"pronoun-dropping\") is a language in which certain classes of pronouns may be omitted when they are pragmatically or grammatically inferable. The precise conditions vary from language to language, and can be quite intricate. The phenomenon of \"pronoun-dropping\" is also commonly referred to as zero or null anaphora. In the case of pro-drop languages, null anaphora refers to the fact that the null position has referential properties, meaning it is not a null dummy pronoun. Pro-drop is only licensed in languages that have a positive setting of the pro-drop parameter, which allows the null element to be identified by its governor. In pro-drop languages with a highly inflected verbal morphology, the expression of the subject pronoun is considered unnecessary because the verbal inflection indicates the person and number of the subject, thus the referent of the null subject can be inferred from the grammatical inflection on the verb. Even though in everyday speech there are instances when who or what is being referred to can be inferred from context, non-pro-drop languages still require the pronoun. However, pro-drop languages allow those referential pronouns to be omitted, or be phonologically null. Among major languages, two which might be called pro-drop languages are Japanese and Korean (featuring pronoun deletion not only for subjects, but for practically all grammatical contexts). Chinese, Slavic languages,, American Sign Language and Vietnamese also exhibit frequent pro-drop features. In contrast, non-pro-drop is an areal feature of many northern European languages (see Standard Average European), including French, (standard) German, English and Emilian. Some languages might be considered only partially pro-drop in that they allow deletion of the subject pronoun. These null-subject languages include most Romance languages (French is an exception) as well as all the Balto-Slavic languages and to a limited extent Icelandic. Colloquial and dialectal German, unlike the standard language, are also partially pro-drop; they typically allow deletion of the subject pronoun in main clauses without inversion, but not otherwise. Hungarian allows deletion of both the subject and object pronouns.",
    "Compound-term_processing": "Compound-term processing, in information-retrieval, is search result matching on the basis of compound terms. Compound terms are built by combining two or more simple terms; for example, \"triple\" is a single word term, but \"triple heart bypass\" is a compound term. Compound-term processing is a new approach to an old problem: how can one improve the relevance of search results while maintaining ease of use? Using this technique, a search for survival rates following a triple heart bypass in elderly people will locate documents about this topic even if this precise phrase is not contained in any document. This can be performed by a concept search, which itself uses compound-term processing. This will extract the key concepts automatically (in this case \"survival rates\", \"triple heart bypass\" and \"elderly people\") and use these concepts to select the most relevant documents.",
    "Computer-assisted_language_learning": "Computer-assisted language learning (CALL), British, or Computer-Aided Instruction (CAI)/Computer-Aided Language Instruction (CALI), American, is briefly defined in a seminal work by Levy (1997: p. 1) as \"the search for and study of applications of the computer in language teaching and learning\". CALL embraces a wide range of information and communications technology applications and approaches to teaching and learning foreign languages, from the \"traditional\" drill-and-practice programs that characterised CALL in the 1960s and 1970s to more recent manifestations of CALL, e.g. as used in a virtual learning environment and Web-based distance learning. It also extends to the use of , interactive whiteboards, Computer-mediated communication (CMC), , and mobile-assisted language learning (MALL). The term CALI (computer-assisted language instruction) was in use before CALL, reflecting its origins as a subset of the general term CAI (computer-assisted instruction). CALI fell out of favour among language teachers, however, as it appeared to imply a teacher-centred approach (instructional), whereas language teachers are more inclined to prefer a student-centred approach, focusing on learning rather than instruction. CALL began to replace CALI in the early 1980s (Davies & Higgins 1982: p. 3) and it is now incorporated into the names of the growing number of  worldwide. An alternative term, technology-enhanced language learning (TELL), also emerged around the early 1990s: e.g. the TELL Consortium project, University of Hull. The current philosophy of CALL puts a strong emphasis on student-centred materials that allow learners to work on their own. Such materials may be structured or unstructured, but they normally embody two important features: interactive learning and individualised learning. CALL is essentially a tool that helps teachers to facilitate the language learning process. It can be used to reinforce what has already been learned in the classroom or as a remedial tool to help learners who require additional support. The design of CALL materials generally takes into consideration principles of language pedagogy and methodology, which may be derived from different learning theories (e.g. behaviourist, cognitive, constructivist) and second-language learning theories such as Stephen Krashen's monitor hypothesis. A combination of face-to-face teaching and CALL is usually referred to as blended learning. Blended learning is designed to increase learning potential and is more commonly found than pure CALL (Pegrum 2009: p. 27). See Davies et al. (2011: Section 1.1, What is CALL?). See also Levy & Hubbard (2005), who raise the question Why call CALL \"CALL\"?",
    "Seq2seq": "Seq2seq is a family of machine learning approaches used for language processing. Applications include language translation, image captioning, conversational models and text summarization.",
    "ELIZA": "ELIZA is an early natural language processing computer program created from 1964 to 1966 at the MIT Artificial Intelligence Laboratory by Joseph Weizenbaum. Created to demonstrate the superficiality of communication between humans and machines, Eliza simulated conversation by using a \"pattern matching\" and substitution methodology that gave users an illusion of understanding on the part of the program, but had no built in framework for contextualizing events. Directives on how to interact were provided by \"scripts\", written originally in MAD-Slip, which allowed ELIZA to process user inputs and engage in discourse following the rules and directions of the script. The most famous script, DOCTOR, simulated a Rogerian psychotherapist (in particular, Carl Rogers, who was well-known for simply parroting back at patients what they'd just said),and used rules, dictated in the script, to respond with non-directional questions to user inputs. As such, ELIZA was one of the first chatterbots and one of the first programs capable of attempting the Turing test. ELIZA's creator, Weizenbaum regarded the program as a method to show the superficiality of communication between man and machine, but was surprised by the number of individuals who attributed human-like feelings to the computer program, including Weizenbaum\u2019s secretary. Many academics believed that the program would be able to positively influence the lives of many people, particularly those suffering from psychological issues, and that it could aid doctors working on such patients' treatment. While ELIZA was capable of engaging in discourse, ELIZA could not converse with true understanding. However, many early users were convinced of ELIZA\u2019s intelligence and understanding, despite Weizenbaum\u2019s insistence to the contrary.",
    "Grammar_induction": "Grammar induction (or grammatical inference) is the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.",
    "Automatic_summarization": "Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content. In addition to text, images and videos can also be summarized. Text summarization finds the most informative sentences in a document; image summarization finds the most representative images within an image collection; video summarization extracts the most important frames from the video content.",
    "World_Wide_Web": "The World Wide Web (WWW), commonly known as the Web, is an information system where documents and other web resources are identified by Uniform Resource Locators (URLs, such as https://example.com/), which may be interlinked by hypertext, and are accessible over the Internet. The resources of the Web are transferred via the Hypertext Transfer Protocol (HTTP) and may be accessed by users by a software application called a web browser and are published by a software application called a web server. English engineer and computer scientist Sir Timothy John Berners-Lee invented the World Wide Web in 1989. He wrote the first web browser in 1990 while employed at CERN near Geneva, Switzerland. The browser was released outside CERN in 1991, first to other research institutions starting in January 1991 and then to the general public in August 1991. The World Wide Web has been central to the development of the Information Age and is the primary tool billions of people use to interact on the Internet. Web resources may be any type of downloaded media, but web pages are hypertext media that have been formatted in Hypertext Markup Language (HTML). Such formatting allows for embedded hyperlinks that contain URLs and permit users to navigate to other web resources. In addition to text, web pages may contain references to images, video, audio, and software components which are displayed in the user's web browser as coherent pages of multimedia content. Multiple web resources with a common theme, a common domain name, or both, make up a website. Websites are stored in computers that are running a program called a web server that responds to requests made over the Internet from web browsers running on a user's computer. Website content can be largely provided by a publisher, or interactively where users contribute content or the content depends upon the users or their actions. Websites may be provided for a myriad of informative, entertainment, commercial, governmental, or non-governmental reasons.",
    "Discourse": "Discourse (from Latin: discursus, lit. 'running to and from') generally denotes written and spoken communications, though its usage differs between various disciplines and approaches. For instance, in semantics and discourse analysis, it is a conceptual generalization of conversation within each modality and context of communication. Moreover, in regard to semantics, discourse is understood as the totality of codified language (i.e., vocabulary) used in a given field of intellectual enquiry and of social practice, such as legal discourse, medical discourse, religious discourse, etc. In the work of philosopher Michel Foucault, and that of the social theoreticians Foucault inspired, discourse describes \"an entity of sequences, of signs, in that they are\", statements (French: \u00e9nonc\u00e9s) in conversation. As discourse, a statement is not a unit of semiotic signs, but an abstract construct that allows such signs to assign meaning, thus conveying specific, repeatable communications to, between, and among objects, subjects, and statements. As such, a discourse is composed of semiotic sequences (relations among signs that communicate meaning) between and among objects, subjects, and statements. In simple terms, Foucault's analysis of a discourse examines and determines the connections among language, as well as structure and agency. Foucault applied what he called \"discursive formation\" (French: formation discursive), a term that conceptually describes the regular communications (written and spoken) that produce such discourses (e.g. informal conversations), in his analyses of large bodies of knowledge, such as political economy and natural history.",
    "Relationship_extraction": "A relationship extraction task requires the detection and classification of semantic relationship mentions within a set of artifacts, typically from text or XML documents. The task is very similar to that of information extraction (IE), but IE additionally requires the removal of repeated relations () and generally refers to the extraction of many different relationships.",
    "Racter": "Racter is an artificial intelligence computer program that generates English language prose at random.",
    "Natural-language_user_interface": "Natural-language user interface (LUI or NLUI) is a type of computer human interface where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications. In interface design, natural-language interfaces are sought after for their speed and ease of use, but most suffer the challenges to understanding wide varieties of ambiguous input.Natural-language interfaces are an active area of study in the field of natural-language processing and computational linguistics. An intuitive general natural-language interface is one of the active goals of the Semantic Web. Text interfaces are \"natural\" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional keyword search engine could be described as a \"shallow\" natural-language user interface.",
    "Word_embedding": "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension. Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear. Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis.",
    "Probability": "Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur or how likely it is that a proposition is true. The probability of an event is a number between 0 and 1, where, roughly speaking, 0 indicates impossibility of the event and 1 indicates certainty. The higher the probability of an event, the more likely it is that the event will occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (\"heads\" and \"tails\") are both equally probable; the probability of \"heads\" equals the probability of \"tails\"; and since no other outcomes are possible, the probability of either \"heads\" or \"tails\" is 1/2 (which could also be written as 0.5 or 50%). These concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in such areas of study as mathematics, statistics, finance, gambling, science (in particular physics), artificial intelligence/machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.",
    "WordNet": "WordNet is a lexical database of semantic relations between words in more than 200 languages. WordNet links words into semantic relations including synonyms, hyponyms, and meronyms. The synonyms are grouped into synsets with short definitions and usage examples. WordNet can thus be seen as a combination and extension of a dictionary and thesaurus. While it is accessible to human users via a web browser, its primary use is in automatic text analysis and artificial intelligence applications. WordNet was first created in the English language and the English WordNet database and software tools have been released under a BSD style license and are freely available for download from that WordNet website.",
    "AI-complete": "In the field of artificial intelligence, the most difficult problems are informally known as AI-complete or AI-hard, implying that the difficulty of these computational problems, assuming intelligence is computational, is equivalent to that of solving the central artificial intelligence problem\u2014making computers as intelligent as people, or strong AI. To call a problem AI-complete reflects an attitude that it would not be solved by a simple specific algorithm. AI-complete problems are hypothesised to include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem. Currently, AI-complete problems cannot be solved with modern computer technology alone, but would also require human computation. This property could be useful, for example, to test for the presence of humans as CAPTCHAs aim to do, and for computer security to circumvent brute-force attacks.",
    "Probabilistic_context-free_grammar": "Grammar theory to model symbol strings originated from work in computational linguistics aiming to understand the structure of natural languages. Probabilistic context free grammars (PCFGs) have been applied in probabilistic modeling of RNA structures almost 40 years after they were introduced in computational linguistics. PCFGs extend context-free grammars similar to how hidden Markov models extend regular grammars. Each production is assigned a probability. The probability of a derivation (parse) is the product of the probabilities of the productions used in that derivation. These probabilities can be viewed as parameters of the model, and for large problems it is convenient to learn these parameters via machine learning. A probabilistic grammar's validity is constrained by context of its training dataset. PCFGs have application in areas as diverse as natural language processing to the study the structure of RNA molecules and design of programming languages. Designing efficient PCFGs has to weigh factors of scalability and generality. Issues such as grammar ambiguity must be resolved. The grammar design affects results accuracy. Grammar parsing algorithms have various time and memory requirements.",
    "Latent_semantic_analysis": "Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Documents are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two columns. Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents. An information retrieval technique using latent semantic structure was patented in 1988 (US Patent 4,839,853, now expired) by Scott Deerwester, Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer,  and . In the context of its application to information retrieval, it is sometimes called latent semantic indexing (LSI).",
    "Natural-language_programming": "Natural-language programming (NLP) is an ontology-assisted way of programming in terms of natural-language sentences, e.g. English. A structured document with Content, sections and subsections for explanations of sentences forms a NLP document, which is actually a computer program. Natural languages and natural-language user interfaces include Inform 7, a natural programming language for making interactive fiction, Ring, a general-purpose language, Shakespeare, an esoteric natural programming language in the style of the plays of William Shakespeare, and Wolfram Alpha, a computational knowledge engine, using natural-language input. Some methods for program synthesis are based on natural-language programming.",
    "Meitei_language": "Meitei, or Meetei (also Manipuri ; Meitheilon, Meeteilon, Mee\u0281teilon, from Meithei + -lon 'language'; Kathe) is a Sino-Tibetan language and the predominant language and lingua franca in the southeastern Himalayan state of Manipur, in northeastern India. It is the one of the official languages of the Government of India. Meitei is the most spoken among indigenous languages in Northeast India after Assamese. In the 2011 census of India, there were 1.8 million native speakers of Meitei. Additionally, there are around 200,000 native speakers of Meitei abroad. Meiteilon is also spoken in the Northeast Indian states of Assam and Tripura and in Bangladesh and Burma (now Myanmar). It is currently classified as a vulnerable language by UNESCO. Meiteilon is a tonal language whose exact classification within Sino-Tibetan remains unclear. It has lexical resemblances to Kuki and Tangkhul Naga. It has been recognised (under the name Manipuri) by the Indian Union and has been included in the list of scheduled languages (included in the 8th schedule by the 71st amendment of the constitution in 1992). Meiteilon is taught as a subject up to the post-graduate level (Ph.D.) in some universities of India, apart from being a medium of instruction up to the undergraduate level in Manipur. Education in government schools is provided in Meiteilon through the eighth standard.",
    "Thematic_relation": "In certain theories of linguistics, thematic relations, also known as semantic roles, are the various roles that a noun phrase may play with respect to the action or state described by a governing verb, commonly the sentence's main verb. For example, in the sentence \"Susan ate an apple\", Susan is the doer of the eating, so she is an agent; the apple is the item that is eaten, so it is a patient.While most modern linguistic theories make reference to such relations in one form or another, the general term, as well as the terms for specific relations, varies:\"participant role\", \"semantic role\", and \"deep case\" have also been employed with similar sense.",
    "Speech_segmentation": "Speech segmentation is the process of identifying the boundaries between words, syllables, or phonemes in spoken natural languages. The term applies both to the mental processes used by humans, and to artificial processes of natural language processing. Speech segmentation is a subfield of general speech perception and an important subproblem of the technologically focused field of speech recognition, and cannot be adequately solved in isolation. As in most natural language processing problems, one must take into account context, grammar, and semantics, and even so the result is often a probabilistic division (statistically based on likelihood) rather than a categorical one. Though it seems that coarticulation\u2014a phenomenon which may happen between adjacent words just as easily as within a single word\u2014presents the main challenge in speech segmentation across languages, some other problems and strategies employed in solving those problems can be seen in the following sections. This problem overlaps to some extent with the problem of text segmentation that occurs in some languages which are traditionally written without inter-word spaces, like Chinese and Japanese, compared to writing systems which indicate speech segmentation between words by a word divider, such as the space. However, even for those languages, text segmentation is often much easier than speech segmentation, because the written language usually has little interference between adjacent words, and often contains additional clues not present in speech (such as the use of Chinese characters for word stems in Japanese).",
    "Coarticulation": "Coarticulation in its general sense refers to a situation in which a conceptually isolated speech sound is influenced by, and becomes more like, a preceding or following speech sound. There are two types of coarticulation: anticipatory coarticulation, when a feature or characteristic of a speech sound is anticipated (assumed) during the production of a preceding speech sound; and carryover or perseverative coarticulation, when the effects of a sound are seen during the production of sound(s) that follow. Many models have been developed to account for coarticulation. They include the look-ahead, articulatory syllable, time-locked, window, coproduction and articulatory phonology models. Coarticulation in phonetics refers to two different phenomena: \n* the assimilation of the place of articulation of one speech sound to that of an adjacent speech sound. For example, while the sound /n/ of English normally has an alveolar place of articulation, in the word tenth it is pronounced with a dental place of articulation because the following sound, /\u03b8/, is dental. \n* the production of a co-articulated consonant, that is, a consonant with two simultaneous places of articulation. An example of such a sound is the voiceless labial-velar plosive /k\u0361p/ found in many West African languages. The term coarticulation may also refer to the transition from one articulatory gesture to another.",
    "Machine_learning": "Machine learning (ML) is the study of computer algorithms that improve automatically through experience. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks. Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. In its application across business problems, machine learning is also referred to as predictive analytics.",
    "Category:Computational_linguistics": "",
    "Natural_language": "In neuropsychology, linguistics, and the philosophy of language, a natural language or ordinary language is any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forms, such as speech or signing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic.",
    "Lexical_analysis": "In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer, or scanner, although scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.",
    "Proofreading": "Proofreading is the reading of a galley proof or an electronic copy of a publication to find and correct production errors of text or art. Proofreading is the final step in the editorial cycle before publication.",
    "Text_corpus": "In linguistics, a corpus (plural corpora) or text corpus is a language resource consisting of a large and structured set of texts (nowadays usually electronically stored and processed). In corpus linguistics, they are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.",
    "Chinese_room": "The Chinese room argument holds that a digital computer executing a program cannot be shown to have a \"mind\", \"understanding\" or \"consciousness\", regardless of how intelligently or human-like the program may make the computer behave. The argument was first presented by philosopher John Searle in his paper, \"Minds, Brains, and Programs\", published in Behavioral and Brain Sciences in 1980. It has been widely discussed in the years since. The centerpiece of the argument is a thought experiment known as the Chinese room. The argument is directed against the philosophical positions of functionalism and computationalism, which hold that the mind may be viewed as an information-processing system operating on formal symbols, and that simulation of a given mental state is sufficient for its presence. Specifically, the argument is intended to refute a position Searle calls strong AI: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Although it was originally presented in reaction to the statements of artificial intelligence (AI) researchers, it is not an argument against the goals of mainstream AI research, because it does not limit the amount of intelligence a machine can display. The argument applies only to digital computers running programs and does not apply to machines in general.",
    "Discourse_representation_theory": "In formal linguistics, discourse representation theory (DRT) is a framework for exploring meaning under a formal semantics approach. One of the main differences between DRT-style approaches and traditional Montagovian approaches is that DRT includes a level of abstract mental representations (discourse representation structures, DRS) within its formalism, which gives it an intrinsic ability to handle meaning across sentence boundaries. DRT was created by Hans Kamp in 1981. A very similar theory was developed independently by Irene Heim in 1982, under the name of File Change Semantics (FCS). Discourse representation theories have been used to implement semantic parsers and natural language understanding systems.",
    "Information_retrieval": "Information retrieval (IR) is the activity of obtaining information system resources that are relevant to an information need from a collection of those resources. Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds. Automated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents. Web search engines are the most visible IR applications.",
    "Transformer_(machine_learning_model)": "The Transformer is a deep learning model introduced in 2017, used primarily in the field of natural language processing (NLP). Like recurrent neural networks (RNNs), Transformers are designed to handle sequential data, such as natural language, for tasks such as translation and text summarization. However, unlike RNNs, Transformers do not require that the sequential data be processed in order. For example, if the input data is a natural language sentence, the Transformer does not need to process the beginning of it before the end. Due to this feature, the Transformer allows for much more parallelization than RNNs and therefore reduced training times. Since their introduction, Transformers have become the model of choice for tackling many problems in NLP, replacing older recurrent neural network models such as the long short-term memory (LSTM). Since the Transformer model facilitates more parallelization during training, it has enabled training on larger datasets than was possible before it was introduced. This has led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which have been trained with huge general language datasets, and can be fine-tuned to specific language tasks.",
    "Speech_processing": "Speech processing is the study of speech signals and the processing methods of signals. The signals are usually processed in a digital representation, so speech processing can be regarded as a special case of digital signal processing, applied to speech signals. Aspects of speech processing includes the acquisition, manipulation, storage, transfer and output of speech signals. The input is called speech recognition and the output is called speech synthesis.",
    "Morpheme": "A morpheme is the smallest meaningful unit in a language. A morpheme is not identical to a word. The main difference between them is that a morpheme sometimes does not stand alone, but a word, by definition, always stands alone. The linguistics field of study dedicated to morphemes is called morphology. When a morpheme stands by itself, it is considered as a root because it has a meaning of its own (such as the morpheme cat). When it depends on another morpheme to express an idea, it is an affix because it has a grammatical function (such as the \u2013s in cats to indicate that it is plural). Every word is composed of one or more morphemes.",
    "Knowledge_extraction": "Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criteria is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data. The RDB2RDF W3C group  is currently standardizing a language for extraction of resource description frameworks (RDF) from relational databases. Another popular example for knowledge extraction is the transformation of Wikipedia into structured data and also the mapping to existing knowledge (see DBpedia and Freebase).",
    "Stative_verb": "According to some linguistics theories, a stative verb is one that describes a state of being, in contrast to a dynamic verb, which describes an action. The difference can be categorized by saying that stative verbs describe situations that are static or unchanging throughout their entire duration, whereas dynamic verbs describe processes that entail change over time. Many languages distinguish between these two types in terms of how they can be used grammatically.",
    "Spoken_dialog_systems": "A spoken dialog system is a computer system able to converse with a human with voice. It has two essential components that do not exist in a written text dialog system: a speech recognizer and a text-to-speech module (written text dialog systems usually use other input systems provided by an OS). In can be further distinguished from command and control speech systems that can respond to requests but do not attempt to maintain continuity over time.",
    "Speech_synthesis": "Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech. Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely \"synthetic\" voice output. The quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with visual impairments or reading disabilities to listen to written words on a home computer. Many computer operating systems have included speech synthesizers since the early 1990s. A text-to-speech system (or \"engine\") is composed of two parts: a front-end and a back-end. The front-end has two major tasks. First, it converts raw text containing symbols like numbers and abbreviations into the equivalent of written-out words. This process is often called text normalization, pre-processing, or tokenization. The front-end then assigns phonetic transcriptions to each word, and divides and marks the text into prosodic units, like phrases, clauses, and sentences. The process of assigning phonetic transcriptions to words is called text-to-phoneme or grapheme-to-phoneme conversion. Phonetic transcriptions and prosody information together make up the symbolic linguistic representation that is output by the front-end. The back-end\u2014often referred to as the synthesizer\u2014then converts the symbolic linguistic representation into sound. In certain systems, this part includes the computation of the target prosody (pitch contour, phoneme durations), which is then imposed on the output speech.",
    "Part-of-speech_tagging": "In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context.A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc. Once performed by hand, POS tagging is now done in the context of computational linguistics, using algorithms which associate discrete terms, as well as hidden parts of speech, by a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill's tagger, one of the first and most widely used English POS-taggers, employs rule-based algorithms.",
    "Neural_machine_translation": "Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.",
    "Vocabulary": "A vocabulary, also known as a wordstock or word-stock, is a set of familiar words within a person's language. A vocabulary, usually developed with age, serves as a useful and fundamental tool for communication and acquiring knowledge. Acquiring an extensive vocabulary is one of the largest challenges in learning a second language.",
    "Discourse_analysis": "Discourse analysis (DA), or discourse studies, is an approach to the analysis of written, vocal, or sign language use, or any significant semiotic event. The objects of discourse analysis (discourse, writing, conversation, communicative event) are variously defined in terms of coherent sequences of sentences, propositions, speech, or turns-at-talk. Contrary to much of traditional linguistics, discourse analysts not only study language use 'beyond the sentence boundary' but also prefer to analyze 'naturally occurring' language use, not invented examples. Text linguistics is a closely related field. The essential difference between discourse analysis and text linguistics is that discourse analysis aims at revealing socio-psychological characteristics of a person/persons rather than text structure. Discourse analysis has been taken up in a variety of disciplines in the humanities and social sciences, including linguistics, education, sociology, anthropology, social work, cognitive psychology, social psychology, area studies, cultural studies, international relations, human geography, environmental science, communication studies, biblical studies, public relations and translation studies, each of which is subject to its own assumptions, dimensions of analysis, and methodologies.",
    "Distributional_semantics": "Distributional semantics is a research area that develops and studies theories and methods for quantifying and categorizing semantic similarities between linguistic items based on their distributional properties in large samples of language data. The basic idea of distributional semantics can be summed up in the so-called Distributional hypothesis: linguistic items with similar distributions have similar meanings.",
    "Named-entity_recognition": "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. Most research on NER systems has been structured as taking an unannotated block of text, such as this one: Jim bought 300 shares of Acme Corp. in 2006. And producing an annotated block of text that highlights the names of entities: [Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time. In this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified. State-of-the-art NER systems for English produce near-human performance. For example, the best system entering MUC-7 scored 93.39% of F-measure while human annotators scored 97.60% and 96.95%.",
    "Unsupervised_learning": "Unsupervised learning is a type of machine learning that looks for previously undetected patterns in a data set with no pre-existing labels and with a minimum of human supervision. In contrast to supervised learning that usually makes use of human-labeled data, unsupervised learning, also known as self-organization allows for modeling of probability densities over inputs. It forms one of the three main categories of machine learning, along with supervised and reinforcement learning. Semi-supervised learning, a related variant, makes use of supervised and unsupervised techniques. Two of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships. Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group. A central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It could be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution  conditioned on the label  of input data; unsupervised learning intends to infer an a priori probability distribution . Generative adversarial networks can also be used with supervised learning, though they can also be applied to unsupervised and reinforcement techniques.",
    "Morphology_(linguistics)": "In linguistics, morphology () is the study of words, how they are formed, and their relationship to other words in the same language. It analyzes the structure of words and parts of words, such as stems, root words, prefixes, and suffixes. Morphology also looks at parts of speech, intonation and stress, and the ways context can change a word's pronunciation and meaning. Morphology differs from morphological typology, which is the classification of languages based on their use of words, and lexicology, which is the study of words and how they make up a language's vocabulary. While words, along with clitics, are generally accepted as being the smallest units of syntax, in most languages, if not all, many words can be related to other words by rules that collectively describe the grammar for that language. For example, English speakers recognize that the words dog and dogs are closely related, differentiated only by the plurality morpheme \"-s\", only found bound to noun phrases. Speakers of English, a fusional language, recognize these relations from their innate knowledge of English's rules of word formation. They infer intuitively that dog is to dogs as cat is to cats; and, in similar fashion, dog is to dog catcher as dish is to dishwasher. By contrast, Classical Chinese has very little morphology, using almost exclusively unbound morphemes (\"free\" morphemes) and depending on word order to convey meaning. (Most words in modern Standard Chinese [\"Mandarin\"], however, are compounds and most roots are bound.) These are understood as grammars that represent the morphology of the language. The rules understood by a speaker reflect specific patterns or regularities in the way words are formed from smaller units in the language they are using, and how those smaller units interact in speech. In this way, morphology is the branch of linguistics that studies patterns of word formation within and across languages and attempts to formulate rules that model the knowledge of the speakers of those languages. Phonological and orthographic modifications between a base word and its origin may be partial to literacy skills. Studies have indicated that the presence of modification in phonology and orthography makes morphologically complex words harder to understand and that the absence of modification between a base word and its origin makes morphologically complex words easier to understand. Morphologically complex words are easier to comprehend when they include a base word. Polysynthetic languages, such as Chukchi, have words composed of many morphemes. The Chukchi word \"t\u0259mey\u014b\u0259levtp\u0259\u03b3t\u0259rk\u0259n\", for example, meaning \"I have a fierce headache\", is composed of eight morphemes t-\u0259-mey\u014b-\u0259-levt-p\u0259\u03b3t-\u0259-rk\u0259n that may be glossed. The morphology of such languages allows for each consonant and vowel to be understood as morphemes, while the grammar of the language indicates the usage and understanding of each morpheme. The discipline that deals specifically with the sound changes occurring within morphemes is morphophonology.",
    "Turing_test": "The Turing test, developed by Alan Turing in 1950, is a test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human. Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses. The evaluator would be aware that one of the two partners in conversation is a machine, and all participants would be separated from one another. The conversation would be limited to a text-only channel such as a computer keyboard and screen so the result would not depend on the machine's ability to render words as speech. If the evaluator cannot reliably tell the machine from the human, the machine is said to have passed the test. The test results do not depend on the machine's ability to give correct answers to questions, only how closely its answers resemble those a human would give. The test was introduced by Turing in his 1950 paper, \"Computing Machinery and Intelligence\", while working at the University of Manchester (Turing, 1950; p. 460). It opens with the words: \"I propose to consider the question, 'Can machines think?'\" Because \"thinking\" is difficult to define, Turing chooses to \"replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.\" Turing describes the new form of the problem in terms of a three-person game called the \"imitation game\", in which an interrogator asks questions of a man and a woman in another room in order to determine the correct sex of the two players. Turing's new question is: \"Are there imaginable digital computers which would do well in the imitation game?\" This question, Turing believed, is one that can actually be answered. In the remainder of the paper, he argued against all the major objections to the proposition that \"machines can think\". Since Turing first introduced his test, it has proven to be both highly influential and widely criticised, and it has become an important concept in the philosophy of artificial intelligence. Some of these criticisms, such as John Searle's Chinese room, are controversial in their own right.",
    "John_Searle": "John Rogers Searle (; born July 31, 1932) is an American philosopher. He was Willis S. and Marion Slusser Professor Emeritus of the Philosophy of Mind and Language and Professor of the Graduate School at the University of California, Berkeley. Widely noted for his contributions to the philosophy of language, philosophy of mind, and social philosophy, he began teaching at UC Berkeley in 1959. As an undergraduate at the University of Wisconsin\u2013Madison, Searle was secretary of \"Students against Joseph McCarthy\". He received all his university degrees, BA, MA, and DPhil, from the University of Oxford, where he held his first faculty positions. Later, at UC Berkeley, he became the first tenured professor to join the 1964\u20131965 Free Speech Movement. In the late 1980s, Searle challenged the restrictions of Berkeley's 1980 rent stabilization ordinance. Following what came to be known as the California Supreme Court's \"Searle Decision\" of 1990, Berkeley changed its rent control policy, leading to large rent increases between 1991 and 1994. In 2000 Searle received the Jean Nicod Prize; in 2004, the National Humanities Medal; and in 2006, the Mind & Brain Prize. Searle's early work on speech acts, influenced by J. L. Austin and Ludwig Wittgenstein, helped establish his reputation. His notable concepts include the \"Chinese room\" argument against \"strong\" artificial intelligence. In June 2019, Searle was stripped of his emeritus status at the University of California, Berkeley, having violated the university\u2019s sexual harassment policies.",
    "Blocks_world": "The blocks world is one of the most famous planning domains in artificial intelligence. The algorithm is similar to a set of wooden blocks of various shapes and colors sitting on a table. The goal is to build one or more vertical stacks of blocks. Only one block may be moved at a time: it may either be placed on the table or placed atop another block. Because of this, any blocks that are, at a given time, under another block cannot be moved. Moreover, some kinds of blocks cannot have other blocks stacked on top of them. The simplicity of this toy world lends itself readily to classical symbolic artificial intelligence approaches, in which the world is modeled as a set of abstract symbols which may be reasoned about.",
    "Deep_learning": "Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep learning architectures such as , deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, machine vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance. Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog. The adjective \"deep\" in deep learning comes from the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, and then that a network with a nonpolynomial activation function with one hidden layer of unbounded width can on the other hand so be. Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, whence the \"structured\" part.",
    "Lemmatisation": "Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. In computational linguistics, lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. As a result, developing efficient lemmatisation algorithms is an open area of research.",
    "Natural-language_understanding": "Natural-language understanding (NLU) or natural-language interpretation (NLI) is a subtopic of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem. There is considerable commercial interest in the field because of its application to automated reasoning, machine translation, question answering, news-gathering, text categorization, voice-activation, archiving, and large-scale content analysis.",
    "Pronoun": "In linguistics and grammar, a pronoun (abbreviated PRO) is a word that substitutes for a noun or noun phrase. It is a particular case of a pro-form. Pronouns have traditionally been regarded as one of the parts of speech, but some modern theorists would not consider them to form a single class, in view of the variety of functions they perform cross-linguistically. An example of a pronoun is \"you\", which is both plural and singular. Subtypes include personal and possessive pronouns, reflexive and reciprocal pronouns, demonstrative pronouns, relative and interrogative pronouns, and indefinite pronouns. The use of pronouns often involves anaphora, where the meaning of the pronoun is dependent on an antecedent. For example, in the sentence That poor man looks as if he needs a new coat, the antecedent of the pronoun he is dependent on that poor man. The adjective associated with \"pronoun\" is \"pronominal\". A pronominal is also a word or phrase that acts as a pronoun. For example, in That's not the one I wanted, the phrase the one (containing the prop-word one) is a pronominal.",
    "Alan_Turing": "Alan Mathison Turing  (; 23 June 1912 \u2013 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer. Turing is widely considered to be the father of theoretical computer science and artificial intelligence. Despite these accomplishments, he was never fully recognised in his home country during his lifetime due to his homosexuality and because much of his work was covered by the Official Secrets Act. During the Second World War, Turing worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the section that was responsible for German naval cryptanalysis. Here, he devised a number of techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bombe method, an electromechanical machine that could find settings for the Enigma machine. Turing played a crucial role in cracking intercepted coded messages that enabled the Allies to defeat the Nazis in many crucial engagements, including the Battle of the Atlantic, and in so doing helped win the war. Due to the problems of counterfactual history, it is hard to estimate the precise effect Ultra intelligence had on the war, but at the upper end it has been estimated that this work shortened the war in Europe by more than two years and saved over 14 million lives. After the war Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine. The Automatic Computing Engine was one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory, at the Victoria University of Manchester, where he helped develop the Manchester computers and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis and predicted oscillating chemical reactions such as the Belousov\u2013Zhabotinsky reaction, first observed in the 1960s. Turing was prosecuted in 1952 for homosexual acts; the Labouchere Amendment of 1885 had mandated that \"gross indecency\" was a criminal offence in the UK. He accepted chemical castration treatment, with DES, as an alternative to prison. Turing died in 1954, 16 days before his 42nd birthday, from cyanide poisoning. An inquest determined his death as a suicide, but it has been noted that the known evidence is also consistent with accidental poisoning. In 2009, following an Internet campaign, British Prime Minister Gordon Brown made an  on behalf of the British government for \"the appalling way he was treated\". Queen Elizabeth II granted Turing a posthumous pardon in 2013. The \"Alan Turing law\" is now an informal term for a 2017 law in the United Kingdom that retroactively pardoned men cautioned or convicted under historical legislation that outlawed homosexual acts.",
    "Inflection": "In linguistic morphology, inflection (or inflexion) is a process of word formation, in which a word is modified to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, mood, animacy, and definiteness. The inflection of verbs is called conjugation, and one can refer to the inflection of nouns, adjectives, adverbs, pronouns, determiners, participles, prepositions and postpositions, numerals, articles etc., as declension. An inflection expresses grammatical categories with affixation (such as prefix, suffix, infix, circumfix, and transfix), apophony (as Indo-European ablaut), or other modifications. For example, the Latin verb ducam, meaning \"I will lead\", includes the suffix -am, expressing person (first), number (singular), and tense-mood (future indicative or present subjunctive). The use of this suffix is an inflection. In contrast, in the English clause \"I will lead\", the word lead is not inflected for any of person, number, or tense; it is simply the bare form of a verb. The inflected form of a word often contains both one or more free morphemes (a unit of meaning which can stand by itself as a word), and one or more bound morphemes (a unit of meaning which cannot stand alone as a word). For example, the English word cars is a noun that is inflected for number, specifically to express the plural; the content morpheme car is unbound because it could stand alone as a word, while the suffix -s is bound because it cannot stand alone as a word. These two morphemes together form the inflected word cars. Words that are never subject to inflection are said to be invariant; for example, the English verb must is an invariant item: it never takes a suffix or changes form to signify a different grammatical category. Its categories can be determined only from its context. Languages that seldom make use of inflection, such as Standard Chinese, are said to be analytic or isolating. Requiring the forms or inflections of more than one word in a sentence to be compatible with each other according to the rules of the language is known as concord or agreement. For example, in \"the choir sings\", \"choir\" is a singular noun, so \"sing\" is constrained in the present tense to use the third person singular suffix \"s\". The sentence *\"the choir sing\" is not grammatically correct in English. Languages that have some degree of inflection are synthetic languages. These can be highly inflected (such as Latin, Greek, Biblical Hebrew, and Sanskrit), or weakly inflected (such as English). Languages that are so inflected that a sentence can consist of a single highly inflected word (such as many Native American languages) are called polysynthetic languages. Languages in which each inflection conveys only a single grammatical category, such as Finnish, are known as agglutinative languages, while languages in which a single inflection can convey multiple grammatical roles (such as both nominative case and plural, as in Latin and German) are called fusional.",
    "SHRDLU": "SHRDLU was an early natural language understanding computer program, developed by Terry Winograd at MIT in 1968\u20131970. In the program, the user carries on a conversation with the computer, moving objects, naming collections and querying the state of a simplified \"blocks world\", essentially a virtual box filled with different blocks. SHRDLU was written in the Micro Planner and Lisp programming language on the DEC PDP-6 computer and a DEC graphics terminal. Later additions were made at the computer graphics labs at the University of Utah, adding a full 3D rendering of SHRDLU's \"world\". The name SHRDLU was derived from ETAOIN SHRDLU, the arrangement of the letter keys on a Linotype machine, arranged in descending order of usage frequency in English.",
    "Word-sense_disambiguation": "In computational linguistics, word-sense disambiguation (WSD) is an open problem concerned with identifying which sense of a word is used in a sentence. The solution to this issue impacts other computer-related writing, such as discourse, improving relevance of search engines, anaphora resolution, coherence, and inference. The human brain is quite proficient at word-sense disambiguation. That natural language is formed in a way that requires so much of it is a reflection of that neurologic reality. In other words, human language developed in a way that reflects (and also has helped to shape) the innate ability provided by the brain's neural networks. In computer science and the information technology that it enables, it has been a long-term challenge to develop the ability in computers to do natural language processing and machine learning. A rich variety of techniques have been researched, from dictionary-based methods that use the knowledge encoded in lexical resources, to supervised machine learning methods in which a classifier is trained for each distinct word on a corpus of manually sense-annotated examples, to completely unsupervised methods that cluster occurrences of words, thereby inducing word senses. Among these, supervised learning approaches have been the most successful algorithms to date. Accuracy of current algorithms is difficult to state without a host of caveats. In English, accuracy at the coarse-grained (homograph) level is routinely above 90%, with some methods on particular homographs achieving over 96%. On finer-grained sense distinctions, top accuracies from 59.1% to 69.0% have been reported in evaluation exercises (SemEval-2007, Senseval-2), where the baseline accuracy of the simplest possible algorithm of always choosing the most frequent sense was 51.4% and 57%, respectively.",
    "Adjective": "In linguistics, an adjective (abbreviated adj) is a word that modifies a noun or noun phrase or describes its referent. Its semantic role is to change information given by the noun. Adjectives are one of the main parts of speech of the English language, although historically they were classed together with nouns. Certain words that were traditionally considered to be adjectives, including the, this, my, etc., are today usually classed separately, as determiners.",
    "Hidden_Markov_model": "Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process \u2013 call it  \u2013 with unobservable (\"hidden\") states. HMM assumes that there is another process  whose behavior \"depends\" on . The goal is to learn about  by observing . HMM stipulates that, for each time instance , the conditional probability distribution of  given the history  must NOT depend on . Hidden Markov models are known for their applications to reinforcement learning and temporal pattern recognition such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics.",
    "Verb": "A verb, from the Latin verbum meaning word, is a word (part of speech) that in syntax conveys an action (bring, read, walk, run, learn), an occurrence (happen, become), or a state of being (be, exist, stand). In the usual description of English, the basic form, with or without the particle to, is the infinitive. In many languages, verbs are inflected (modified in form) to encode tense, aspect, mood, and voice. A verb may also agree with the person, gender or number of some of its arguments, such as its subject, or object. Verbs have tenses: present, to indicate that an action is being carried out; past, to indicate that an action has been done; future, to indicate that an action will be done.",
    "Semantics": "Semantics (from Ancient Greek: \u03c3\u03b7\u03bc\u03b1\u03bd\u03c4\u03b9\u03ba\u03cc\u03c2 s\u0113mantik\u00f3s, \"significant\") is the linguistic and philosophical study of meaning in language, programming languages, formal logic, and semiotics. It is concerned with the relationship between signifiers\u2014like words, phrases, signs, and symbols\u2014and what they stand for in reality, their denotation. In the international scientific vocabulary semantics is also called semasiology. The word semantics was first used by Michel Br\u00e9al, a French philologist. It denotes a range of ideas\u2014from the popular to the highly technical. It is often used in ordinary language for denoting a problem of understanding that comes down to word selection or connotation. This problem of understanding has been the subject of many formal enquiries, over a long period of time, especially in the field of formal semantics. In linguistics, it is the study of the interpretation of signs or symbols used in agents or communities within particular circumstances and contexts. Within this view, sounds, facial expressions, body language, and proxemics have semantic (meaningful) content, and each comprises several branches of study. In written language, things like paragraph structure and punctuation bear semantic content; other forms of language bear other semantic content. The formal study of semantics intersects with many other fields of inquiry, including lexicology, syntax, pragmatics, etymology and others. Independently, semantics is also a well-defined field in its own right, often with synthetic properties. In the philosophy of language, semantics and reference are closely connected. Further related fields include philology, communication, and semiotics. The formal study of semantics can therefore be manifold and complex. Semantics contrasts with syntax, the study of the combinatorics of units of a language (without reference to their meaning), and pragmatics, the study of the relationships between the symbols of a language, their meaning, and the users of the language. Semantics as a field of study also has significant ties to various representational theories of meaning including truth theories of meaning, coherence theories of meaning, and correspondence theories of meaning. Each of these is related to the general philosophical study of reality and the representation of meaning.",
    "Cache_language_model": "A cache language model is a type of statistical language model. These occur in the natural language processing subfield of computer science and assign probabilities to given sequences of words by means of a probability distribution. Statistical language models are key components of speech recognition systems and of many machine translation systems: they tell such systems which possible output word sequences are probable and which are improbable. The particular characteristic of a cache language model is that it contains a cache component and assigns relatively high probabilities to words or word sequences that occur elsewhere in a given text. The primary, but by no means sole, use of cache language models is in speech recognition systems. To understand why it is a good idea for a statistical language model to contain a cache component one might consider someone who is dictating a letter about elephants to a speech recognition system. Standard (non-cache) N-gram language models will assign a very low probability to the word \"elephant\" because it is a very rare word in English. If the speech recognition system does not contain a cache component the person dictating the letter may be annoyed: each time the word \"elephant\" is spoken another sequence of words with a higher probability according to the N-gram language model may be recognized (e.g., \"tell a plan\"). These erroneous sequences will have to be deleted manually and replaced in the text by \"elephant\" each time \"elephant\" is spoken. If the system has a cache language model, \"elephant\" will still probably be misrecognized the first time it is spoken and will have to be entered into the text manually; however, from this point on the system is aware that \"elephant\" is likely to occur again \u2013 the estimated probability of occurrence of \"elephant\" has been increased, making it more likely that if it is spoken it will be recognized correctly. Once \"elephant\" has occurred several times the system is likely to recognize it correctly every time it is spoken until the letter has been completely dictated. This increase in the probability assigned to the occurrence of \"elephant\" is an example of a consequence of machine learning and more specifically of pattern recognition. There exist variants of the cache language model in which not only single words but also multi-word sequences that have occurred previously are assigned higher probabilities (e.g., if \"San Francisco\" occurred near the beginning of the text subsequent instances of it would be assigned a higher probability). The cache language model was first proposed in a paper published in 1990, after which the IBM speech-recognition group experimented with the concept. The group found that implementation of a form of cache language model yielded a 24% drop in word-error rates once the first few hundred words of a document had been dictated. A detailed survey of language modeling techniques concluded that the cache language model was one of the few new language modeling techniques that yielded improvements over the standard N-gram approach: \"Our caching results show that caching is by far the most useful technique for perplexity reduction at small and medium training data sizes\". The development of the cache language model has generated considerable interest among those concerned with computational linguistics in general and statistical natural language processing in particular: recently there has been interest in applying the cache language model in the field of statistical machine translation. The success of the cache language model in improving word prediction rests on the human tendency to use words in a \"bursty\" fashion: when one is discussing a certain topic in a certain context the frequency with which one uses certain words will be quite different from their frequencies when one is discussing other topics in other contexts. The traditional N-gram language models, which rely entirely on information from a very small number (four, three, or two) of words preceding the word to which a probability is to be assigned, do not adequately model this \"burstiness\". Recently, the cache language model concept - originally conceived for the N-gram statistical language model paradigm - has been adapted for use in the neural paradigm. For instance, recent work on continuous cache language models in the recurrent neural network (RNN) setting has applied the cache concept to much larger contexts than before, yielding significant reductions in perplexity. Another recent line of research involves incorporating a cache component in a feed-forward neural language model (FN-LM) to achieve rapid domain adaptation .",
    "Ambiguity": "Ambiguity is a type of meaning in which a phrase, statement or resolution is not explicitly defined, making several interpretations plausible. A common aspect of ambiguity is uncertainty. It is thus an attribute of any idea or statement whose intended meaning cannot be definitively resolved according to a rule or process with a finite number of steps. (The ambi- part of the term reflects an idea of \"two\", as in \"two meanings\".) The concept of ambiguity is generally contrasted with vagueness. In ambiguity, specific and distinct interpretations are permitted (although some may not be immediately obvious), whereas with information that is vague, it is difficult to form any interpretation at the desired level of specificity. Context may play a role in resolving ambiguity. For example, the same piece of information may be ambiguous in one context and unambiguous in another.",
    "Parliament_of_Canada": "The Parliament of Canada (French: Parlement du Canada) is the federal legislature of Canada, seated at Parliament Hill in Ottawa, and is composed of three parts: the Monarch, the Senate, and the House of Commons. By constitutional convention, the House of Commons is dominant, with the Senate rarely opposing its will. The Senate reviews legislation from a less partisan standpoint and may initiate certain bills. The monarch or their representative, normally the Governor General, provides royal assent to make bills into law. The Governor General, on behalf of the monarch, summons and appoints the 105 senators on the advice of the Prime Minister, while the 338 members of the House of Commons\u2014called members of Parliament (MPs)\u2014each represent an electoral district, commonly referred to as a riding, and are directly elected by Canadian voters. The Governor General also summons Parliament, while either the viceroy or monarch can prorogue or dissolve Parliament, the latter in order to call a general election. Either will read the Throne Speech. The most recent Parliament, summoned by Governor General Julie Payette in 2019, is the 43rd since Confederation.",
    "Conceptual_metaphor": "In cognitive linguistics, conceptual metaphor, or cognitive metaphor, refers to the understanding of one idea, or conceptual domain, in terms of another. An example of this is the understanding of quantity in terms of directionality (e.g. \"the price of peace is rising\") or the understanding of time in terms of money (e.g. \"I spent time at work today\"). A conceptual domain can be any mental organization of human experience. The regularity with which different languages employ the same metaphors, often perceptually based, has led to the hypothesis that the mapping between conceptual domains corresponds to neural mappings in the brain. This theory has gained wide attention, although some researchers question its empirical accuracy. This idea, and a detailed examination of the underlying processes, was first extensively explored by George Lakoff and Mark Johnson in their work Metaphors We Live By in 1980. Since then, the field of metaphor studies within the larger discipline of cognitive linguistics has increasingly developed, with several annual academic conferences, scholarly societies, and research labs contributing to the subject area. Some researchers, such as Gerard Steen, have worked to develop empirical investigative tools for metaphor research, including the Metaphor Identification Procedure, or MIP. In Psychology, Raymond W. Gibbs, Jr., has investigated conceptual metaphor and embodiment through a number of psychological experiments. Other cognitive scientists, for example Gilles Fauconnier, study subjects similar to conceptual metaphor under the labels \"analogy\", \"conceptual blending\" and \"ideasthesia\". Conceptual metaphors are useful for understanding complex ideas in simple terms and therefore are frequently used to give insight to abstract theories and models. For example, the conceptual metaphor of viewing communication as a conduit is one large theory explained with a metaphor. So not only is our everyday communication shaped by the language of conceptual metaphors, but so is the very way we understand scholarly theories. These metaphors are prevalent in communication and we do not just use them in language; we actually perceive and act in accordance with the metaphors.",
    "Full_stop": "The full stop (Commonwealth English), period (North American English) or full point is a punctuation mark. It is used for several purposes, most often to mark the end of a declaratory sentence (as opposed to a question or exclamation); this sentence-terminal use, alone, defines the strictest sense of full stop. The full stop is also often used alone to indicate omitted characters, or in an ellipsis, \u2026, to indicate omitted words. It may be placed after an initial letter used to stand for a name, or sometimes after each individual letter in an initialism or acronym, for example, \"U.S.A.\"; however, this style is declining, and many initialisms like UK or NATO have individually become accepted norms. A full stop is also frequently used at the end of word abbreviations \u2013 in British usage, primarily truncations like Rev., but not after contractions like Revd; however, in American English it is used in both cases. In Anglophone countries, it is used for the decimal point and other purposes, and may be called a point. In computing, it is called a dot. It is sometimes called a  baseline dot to distinguish it from the interpunct (or middle dot). While full stop technically only applies to the full point when used to terminate a sentence, the distinction \u2013 drawn since at least 1897 \u2013 is not maintained by all modern style guides and dictionaries. The full stop symbol derives from the Greek punctuation introduced by Aristophanes of Byzantium in the 3rd century BC. In his system, there were a series of dots whose placement determined their meaning. The full stop at the end of a completed thought or expression was marked by a high dot \u27e8\u02d9\u27e9, called the stigm\u1e15 tele\u00eda (\u03c3\u03c4\u03b9\u03b3\u03bc\u1f74 \u03c4\u03b5\u03bb\u03b5\u03af\u03b1) or \"terminal dot\". The \"middle dot\" \u27e8\u00b7\u27e9, the stigm\u1e15 m\u00e9s\u0113 (\u03c3\u03c4\u03b9\u03b3\u03bc\u1f74 \u03bc\u03ad\u03c3\u03b7), marked a division in a thought occasioning a longer breath (essentially a semicolon) and the low dot \u27e8.\u27e9, called the hypostigm\u1e15 (\u1f51\u03c0\u03bf\u03c3\u03c4\u03b9\u03b3\u03bc\u03ae) or \"underdot\", marked a division in a thought occasioning a shorter breath (essentially a comma). In practice, scribes mostly employed the terminal dot; the others fell out of use and were later replaced by other symbols. From the 9th century, the full stop began appearing as a low mark instead of a high one; by the advent of printing in Western Europe, the low mark was regular and then universal. The name period is first attested (as the Latin loanword peridos) in \u00c6lfric of Eynsham's Old English treatment on grammar. There, it is distinguished from the full stop (the distinctio) and continues the Greek underdot's earlier function as a comma between phrases. It shifted its meaning to a dot marking a full stop in the works of the 16th-century grammarians. In 19th-century texts, both British English and American English were consistent in their usage of the terms period and full stop. The word period was used as a name for what printers often called the \"full point\" or the punctuation mark that was a dot on the baseline and used in several situations. The phrase full stop was only used to refer to the punctuation mark when it was used to terminate a sentence. This distinction seems to be eroding. For example, the 1998 edition of Fowler's Modern English Usage used full point for the character after an abbreviation, but full stop or full point at the end of a sentence; while the 2015 edition treats them as synonymous (and prefers full stop), and New Hart's Rules does likewise (but prefers full point). The last edition (1989) of the original Hart's Rules exclusively used full point.",
    "Agglutination": "Agglutination is a linguistic process pertaining to derivational morphology in which complex words are formed by stringing together morphemes without changing them in spelling or phonetics. Languages that use agglutination widely are called agglutinative languages. An example of such a language is Turkish, where, for example, the word evlerinizden, or \"from your houses\", consists of the morphemes ev-ler-iniz-den, literally translated morpheme-by-morpheme as house-plural-your-from. Agglutinative languages are often contrasted both with languages in which syntactic structure is expressed solely by means of word order and auxiliary words (isolating languages) and with languages in which a single affix typically expresses several syntactic categories and a single category may be expressed by several different affixes (as is the case in inflectional (fusional) languages). However, both fusional and isolating languages may use agglutination in the most-often-used constructs, and use agglutination heavily in certain contexts, such as word derivation. This is the case in English, which has an agglutinated plural marker -(e)s and derived words such as shame\u00b7less\u00b7ness. Agglutinative suffixes are often inserted irrespective of syllabic boundaries, for example, by adding a consonant to the syllable coda as in English tie \u2013 ties. Agglutinative languages also have large inventories of enclitics, which can be and are separated from the word root by native speakers in daily use. The term agglutination is sometimes used more generally to refer to the morphological process of adding suffixes or other morphemes to the base of a word. This subject is treated in more detail in the section on other uses of the term.",
    "Bag-of-words_model": "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision. The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier. An early reference to \"bag of words\" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure.",
    "Formal_grammar": "In formal language theory, a grammar (when the context is not given, often called a formal grammar for clarity) describes how to form strings from a language's alphabet that are valid according to the language's syntax. A grammar does not describe the meaning of the strings or what can be done with them in whatever context\u2014only their form. A formal grammar is defined as a set of production rules for strings in a formal language. Formal language theory, the discipline that studies formal grammars and languages, is a branch of applied mathematics. Its applications are found in theoretical computer science, theoretical linguistics, formal semantics, mathematical logic, and other areas. A formal grammar is a set of rules for rewriting strings, along with a \"start symbol\" from which rewriting starts. Therefore, a grammar is usually thought of as a language generator. However, it can also sometimes be used as the basis for a \"recognizer\"\u2014a function in computing that determines whether a given string belongs to the language or is grammatically incorrect. To describe such recognizers, formal language theory uses separate formalisms, known as automata theory. One of the interesting results of automata theory is that it is not possible to design a recognizer for certain formal languages.Parsing is the process of recognizing an utterance (a string in natural languages) by breaking it down to a set of symbols and analyzing each one against the grammar of the language. Most languages have the meanings of their utterances structured according to their syntax\u2014a practice known as compositional semantics. As a result, the first step to describing the meaning of an utterance in language is to break it down part by part and look at its analyzed form (known as its parse tree in computer science, and as its deep structure in generative grammar).",
    "Grammar": "In linguistics, grammar (from Ancient Greek \u03b3\u03c1\u03b1\u03bc\u03bc\u03b1\u03c4\u03b9\u03ba\u03ae) is the set of structural rules governing the composition of clauses, phrases and words in a natural language. The term refers also to the study of such rules and this field includes phonology, morphology and syntax, often complemented by phonetics, semantics and pragmatics. Fluent speakers of a language variety or lect have a set of internalized rules which constitutes its grammar. The vast majority of the information in the grammar is \u2013 at least in the case of one's native language \u2013 acquired not by conscious study or instruction but by hearing other speakers. Much of this work is done during early childhood; learning a language later in life usually involves more explicit instruction. Thus, grammar is the cognitive information underlying language use. The term \"grammar\" can also describe the rules which govern the linguistic behavior of a group of speakers. For example, the term \"English grammar\" may refer to the whole of English grammar; that is, to the grammars of all the speakers of the language, in which case the term encompasses a great deal of variation. Alternatively, it may refer only to what is common to the grammars of all or most English speakers (such as subject\u2013verb\u2013object word order in simple declarative sentences). It may also refer to the rules of one relatively well-defined form of English (such as standard English for a region). A description, study, or analysis of such rules may also be referred to as a grammar. A reference book describing the grammar of a language is called a \"reference grammar\" or simply \"a grammar\" (see History of English grammars). A fully explicit grammar which exhaustively describes the grammatical constructions of a particular speech variety is called a descriptive grammar. This kind of linguistic description contrasts with linguistic prescription, an attempt to actively discourage or suppress some grammatical constructions, while codifying and promoting others, either in an absolute sense or about a standard variety. For example, some prescriptivists maintain that sentences in English should not end with prepositions, a prohibition that has been traced to John Dryden (13 April 1668 \u2013 January 1688) whose unexplained objection to the practice perhaps led other English speakers to avoid the construction and discourage its use. Yet preposition stranding has a long history in Germanic languages like English, where it is so widespread as to be a standard usage. Outside linguistics, the term grammar is often used in a rather different sense. It may be used more broadly to include conventions of spelling and punctuation, which linguists would not typically consider as part of grammar but rather as part of orthography, the conventions used for writing a language. It may also be used more narrowly to refer to a set of prescriptive norms only, excluding those aspects of a language's grammar which are not subject to variation or debate on their normative acceptability. Jeremy Butterfield claimed that, for non-linguists, \"Grammar is often a generic way of referring to any aspect of English that people object to.\"",
    "German_language": "German (Deutsch, pronounced [d\u0254\u028ft\u0283] ()) is a West Germanic language that is mainly spoken in Central Europe. It is the most widely spoken and official or co-official language in Germany, Austria, Switzerland, South Tyrol in Italy, the German-speaking Community of Belgium, and Liechtenstein. It is one of the three official languages of Luxembourg and a co-official language in the Opole Voivodeship in Poland. The German language is most similar to other languages within the West Germanic language branch, including Afrikaans, Dutch, English, the Frisian languages, Low German/Low Saxon, Luxembourgish, and Yiddish. It also contains close similarities in vocabulary to Danish, Norwegian and Swedish, although they belong to the North Germanic group. German is the second most widely spoken Germanic language, after English. One of the major languages of the world, German is a native language to almost 100 million people worldwide and the most widely spoken native language in the European Union. German is the third most commonly spoken foreign language in the EU after English and French, making it the second biggest language in the EU in terms of overall speakers. German is also the second most widely taught foreign language in the EU after English at primary school level (but third after English and French at lower secondary level), the fourth most widely taught non-English language in the US (after Spanish, French and American Sign Language), the second most commonly used scientific language and the third most widely used language on websites after English and Russian. The German-speaking countries are ranked fifth in terms of annual publication of new books, with one tenth of all books (including e-books) in the world being published in German. In the United Kingdom, German and French are the most sought-after foreign languages for businesses (with 49% and 50% of businesses identifying these two languages as the most useful, respectively). German is an inflected language with four cases for nouns, pronouns and adjectives (nominative, accusative, genitive, dative), three genders (masculine, feminine, neuter), two numbers (singular, plural), and strong and weak verbs. It derives the majority of its vocabulary from the ancient Germanic branch of the Indo-European language family. Some of its vocabulary is derived from Latin and Greek, and fewer are borrowed from French and Modern English. German is a pluricentric language, with its standardized variants being (German, Austrian, and Swiss Standard German). It is also notable for its broad spectrum of dialects, with many unique varieties existing in Europe and other parts of the world. Italy recognizes all the German-speaking minorities in its territory as national historic minorities and protects the varieties of German spoken in several regions of Northern Italy besides South Tyrol. Due to the limited intelligibility between certain varieties and Standard German, as well as the lack of an undisputed, scientific difference between a \"dialect\" and a \"language\", some German varieties or dialect groups (e.g. Low German or Plautdietsch) can be described as either \"languages\" or \"dialects\".",
    "Supervised_learning": "Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). The parallel task in human and animal psychology is often referred to as concept learning.",
    "Tone_(linguistics)": "Tone is the use of pitch in language to distinguish lexical or grammatical meaning \u2013 that is, to distinguish or to inflect words. All verbal languages use pitch to express emotional and other paralinguistic information and to convey emphasis, contrast, and other such features in what is called intonation, but not all languages use tones to distinguish words or their inflections, analogously to consonants and vowels. Languages that do have this feature are called tonal languages; the distinctive tone patterns of such a language are sometimes called tonemes, by analogy with phoneme. Tonal languages are common in East and Southeast Asia, the Pacific, Africa, and the Americas; as many as seventy percent of world languages may be tonal.",
    "Sentence_boundary_disambiguation": "Sentence boundary disambiguation (SBD), also known as sentence breaking, sentence boundary detection, and sentence segmentation, is the problem in natural language processing of deciding where sentences begin and end. Natural language processing tools often require their input to be divided into sentences; however, sentence boundary identification can be challenging due to the potential ambiguity of punctuation marks. In written English, a period may indicate the end of a sentence, or may denote an abbreviation, a decimal point, an ellipsis, or an email address, among other possibilities. About 47% of the periods in the Wall Street Journal corpus denote abbreviations. Question marks and exclamation marks can be similarly ambiguous due to use in emoticons, computer code, and slang. Languages like Japanese and Chinese have unambiguous sentence-ending markers.",
    "Parse_tree": "A parse tree or parsing tree or derivation tree or concrete syntax tree is an ordered, rooted tree that represents the syntactic structure of a string according to some context-free grammar. The term parse tree itself is used primarily in computational linguistics; in theoretical syntax, the term syntax tree is more common. Parse trees concretely reflect the syntax of the input language, making them distinct from the abstract syntax trees used in computer programming. Unlike Reed-Kellogg sentence diagrams used for teaching grammar, parse trees do not use distinct symbol shapes for different types of constituents. Parse trees are usually constructed based on either the constituency relation of constituency grammars (phrase structure grammars) or the dependency relation of dependency grammars. Parse trees may be generated for sentences in natural languages (see natural language processing), as well as during processing of computer languages, such as programming languages. A related concept is that of phrase marker or P-marker, as used in transformational generative grammar. A phrase marker is a linguistic expression marked as to its phrase structure. This may be presented in the form of a tree, or as a bracketed expression. Phrase markers are generated by applying phrase structure rules, and themselves are subject to further transformational rules. A set of possible parse trees for a syntactically ambiguous sentence is called a \"parse forest.\"",
    "Part_of_speech": "In traditional grammar, a part of speech is a category of words (or, more generally, of lexical items) that have similar grammatical properties. Words that are assigned to the same part of speech generally display similar syntactic behavior\u2014they play similar roles within the grammatical structure of sentences\u2014and sometimes similar morphology in that they undergo inflection for similar properties. Commonly listed English parts of speech are noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, numeral, article, or determiner. Other Indo-European languages also have essentially all these word classes; one exception to this generalization is that most Slavic languages as well as Latin and Sanskrit do not have articles. Beyond the Indo-European family, such other European languages as Hungarian and Finnish, both of which belong to the Uralic family, completely lack prepositions or have only very few of them; rather, they have postpositions. Other terms than part of speech\u2014particularly in modern linguistic classifications, which often make more precise distinctions than the traditional scheme does\u2014include word class, lexical class, and lexical category. Some authors restrict the term lexical category to refer only to a particular type of syntactic category; for them the term excludes those parts of speech that are considered to be functional, such as pronouns. The term form class is also used, although this has various conflicting definitions. Word classes may be classified as : open classes (like nouns, verbs and adjectives) acquire new members constantly, while closed classes (such as pronouns and conjunctions) acquire new members infrequently, if at all. Almost all languages have the word classes noun and verb, but beyond these two there are significant variations among different languages. For example: Japanese has as many as three classes of adjectives, where English has one.Chinese, Korean, Japanese and Vietnamese have a class of nominal classifiers.Many languages do not distinguish between adjectives and adverbs, or between adjectives and verbs (see stative verb). Because of such variation in the number of categories and their identifying properties, analysis of parts of speech must be done for each individual language. Nevertheless, the labels for each category are assigned on the basis of universal criteria.",
    "Ontology_(information_science)": "In computer science and information science, an ontology encompasses a representation, formal naming and definition of the categories, properties and relations between the concepts, data and entities that substantiate one, many or all domains of discourse. More simply, an ontology is a way of showing the properties of a subject area and how they are related, by defining a set of concepts and categories that represent the subject. Every academic discipline or field creates ontologies to limit complexity and organize data into information and knowledge. New ontologies improve problem solving within that domain. Translating research papers within every field is a problem made easier when experts from different countries maintain a controlled vocabulary of jargon between each of their languages.",
    "Joseph_Weizenbaum": "Joseph Weizenbaum (8 January 1923 \u2013 5 March 2008) was a German American computer scientist and a professor at MIT. The Weizenbaum Award is named after him. He is considered one of the fathers of modern artificial intelligence.",
    "Artificial_intelligence": "In computer science, artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\". As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. A quip in Tesler's Theorem says \"AI is whatever hasn't been done yet.\" For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology. Modern machine capabilities generally classified as AI include successfully understanding human speech, competing at the highest level in strategic game systems (such as chess and Go), autonomously operating cars, intelligent routing in content delivery networks, and military simulations. Artificial intelligence was founded as an academic discipline in 1955, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an \"AI winter\"), followed by new approaches, success and renewed funding. For most of its history, AI research has been divided into sub-fields that often fail to communicate with each other. These sub-fields are based on technical considerations, such as particular goals (e.g. \"robotics\" or \"machine learning\"), the use of particular tools (\"logic\" or artificial neural networks), or deep philosophical differences. Sub-fields have also been based on social factors (particular institutions or the work of particular researchers). The traditional problems (or goals) of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects. General intelligence is among the field's long-term goals. Approaches include , , and . Many tools are used in AI, including versions of , artificial neural networks, and . The AI field draws upon computer science, information engineering, mathematics, psychology, linguistics, philosophy, and many other fields. The field was founded on the assumption that human intelligence \"can be so precisely described that a machine can be made to simulate it\". This raises philosophical arguments about the mind and the ethics of creating artificial beings endowed with human-like intelligence. These issues have been explored by myth, fiction and philosophy since antiquity. Some people also consider AI to be a danger to humanity if it progresses unabated. Others believe that AI, unlike previous technological revolutions, will create a risk of mass unemployment. In the twenty-first century, AI techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science, software engineering and operations research.",
    "Head-driven_phrase_structure_grammar": "Head-driven phrase structure grammar (HPSG) is a highly lexicalized, constraint-based grammar developed by Carl Pollard and Ivan Sag. It is a type of phrase structure grammar, as opposed to a dependency grammar, and it is the immediate successor to generalized phrase structure grammar. HPSG draws from other fields such as computer science (data type theory and knowledge representation) and uses Ferdinand de Saussure's notion of the sign. It uses a uniform formalism and is organized in a modular way which makes it attractive for natural language processing. An HPSG grammar includes principles and grammar rules and lexicon entries which are normally not considered to belong to a grammar. The formalism is based on lexicalism. This means that the lexicon is more than just a list of entries; it is in itself richly structured. Individual entries are marked with types. Types form a hierarchy. Early versions of the grammar were very lexicalized with few grammatical rules (schema). More recent research has tended to add more and richer rules, becoming more like construction grammar. The basic type HPSG deals with is the sign. Words and phrases are two different subtypes of sign. A word has two features: [PHON] (the sound, the phonetic form) and [SYNSEM] (the syntactic and semantic information), both of which are split into subfeatures. Signs and rules are formalized as typed feature structures.",
    "Parsing": "Parsing, syntax analysis, or syntactic analysis is the process of analyzing a string of symbols, either in natural language, computer languages or data structures, conforming to the rules of a formal grammar. The term parsing comes from Latin pars (orationis), meaning part (of speech). The term has slightly different meanings in different branches of linguistics and computer science. Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams. It usually emphasizes the importance of grammatical divisions such as subject and predicate. Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information. Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input. The term is also used in psycholinguistics when describing language comprehension. In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) \"in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc.\" This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences. Within computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters. The term may also be used to describe a split or separation.",
    "Linguistics": "Linguistics is the scientific study of language. It involves the analysis of language form, language meaning, and language in context. Linguists traditionally analyse human language by observing an interplay between sound and meaning. Linguistics also deals with the social, cultural, historical, and political factors that influence language, through which linguistic and language-based context is often determined. Research on language through the sub-branches of historical and evolutionary linguistics also focuses on how languages change and grow, particularly over an extended period of time. The earliest activities in the documentation and description of language have been attributed to the 6th-century-BC Indian grammarian P\u0101\u1e47ini who wrote a formal description of the Sanskrit language in his A\u1e63\u1e6d\u0101dhy\u0101y\u012b. Related areas of study include the disciplines of semiotics (the study of direct and indirect language through signs and symbols), literary criticism (the historical and ideological analysis of literature, cinema, art, or published material), translation (the conversion and documentation of meaning in written/spoken text from one language or dialect onto another), and speech-language pathology (a corrective method to cure phonetic disabilities and dis-functions at the cognitive level).",
    "Machine_translation": "Machine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation or interactive translation), is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another. On a basic level, MT performs mechanical substitution of words in one language for words in another, but that alone rarely produces a good translation because recognition of whole phrases and their closest counterparts in the target language is needed. Not all words in one language have equivalent words in another language, and many words have more than one meaning. Solving this problem with corpus statistical and neural techniques is a rapidly-growing field that is leading to better translations, handling differences in linguistic typology, translation of idioms, and the isolation of anomalies. Current machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text. Improved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports). The progress and potential of machine translation have been much debated through its history. Since the 1950s, a number of scholars, first and most notably Yehoshua Bar-Hillel, have questioned the possibility of achieving fully automatic machine translation of high quality. Some critics claim that there are in-principle obstacles to automating the translation process.",
    "Analog_signal": "An analog signal is any continuous signal for which the time-varying feature (variable) of the signal is a representation of some other time-varying quantity, i.e., analogous to another time-varying signal. For example, in an analog audio signal, the instantaneous voltage of the signal varies continuously with the pressure of the sound waves. It differs from a digital signal, in which the continuous quantity is a representation of a sequence of discrete values which can only take on one of a finite number of values. The term analog signal usually refers to electrical signals; however, mechanical, pneumatic, hydraulic, human speech, and other systems may also convey or be considered analog signals.",
    "Japanese_language": "Japanese (\u65e5\u672c\u8a9e, Nihongo [\u0272iho\u014b\u0261o] ()) is an East Asian language spoken by about 128 million people, primarily in Japan, where it is the national language. It is a member of the Japonic (or Japanese-Ryukyuan) language family, and its relation to other languages, such as Korean, is debated. Japonic languages have been grouped with other language families such as Ainu, Austroasiatic, and the now-discredited Altaic, but none of these proposals has gained widespread acceptance. Little is known of the language's prehistory, or when it first appeared in Japan. Chinese documents from the 3rd century recorded a few Japanese words, but substantial texts did not appear until the 8th century. During the Heian period (794\u20131185), Chinese had considerable influence on the vocabulary and phonology of Old Japanese. Late Middle Japanese (1185\u20131600) included changes in features that brought it closer to the modern language, and the first appearance of European loanwords. The standard dialect moved from the Kansai region to the Edo (modern Tokyo) region in the Early Modern Japanese period (early 17th century\u2013mid-19th century). Following the end of Japan's self-imposed isolation in 1853, the flow of loanwords from European languages increased significantly. English loanwords, in particular, have become frequent, and Japanese words from English roots have proliferated. Japanese is an agglutinative, mora-timed language with simple phonotactics, a pure vowel system, phonemic vowel and consonant length, and a lexically significant pitch-accent. Word order is normally subject\u2013object\u2013verb with particles marking the grammatical function of words, and sentence structure is topic\u2013comment. Sentence-final particles are used to add emotional or emphatic impact, or make questions. Nouns have no grammatical number or gender, and there are no articles. Verbs are conjugated, primarily for tense and voice, but not person. Japanese equivalents of adjectives are also conjugated. Japanese has a complex system of honorifics with verb forms and vocabulary to indicate the relative status of the speaker, the listener, and persons mentioned. Japanese has no genetic relationship with Chinese, but it makes extensive use of Chinese characters, or kanji (\u6f22\u5b57), in its writing system, and a large portion of its vocabulary is borrowed from Chinese. Along with kanji, the Japanese writing system primarily uses two syllabic (or moraic) scripts, hiragana (\u3072\u3089\u304c\u306a or \u5e73\u4eee\u540d) and katakana (\u30ab\u30bf\u30ab\u30ca or \u7247\u4eee\u540d). Latin script is used in a limited fashion, such as for imported acronyms, and the numeral system uses mostly Arabic numerals alongside traditional Chinese numerals.",
    "Information_extraction": "Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video/documents could be seen as information extraction Due to the difficulty of the problem, current approaches to IE focus on narrowly restricted domains. An example is the extraction from newswire reports of corporate mergers, such as denoted by the formal relation: , from an online news sentence such as: \"Yesterday, New York based Foo Inc. announced their acquisition of Bar Corp.\" A broad goal of IE is to allow computation to be done on the previously unstructured data. A more specific goal is to allow logical reasoning to draw inferences based on the logical content of the input data. Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and context. Information Extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis, IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events in a manner that is similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. For the terrorism example, a template would have slots corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened. An IE system for this problem is required to \u201cunderstand\u201d an attack article only enough to find data corresponding to the slots in this template.",
    "Automated_essay_scoring": "Automated essay scoring (AES) is the use of specialized computer programs to assign grades to essays written in an educational setting. It is a form of educational assessment and an application of natural language processing. Its objective is to classify a large set of textual entities into a small number of discrete categories, corresponding to the possible grades, for example, the numbers 1 to 6. Therefore, it can be considered a problem of statistical classification. Several factors have contributed to a growing interest in AES. Among them are cost, accountability, standards, and technology. Rising education costs have led to pressure to hold the educational system accountable for results by imposing standards. The advance of information technology promises to measure educational achievement at reduced cost. The use of AES for high-stakes testing in education has generated significant backlash, with opponents pointing to research that computers cannot yet grade writing accurately and arguing that their use for such purposes promotes teaching writing in reductive ways (i.e. teaching to the test).",
    "Category:Computational_fields_of_study": "",
    "Question_answering": "Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.",
    "Rhetorical_structure_theory": "Rhetorical structure theory (RST) was originally developed by William Mann and Sandra Thompson of the University of Southern California's Information Sciences Institute (ISI) and defined in a seminal paper in 1988. This theory was developed as part of studies of computer based text generation. Natural language researchers later began using RST in text summarization and other applications. RST addresses text organization by means of relations that hold between parts of text. It explains coherence by postulating a hierarchical, connected structure of texts. In 2000, Daniel Marcu, also of ISI, demonstrated that practical discourse parsing and text summarization also could be achieved using RST.",
    "English_language": "English is a West Germanic language that was first spoken in early medieval England and eventually became a global lingua franca. It is named after the Angles, one of the ancient Germanic peoples that migrated to the area of Great Britain that later took their name, England. Both names derive from Anglia, a peninsula on the Baltic Sea. English is most closely related to Frisian and Low Saxon, while its vocabulary has been significantly influenced by other Germanic languages, particularly Old Norse (a North Germanic language), as well as Latin and French. English has developed over the course of more than 1,400 years. The earliest forms of English, a group of West Germanic (Ingvaeonic) dialects brought to Great Britain by Anglo-Saxon settlers in the 5th century, are collectively called Old English. Middle English began in the late 11th century with the Norman conquest of England; this was a period in which English was influenced by Old French, in particular through its Old Norman dialect. Early Modern English began in the late 15th century with the introduction of the printing press to London, the printing of the King James Bible and the start of the Great Vowel Shift. Modern English has been spreading around the world since the 17th century by the worldwide influence of the British Empire and the United States. Through all types of printed and electronic media of these countries, English has become the leading language of international discourse and the lingua franca in many regions and professional contexts such as science, navigation and law. Modern English grammar is the result of a gradual change from a typical Indo-European dependent marking pattern, with a rich inflectional morphology and relatively free word order, to a mostly analytic pattern with little inflection, a fairly fixed subject\u2013verb\u2013object word order and a complex syntax. Modern English relies more on auxiliary verbs and word order for the expression of complex tenses, aspect and mood, as well as passive constructions, interrogatives and some negation. English is the largest language by number of speakers, and the third most-spoken native language in the world, after Standard Chinese and Spanish. It is the most widely learned second language and is either the official language or one of the official languages in almost 60 sovereign states. There are more people who have learned it as a second language than there are native speakers. It is estimated that there are over 2 billion speakers of English. English is the majority native language in the United States, the United Kingdom, Canada, Australia, New Zealand and Ireland, and it is widely spoken in some areas of the Caribbean, Africa and South Asia. It is a co-official language of the United Nations, the European Union and many other world and regional international organisations. It is the most widely spoken Germanic language, accounting for at least 70% of speakers of this Indo-European branch. English speakers are called \"Anglophones\". Variability among the accents and dialects of English used in different countries and regions\u2014in terms of phonetics and phonology, and sometimes also vocabulary, idioms, grammar, and spelling\u2014does not typically prevent understanding by speakers of other dialects, although mutual unintelligibility can occur at extreme ends of the dialect continuum.",
    "Query_expansion": "Query expansion (QE) is the process of reformulating a given query to improve retrieval performance in information retrieval operations, particularly in the context of query understanding.In the context of search engines, query expansion involves evaluating a user's input (what words were typed into the search query area, and sometimes other types of data) and expanding the search query to match additional documents. Query expansion involves techniques such as: \n* Finding synonyms of words, and searching for the synonyms as well \n* Finding semantically related words (e.g. antonyms, meronyms, hyponyms, hypernyms) \n* Finding all the various morphological forms of words by stemming each word in the search query \n* Fixing spelling errors and automatically searching for the corrected form or suggesting it in the results \n* Re-weighting the terms in the original query Query expansion is a methodology studied in the field of computer science, particularly within the realm of natural language processing and information retrieval.",
    "Feature_learning": "In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. Feature learning can be either supervised or unsupervised. \n* In supervised feature learning, features are learned using labeled input data. Examples include supervised neural networks, multilayer perceptron and (supervised) dictionary learning. \n* In unsupervised feature learning, features are learned with unlabeled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.",
    "Natural-language_generation": "Natural-language generation (NLG) is a software process that transforms structured data into natural language. It can be used to produce long form content for organizations to automate custom reports, as well as produce custom content for a web or mobile application. It can also be used to generate short blurbs of text in interactive conversations (a chatbot) which might even be read out by a text-to-speech system. Automated NLG can be compared to the process humans use when they turn ideas into writing or speech. Psycholinguists prefer the term language production for this process, which can also be described in mathematical terms, or modeled in a computer for psychological research. NLG systems can also be compared to translators of artificial computer languages, such as decompilers or transpilers, which also produce human-readable code generated from an intermediate representation. Human languages tend to be considerably more complex and allow for much more ambiguity and variety of expression than programming languages, which makes NLG more challenging. NLG may be viewed as the opposite of natural-language understanding: whereas in natural-language understanding, the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into words. The practical considerations in building NLU vs. NLG systems are not symmetrical. NLU needs to deal with ambiguous or erroneous user input, whereas the ideas the system wants to express through NLG are generally known precisely. NLG needs to choose a specific, self-consistent textual representation from many potential representations, whereas NLU generally tries to produce a single, normalized representation of the idea expressed. NLG has existed for a long time but commercial NLG technology has only recently become widely available. NLG techniques range from simple template-based systems like a mail merge that generates form letters, to systems that have a complex understanding of human grammar. NLG can also be accomplished by training a statistical model using machine learning, typically on a large corpus of human-written texts.",
    "Foreign_language_writing_aid": "A foreign language writing aid is a computer program or any other instrument that assists a non-native language user (also referred to as a foreign language learner) in writing decently in their target language. Assistive operations can be classified into two categories: on-the-fly prompts and post-writing checks. Assisted aspects of writing include: lexical, syntactic (syntactic and semantic roles of a word's frame),  (context/collocation-influenced word choice and user-intention-driven synonym choice) and idiomatic expression transfer, etc. Different types of foreign language writing aids include automated proofreading applications, text corpora, dictionaries, translation aids and orthography aids.",
    "Dialogue_system": "A dialogue system, or conversational agent (CA), is a computer system intended to converse with a human. Dialogue systems employed one or more of text, speech, graphics, haptics, gestures, and other modes for communication on both the input and output channel. The elements of a dialogue system are not defined, however they are different from chatbot.. The typical GUI wizard engages in a sort of dialog, but it includes very few of the common dialogue system components, and dialog state is trivial.",
    "Cognitive_linguistics": "Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from cognitive psychology, neuropsychology and linguistics. Models and theoretical accounts of cognitive linguistics are considered as psychologically real, and research in cognitive linguistics aims to help understand cognition in general and is seen as a road into the human mind. There has been scientific and terminological controversy around the label 'cognitive linguistics'; there is no consensus on what specifically is meant with the term.",
    "Open-world_assumption": "In a formal system of logic used for knowledge representation, the open-world assumption is the assumption that the truth value of a statement may be true irrespective of whether or not it is known to be true. It is the opposite of the closed-world assumption, which holds that any statement that is true is also known to be true. The open-world assumption (OWA) codifies the informal notion that in general no single agent or observer has complete knowledge, and therefore cannot make the closed-world assumption. The OWA limits the kinds of inference and deductions an agent can make to those that follow from statements that are known to the agent to be true. In contrast, the closed world assumption allows an agent to infer, from its lack of knowledge of a statement being true, anything that follows from that statement being false. Heuristically, the open-world assumption applies when we represent knowledge within a system as we discover it, and where we cannot guarantee that we have discovered or will discover complete information. In the OWA, statements about knowledge that are not included in or inferred from the knowledge explicitly recorded in the system may be considered unknown, rather than wrong or false. Semantic Web languages such as OWL make the open-world assumption. The absence of a particular statement within the web means, in principle, that the statement has not been made explicitly yet, irrespective of whether it would be true or not, and irrespective of whether we believe that it would be true or not. In essence, from the absence of a statement alone, a deductive reasoner cannot (and must not) infer that the statement is false. Many procedural programming languages and databases make the closed-world assumption. For example, if a typical airline database does not contain a seat assignment for a traveler, it is assumed that the traveler has not checked in. The closed-world assumption typically applies when a system has complete control over information; this is the case with many database applications where the database transaction system acts as a central broker and arbiter of concurrent requests by multiple independent clients (e.g., airline booking agents). There are, however, many databases with incomplete information: for example, one cannot assume that because there is no mention on a patient's history of a particular allergy, that the patient does not suffer from that allergy. Example Statement: \"Mary\" \"is a citizen of\" \"France\" Question: Is Paul a citizen of France? \"Closed world\" (for example SQL) answer: No. \"Open world\" answer: Unknown. Under OWA, failure to derive a fact does not imply the opposite. For example, assume we only know that Mary is a citizen of France. From this information we can neither conclude that Paul is not a citizen of France, nor that he is. Therefore, we admit the fact that our knowledge of the world is incomplete. The open-world assumption is closely related to the monotonic nature of first-order logic: adding new information never falsifies a previous conclusion. Namely, if we subsequently learn that Paul is also a citizen of France, this does not change any earlier positive or negative conclusions. The language of logic programs with strong negation allows us to postulate the closed-world assumption for some statements and leave the other statements in the realm of the open-world assumption. An intermediate ground between OWA and CWA is provided by the partial-closed world assumption (PCWA). Under the PCWA, the knowledge base is generally treated under open-world semantics, yet it is possible to assert parts that should be treated under closed-world semantics, via completeness assertions. The PCWA is especially needed for situations where the CWA is not applicable due to an open domain, yet the OWA is too credulous in allowing anything to be possibly true.",
    "ALPAC": "ALPAC (Automatic Language Processing Advisory Committee) was a committee of seven scientists led by John R. Pierce, established in 1964 by the United States government in order to evaluate the progress in computational linguistics in general and machine translation in particular. Its report, issued in 1966, gained notoriety for being very skeptical of research done in machine translation so far, and emphasizing the need for basic research in computational linguistics; this eventually caused the U.S. government to reduce its funding of the topic dramatically. The ALPAC was set up in April 1964 with John R. Pierce as the chairman. The committee consisted of: 1.  \n* John R. Pierce, who at the time worked for Bell Telephone Laboratories 2.  \n* John B. Carroll, a psychologist from Harvard University 3.  \n* Eric P. Hamp, a linguist from the University of Chicago 4.  \n* David G. Hays, a Machine Translation researcher from RAND Corporation 5.  \n* Charles F. Hockett, a linguist from Cornell University 6.  \n* Anthony G. Oettinger, a Machine Translation researcher from Harvard University 7.  \n* Alan Perlis, an Artificial Intelligence researcher from Carnegie Institute of Technology Testimony was heard from: \n* Paul Garvin of  \n* Gilbert King of Itek Corporation and previously from IBM \n* Winfred P. Lehmann from University of Texas \n* Jules Mersel of Bunker-Ramo Corporation ALPAC's final recommendations (p. 34) were, therefore, that research should be supported on: 1.  \n* practical methods for evaluation of translations; 2.  \n* means for speeding up the human translation process; 3.  \n* evaluation of quality and cost of various sources of translations; 4.  \n* investigation of the utilization of translations, to guard against production of translations that are never read; 5.  \n* study of delays in the over-all translation process, and means for eliminating them, both in journals and in individual items; 6.  \n* evaluation of the relative speed and cost of various sorts of machine-aided translation; 7.  \n* adaptation of existing mechanized editing and production processes in translation; 8.  \n* the over-all translation process; and 9.  \n* production of adequate reference works for the translator, including the adaptation of glossaries that now exist primarily for automatic dictionary look-up in machine translation",
    "Word2vec": "Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space. Word2vec was created and published in 2013 by a team of researchers led by Tomas Mikolov at Google and patented. The algorithm has been subsequently analysed and explained by other researchers. Embedding vectors created using the Word2vec algorithm have some advantages compared to earlier algorithms such as latent semantic analysis.",
    "Category:Artificial_intelligence": "",
    "Computational_linguistics": "Computational linguistics is an interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective, as well as the study of appropriate computational approaches to linguistic questions. Traditionally, computational linguistics was performed by computer scientists who had specialized in the application of computers to the processing of a natural language. Today, computational linguists often work as members of interdisciplinary teams, which can include regular linguists, experts in the target language, and computer scientists. In general, computational linguistics draws upon the involvement of linguists, computer scientists, experts in artificial intelligence, mathematicians, logicians, philosophers, cognitive scientists, cognitive psychologists, psycholinguists, anthropologists and neuroscientists, among others. Computational linguistics has both theoretical and applied components. Theoretical computational linguistics focuses on issues in theoretical linguistics and cognitive science and applied computational linguistics focuses on the practical outcome of modeling human language use. The Association for Computational Linguistics defines computational linguistics as: ...the scientific study of language from a computational perspective. Computational linguists are interested in providing computational models of various kinds of linguistic phenomena.",
    "Language_technology": "Language technology, often called human language technology (HLT), studies methods of how computer programs or electronic devices can analyze, produce, modify or respond to human texts and speech. It consists of natural language processing (NLP) and computational linguistics (CL) on the one hand, and speech technology on the other. It also includes many application oriented aspects of these. Working with language technology often requires broad knowledge not only about linguistics but also about computer science. The Globalization and Localization Association (GALA), maintains a directory of language technology and software for translators and localizers. For many of the world's lesser known languages, the foundation of language technology is providing communities with fonts and keyboard setups so their languages can be written on computers or mobile devices.",
    "Lexical_semantics": "Lexical semantics (also known as lexicosemantics), is a subfield of linguistic semantics. The units of analysis in lexical semantics are lexical units which include not only words but also sub-words or sub-units such as affixes and even compound words and phrases. Lexical units include the catalogue of words in a language, the lexicon. Lexical semantics looks at how the meaning of the lexical units correlates with the structure of the language or syntax. This is referred to as syntax-semantic interface. The study of lexical semantics looks at: \n* the classification and decomposition of lexical items \n* the differences and similarities in lexical semantic structure cross-linguistically \n* the relationship of lexical meaning to sentence meaning and syntax. Lexical units, also referred to as syntactic atoms, can stand alone such as in the case of root words or parts of compound words or they necessarily attach to other units such as prefixes and suffixes do. The former are called free morphemes and the latter bound morphemes. They fall into a narrow range of meanings (semantic fields) and can combine with each other to generate new denotations.",
    "Query_understanding": "Query understanding is the process of inferring the intent of a search engine user by extracting semantic meaning from the searcher\u2019s keywords. Query understanding methods generally take place before the search engine retrieves and ranks results. It is related to natural language processing but specifically focused on the understanding of search queries. Query understanding is at the heart of technologies like Amazon Alexa, Apple's Siri. Google Assistant, IBM's Watson, and Microsoft's Cortana.",
    "Generative_grammar": "Generative grammar is a linguistic theory that regards linguistics as the study of a hypothesised innate grammatical structure. A sociobiological modification of structuralist theories, especially glossematics, generative grammar considers grammar as a system of rules that generates exactly those combinations of words that form grammatical sentences in a given language. The difference from structural and functional models is that the object is placed into the verb phrase in generative grammar. This purportedly cognitive structure is thought of being a part of a universal grammar, a syntactic structure which is caused by a genetic mutation in humans. Generativists have created numerous theories to make the NP VP (NP) analysis work in natural language description. That is, the subject and the verb phrase appearing as independent constituents, and the object placed within the verb phrase. A main point of interest remains in how to appropriately analyse Wh-movement and other cases where the subject appears to separate the verb from the object. Although claimed by generativists as a cognitively real structure, neuroscience has found no evidence for it. Due to its nonstandard view of the brain, some researchers have called the scientific premises of generative grammar into question.",
    "Statistical_inference": "Statistical inference is the process of using data analysis to deduce properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population. In machine learning, the term inference is sometimes used instead to mean \"make a prediction, by evaluating an already trained model\"; in this context deducing properties of the model is referred to as training or learning (rather than inference), and using a model for prediction is referred to as inference (instead of prediction); see also predictive inference.",
    "Neural_network": "A neural network is a network or circuit of neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus a neural network is either a biological neural network, made up of real biological neurons, or an artificial neural network, for solving artificial intelligence (AI) problems. The connections of the biological neuron are modeled as weights. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be \u22121 and 1. These artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information.",
    "Chinese_language": "Chinese (simplified Chinese: \u6c49\u8bed; traditional Chinese: \u6f22\u8a9e; pinyin: H\u00e0ny\u01d4; lit.: 'Han language' or especially though not exclusively for written Chinese: \u4e2d\u6587; Zh\u014dngw\u00e9n; 'Chinese writing') is a family of East Asian analytic languages that form the Sinitic branch of the Sino-Tibetan languages. Chinese languages are spoken by the ethnic Han Chinese majority and many minority ethnic groups in China. About 1.2 billion people (around 16% of the world's population) speak some form of Chinese as their first language. The varieties of Chinese are usually considered by native speakers to be regional variants of ethnic Chinese speech, without consideration of whether they are mutually intelligible. Due to their lack of mutual intelligibility, they are generally described as distinct languages (perhaps hundreds) by linguists who sometimes note that they are more varied than the Romance languages. Investigation of the historical relationships among the Sinitic languages is just getting started. Currently, most classifications posit 7 to 13 main regional groups, based on often superficial phonetic developments, of which the most populous by far is Mandarin (about 800 million speakers, e.g. Southwestern Mandarin), followed by Min (75 million, e.g. Southern Min), Wu (74 million, e.g. Shanghainese) and Yue (68 million, e.g. Cantonese). These groups are unintelligible to each other and generally many of their subgroups are mutually unintelligible as well (e.g., not only is Min Chinese a family of mutually unintelligible languages, but Southern Min itself is not a single language). There are, however, several transitional areas, where languages and dialects from different branches share enough features for some limited intelligibility between neighboring areas. Examples are New Xiang and Southwest Mandarin, Xuanzhou Wu and Lower Yangtze Mandarin, Jin and Central Plains Mandarin and certain divergent dialects of Hakka with Gan (though these are unintelligible with mainstream Hakka). All varieties of Chinese are tonal to at least some degree and largely analytic. Standard Chinese (P\u01d4t\u014dnghu\u00e0/Gu\u00f3y\u01d4/Hu\u00e1y\u01d4) is a standardized form of spoken Chinese based on the Beijing dialect of Mandarin. It is an official language of China, similar to one of the national languages of Taiwan (Taiwanese Mandarin) and one of the four official languages of Singapore. It is one of the six official languages of the United Nations. The written form of the standard language (\u4e2d\u6587, Zh\u014dngw\u00e9n), based on the logograms known as Chinese characters (\u6c49\u5b57/\u6f22\u5b57, H\u00e0nz\u00ec), is shared by literate speakers of otherwise unintelligible dialects. The earliest Chinese written records are Shang dynasty-era oracle inscriptions, which can be traced back to 1250 BCE. The phonetic categories of Archaic Chinese can be reconstructed from the rhymes of ancient poetry. During the Northern and Southern dynasties period, Middle Chinese went through several sound changes and split into several varieties following prolonged geographic and political separation. Qieyun, a rime dictionary, recorded a compromise between the pronunciations of different regions. The royal courts of the Ming and early Qing dynasties operated using a koin\u00e9 language (Guanhua) based on Nanjing dialect of Lower Yangtze Mandarin. Standard Chinese was adopted in the 1930s and is now an official language of both the People's Republic of China and the Republic of China on Taiwan.",
    "Transformational_grammar": "In linguistics, transformational grammar (TG) or transformational-generative grammar (TGG) is part of the theory of generative grammar, especially of natural languages. It considers grammar to be a system of rules that generate exactly those combinations of words that form grammatical sentences in a given language and involves the use of defined operations (called transformations) to produce new sentences from existing ones. This mechanism was first introduced to general linguistics by the structural linguist Louis Hjelmslev, son of the mathematician Johannes Hjelmslev who invented the Hjelmslev transformation. A modification which separated discourse and semantics from the syntactic model was subsequently made by Zellig Harris, giving rise to what became known as transformational generative grammar. The full Hjelmslevian conception, in contrast, is incorporated into functional grammar.",
    "Textual_entailment": "Textual entailment (TE) in natural language processing is a directional relation between text fragments. The relation holds whenever the truth of one text fragment follows from another text. In the TE framework, the entailing and entailed texts are termed text (t) and hypothesis (h), respectively. Textual entailment is not the same as pure logical entailment \u2013 it has a more relaxed definition: \"t entails h\" (t \u21d2 h) if, typically, a human reading t would infer that h is most likely true. (Alternatively: t \u21d2 h if and only if, typically, a human reading t would be justified in inferring the proposition expressed by h from the proposition expressed by t.) The relation is directional because even if \"t entails h\", the reverse \"h entails t\" is much less certain. Determining whether this relationship holds is an informal task, one which sometimes overlaps with the formal tasks of formal semantics (satisfying a strict condition will usually imply satisfaction of a less strict conditioned); additionally, textual entailment partially subsumes word entailment.",
    "Abstract_Meaning_Representation": "Abstract Meaning Representation (AMR) is a semantic representation language. AMR graphs are rooted, labeled, directed, acyclic graphs (DAGs), comprising whole sentences. They are intended to abstract away from syntactic representations, in the sense that sentences which are similar in meaning should be assigned the same AMR, even if they are not identically worded. By nature, the AMR language is biased towards English \u2013 it is not meant to function as an international auxiliary language.",
    "Stochastic_grammar": "A stochastic grammar (statistical grammar) is a grammar framework with a probabilistic notion of grammaticality: \n* Stochastic context-free grammar \n* Statistical parsing \n* Data-oriented parsing \n* Hidden Markov model \n* Estimation theory Statistical natural language processing uses stochastic, probabilistic and statistical methods, especially to resolve difficulties that arise because longer sentences are highly ambiguous when processed with realistic grammars, yielding thousands or millions of possible analyses. Methods for disambiguation often involve the use of corpora and Markov models. \"A probabilistic model consists of a non-probabilistic model plus some numerical quantities; it is not true that probabilistic models are inherently simpler or less structural than non-probabilistic models.\"",
    "Truecasing": "Truecasing is the problem in natural language processing (NLP) of determining the proper capitalization of words where such information is unavailable. This commonly comes up due to the standard practice (in English and many other languages) of automatically capitalizing the first word of a sentence. It can also arise in badly cased or noncased text (for example, all-lowercase or all-uppercase text messages). Truecasing is unnecessary in languages whose scripts do not have a distinction between uppercase and lowercase letters. This includes all languages not written in the Latin, Greek, Cyrillic or Armenian alphabets, such as Japanese, Chinese, Thai, Hebrew, Arabic, Hindi, and Georgian.",
    "Spanish_language": "Spanish () or Castilian ( (), ) is a Romance language that originated in the Iberian Peninsula of Europe and today is a global language with more than 483 million native speakers, mainly in Spain and the Americas. It is the world's second-most spoken native language, after Mandarin Chinese, and the world's fourth-most spoken language, after English, Mandarin Chinese and Hindi. Spanish is a part of the Ibero-Romance group of languages, which evolved from several dialects of Vulgar Latin in Iberia after the collapse of the Western Roman Empire in the 5th century. The oldest Latin texts with traces of Spanish come from mid-northern Iberia in the 9th century, and the first systematic written use of the language happened in Toledo, a prominent city of the Kingdom of Castile, in the 13th century. Beginning in 1492, the Spanish language was taken to the viceroyalties of the Spanish Empire, most notably to the Americas, as well as territories in Africa, Oceania and the Philippines. Analyzing the degree of difference from a language's parent (Latin, in the case of Romance languages) by comparing phonology, inflection, syntax, vocabulary, and intonation, Spanish is one of the closest Romance languages to Latin (20% distance), only behind Sardinian (8% distance) and Italian (12% distance). Around 75% of modern Spanish vocabulary is derived from Latin, including Latin borrowings from Ancient Greek.Spanish vocabulary has been in contact with Arabic during the Al-Andalus era and around 8% of its vocabulary has an Arabic lexical root. It has also had influences from Basque, Iberian, Celtiberian, Visigothic, and other neighboring Ibero-Romance languages. Additionally, it has absorbed vocabulary from other languages, particularly other Romance languages\u2014French, Italian, Andalusi Romance, Portuguese, Galician, Catalan, Occitan, and Sardinian\u2014as well as from Quechua, Nahuatl, and other indigenous languages of the Americas. The Spanish language does not feature prominently in scientific writing, though it is better represented in the humanities. 75% of scientific production in Spanish is divided into three thematic areas: social sciences, medical sciences and arts/humanities. Spanish is the third most used language on the internet after English and Chinese. Spanish is one of the six official languages of the United Nations. It is also used as an official language by the European Union, the Organization of American States, the Union of South American Nations, the Community of Latin American and Caribbean States, the African Union and many other international organizations.",
    "Outline_of_natural_language_processing": "The following outline is provided as an overview of and topical guide to natural language processing: Natural language processing \u2013 computer activity in which computers are entailed to analyze, understand, alter, or generate natural language. This includes the automation of any or all linguistic forms, activities, or methods of communication, such as conversation, correspondence, reading, written composition, dictation, publishing, translation, lip reading, and so on. Natural language processing is also the name of the branch of computer science, artificial intelligence, and linguistics concerned with enabling computers to engage in communication using natural language(s) in all forms, including but not limited to speech, print, writing, and signing.",
    "Cognition": "Cognition ( ()) refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses\". It encompasses many aspects of intellectual functions and processes such as attention, the formation of knowledge, memory and working memory, judgment and evaluation, reasoning and \"computation\", problem solving and decision making, comprehension and production of language. Cognitive processes use existing knowledge and generate new knowledge. Cognitive processes are analyzed from different perspectives within different contexts, notably in the fields of linguistics, anesthesia, neuroscience, psychiatry, psychology, education, philosophy, anthropology, biology, systemics, logic, and computer science. These and other different approaches to the analysis of cognition are synthesised in the developing field of cognitive science, a progressively autonomous academic discipline.",
    "Decision_tree": "A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements. Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.",
    "Capitalization": "Capitalization (North American English) or capitalisation (British English) is writing a word with its first letter as a capital letter (uppercase letter) and the remaining letters in lower case, in writing systems with a case distinction. The term also may refer to the choice of the casing applied to text. Conventional writing systems (orthographies) for different languages have different conventions for capitalization, for example the capitalization of titles. Conventions also vary, to a lesser extent, between different style guides. The full rules of capitalization for English are complicated. The rules have also changed over time, generally to capitalize fewer words. The conventions used in an 18th-century document will be unfamiliar to a modern reader; for instance, many common nouns are capitalized. The systematic use of capitalized and uncapitalized words in running text is called \"mixed case\".",
    "Speech_recognition": "Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. Some speech recognition systems require \"training\" (also called \"enrollment\") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called \"speaker independent\" systems. Systems that use training are called \"speaker dependent\". Speech recognition applications include voice user interfaces such as voice dialing (e.g. \"call home\"), call routing (e.g. \"I would like to make a collect call\"), domotic appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics, speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input). The term voice recognition or speaker identification  refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process. From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems.",
    "Text_segmentation": "Text segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics. The term applies both to mental processes used by humans when reading text, and to artificial processes implemented in computers, which are the subject of natural language processing. The problem is non-trivial, because while some written languages have explicit word boundary markers, such as the word spaces of written English and the distinctive initial, medial and final letter shapes of Arabic, such signals are sometimes ambiguous and not present in all written languages. Compare speech segmentation, the process of dividing speech into linguistically meaningful portions.",
    "Biomedical_text_mining": "Biomedical text mining (including biomedical natural language processing or BioNLP) refers to the methods and study of how text mining may be applied to texts and literature of the biomedical and molecular biology domains. As a field of research, biomedical text mining incorporates ideas from natural language processing, bioinformatics, medical informatics and computational linguistics. The strategies developed through studies in this field are frequently applied to the biomedical and molecular biology literature available through services such as PubMed.",
    "Punctuation": "Punctuation (or sometimes interpunction) is the use of spacing, conventional signs, and certain typographical devices as aids to the understanding and correct reading of written text, whether read silently or aloud. Another description is, \"It is the practice action or system of inserting points or other small marks into texts in order to aid interpretation; division of text into sentences, clauses, etc., by means of such marks.\" In written English, punctuation is vital to disambiguate the meaning of sentences. For example: \"woman, without her man, is nothing\" (emphasizing the importance of men to women), and \"woman: without her, man is nothing\" (emphasizing the importance of women to men) have very different meanings; as do \"eats shoots and leaves\" (which means the subject consumes plant growths) and \"eats, shoots, and leaves\" (which means the subject eats first, then fires a weapon, and then leaves the scene). The sharp differences in meaning are produced by the simple differences in punctuation within the example pairs, especially the latter. The rules of punctuation vary with language, location, register, and time and are constantly evolving. Certain aspects of punctuation are stylistic and are thus the author's (or editor's) choice, or tachygraphic (shorthand) language forms, such as those used in online chat and text messages.",
    "First-order_logic": "First-order logic\u2014also known as predicate logic, quantificational logic, and first-order predicate calculus\u2014is a collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects and allows the use of sentences that contain variables, so that rather than propositions such as Socrates is a man one can have expressions in the form \"there exists x such that x is Socrates and x is a man\" and there exists is a quantifier while x is a variable. This distinguishes it from propositional logic, which does not use quantifiers or relations; in this sense, propositional logic is the foundation of first-order logic. A theory about a topic is usually a first-order logic together with a specified domain of discourse over which the quantified variables range, finitely many functions from that domain to itself, finitely many predicates defined on that domain, and a set of axioms believed to hold for those things. Sometimes \"theory\" is understood in a more formal sense, which is just a set of sentences in first-order logic. The adjective \"first-order\" distinguishes first-order logic from higher-order logic in which there are predicates having predicates or functions as arguments, or in which one or both of predicate quantifiers or function quantifiers are permitted. In first-order theories, predicates are often associated with sets. In interpreted higher-order theories, predicates may be interpreted as sets of sets. There are many deductive systems for first-order logic which are both sound (all provable statements are true in all models) and complete (all statements which are true in all models are provable). Although the logical consequence relation is only semidecidable, much progress has been made in automated theorem proving in first-order logic. First-order logic also satisfies several metalogical theorems that make it amenable to analysis in proof theory, such as the L\u00f6wenheim\u2013Skolem theorem and the compactness theorem. First-order logic is the standard for the formalization of mathematics into axioms and is studied in the foundations of mathematics.Peano arithmetic and Zermelo\u2013Fraenkel set theory are axiomatizations of number theory and set theory, respectively, into first-order logic.No first-order theory, however, has the strength to uniquely describe a structure with an infinite domain, such as the natural numbers or the real line. Axiom systems that do fully describe these two structures (that is, categorical axiom systems) can be obtained in stronger logics such as second-order logic. The foundations of first-order logic were developed independently by Gottlob Frege and Charles Sanders Peirce. For a history of first-order logic and how it came to dominate formal logic, see Jos\u00e9 Ferreir\u00f3s (2001).",
    "Arabic": "Arabic (\u0627\u064e\u0644\u0652\u0639\u064e\u0631\u064e\u0628\u0650\u064a\u064e\u0651\u0629\u064f, al-\u02bfarabiyyah, [al \u0295ara\u02c8bij\u02d0a] () or \u0639\u064e\u0631\u064e\u0628\u0650\u064a\u0651\u200e, \u02bfarab\u012by, [\u02c8\u0295arabi\u02d0] () or [\u0295ara\u02c8bij]) is a Semitic language that first emerged in the 1st to 4th centuries CE. It is now the lingua franca of the Arab world. It is named after the Arabs, a term initially used to describe peoples living in the area bounded by Mesopotamia in the east and the Anti-Lebanon mountains in the west, in Northwestern Arabia and in the Sinai Peninsula. The ISO assigns language codes to thirty varieties of Arabic, including its standard form, Modern Standard Arabic, also referred to as Literary Arabic, which is modernized Classical Arabic. This distinction exists primarily among Western linguists; Arabic speakers themselves generally do not distinguish between Modern Standard Arabic and Classical Arabic, but rather refer to both as al-\u02bfarabiyyatu l-fu\u1e63\u1e25\u0101 (\u0627\u064e\u0644\u0639\u064e\u0631\u064e\u0628\u0650\u064a\u064e\u0651\u0629\u064f \u0671\u0644\u0652\u0641\u064f\u0635\u0652\u062d\u064e\u0649\u0670, \"the purest Arabic\") or simply al-fu\u1e63\u1e25\u0101 (\u0627\u064e\u0644\u0652\u0641\u064f\u0635\u0652\u062d\u064e\u0649\u0670). Arabic is widely taught in schools and universities and is used to varying degrees in workplaces, government and the media. Arabic, in its standard form, is the official language of 26 states, as well as the liturgical language of the religion of Islam, since the Quran and Hadith were written in Arabic. During the Middle Ages, Arabic was a major vehicle of culture in Europe, especially in science, mathematics and philosophy. As a result, many European languages have also borrowed many words from it. Arabic influence, mainly in vocabulary, is seen in European languages\u2014mainly Spanish and to a lesser extent Portuguese and Catalan\u2014owing to both the proximity of Christian European and Muslim Arab civilizations and the long-lasting Arabic culture and language presence mainly in Southern Iberia during the Al-Andalus era. Sicilian has about 500 Arabic words, many of which relate to agriculture and related activities, as a legacy of the Emirate of Sicily from the mid-9th to mid-10th centuries, while Maltese language is a Semitic language developed from a dialect of Arabic and written in the Latin alphabet. The Balkan languages, including Greek and Bulgarian, have also acquired a significant number of Arabic words through contact with Ottoman Turkish. Arabic has influenced many other languages around the globe throughout its history. Some of the most influenced languages are Persian, Turkish, Hindustani (Hindi and Urdu), Kashmiri, Kurdish, Bosnian, Kazakh, Bengali, Malay (Indonesian and Malaysian), Maldivian, Pashto, Punjabi, Albanian, Armenian, Azerbaijani, Sicilian, Spanish, Greek, Bulgarian, Tagalog, Assamese, Sindhi, Odia and Hausa and some languages in parts of Africa. Conversely, Arabic has borrowed words from other languages, including Hebrew, Greek, Aramaic, and Persian in medieval times and languages such as English and French in modern times. Arabic is the liturgical language of 1.8 billion Muslims, and Arabic is one of six official languages of the United Nations. All varieties of Arabic combined are spoken by perhaps as many as 422 million speakers (native and non-native) in the Arab world, making it the fifth most spoken language in the world. Arabic is written with the Arabic alphabet, which is an abjad script and is written from right to left, although the spoken varieties are sometimes written in ASCII Latin from left to right with no standardized orthography.",
    "Computer": "A computer is a machine that can be instructed to carry out sequences of arithmetic or logical operations automatically via computer programming. Modern computers have the ability to follow generalized sets of operations, called programs. These programs enable computers to perform an extremely wide range of tasks. A \"complete\" computer including the hardware, the operating system (main software), and peripheral equipment required and used for \"full\" operation can be referred to as a computer system. This term may as well be used for a group of computers that are connected and work together, in particular a computer network or computer cluster. Computers are used as control systems for a wide variety of industrial and consumer devices. This includes simple special purpose devices like microwave ovens and remote controls, factory devices such as industrial robots and computer-aided design, and also general purpose devices like personal computers and mobile devices such as smartphones. The Internet is run on computers and it connects hundreds of millions of other computers and their users. Early computers were only conceived as calculating devices. Since ancient times, simple manual devices like the abacus aided people in doing calculations. Early in the Industrial Revolution, some mechanical devices were built to automate long tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit (IC) chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s. The speed, power and versatility of computers have been increasing dramatically ever since then, with MOS transistor counts increasing at a rapid pace (as predicted by Moore's law), leading to the Digital Revolution during the late 20th to early 21st centuries. Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU) in the form of a metal-oxide-semiconductor (MOS) microprocessor, along with some type of computer memory, typically MOS semiconductor memory chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved.",
    "Coreference": "In linguistics, coreference, sometimes written co-reference, occurs when two or more expressions in a text refer to the same person or thing; they have the same referent, e.g. Bill said he would come; the proper noun Bill and the pronoun he refer to the same person, namely to Bill. Coreference is the main concept underlying binding phenomena in the field of syntax. The theory of binding explores the syntactic relationship that exists between coreferential expressions in sentences and texts. When two expressions are coreferential, the one is usually a full form (the antecedent) and the other is an abbreviated form (a proform or anaphor). Linguists use indices to show coreference, as with the i index in the example Billi said hei would come. The two expressions with the same reference are coindexed, hence in this example Bill and he are coindexed, indicating that they should be interpreted as coreferential.",
    "Jabberwacky": "Jabberwacky is a chatterbot created by British programmer Rollo Carpenter. Its stated aim is to \"simulate natural human chat in an interesting, entertaining and humorous manner\". It is an early attempt at creating an artificial intelligence through human interaction.",
    "Anaphora_(linguistics)": "In linguistics, anaphora () is the use of an expression whose interpretation depends upon another expression in context (its antecedent or postcedent). In a narrower sense, anaphora is the use of an expression that depends specifically upon an antecedent expression and thus is contrasted with cataphora, which is the use of an expression that depends upon a postcedent expression. The anaphoric (referring) term is called an anaphor. For example, in the sentence Sally arrived, but nobody saw her, the pronoun her is an anaphor, referring back to the antecedent Sally. In the sentence Before her arrival, nobody saw Sally, the pronoun her refers forward to the postcedent Sally, so her is now a cataphor (and an anaphor in the broader, but not the narrower, sense). Usually, an anaphoric expression is a proform or some other kind of deictic (contextually-dependent) expression. Both anaphora and cataphora are species of endophora, referring to something mentioned elsewhere in a dialog or text. Anaphora is an important concept for different reasons and on different levels: first, anaphora indicates how discourse is constructed and maintained; second, anaphora binds different syntactical elements together at the level of the sentence; third, anaphora presents a challenge to natural language processing in computational linguistics, since the identification of the reference can be difficult; and fourth, anaphora partially reveals how language is understood and processed, which is relevant to fields of linguistics interested in cognitive psychology.",
    "Multimodal_sentiment_analysis": "Multimodal sentiment analysis is a new dimension of the traditional text-based sentiment analysis, which goes beyond the analysis of texts, and includes other modalities such as audio and visual data. It can be bimodal, which includes different combinations of two modalities, or trimodal, which incorporates three modalities. With the extensive amount of social media data available online in different forms such as videos and images, the conventional text-based sentiment analysis has evolved into more complex models of multimodal sentiment analysis, which can be applied in the development of virtual assistants, analysis of YouTube movie reviews, analysis of news videos, and emotion recognition (sometimes known as emotion detection) such as depression monitoring, among others. Similar to the traditional sentiment analysis, one of the most basic task in multimodal sentiment analysis is sentiment classification, which classifies different sentiments into categories such as positive, negative, or neutral. The complexity of analyzing text, audio, and visual features to perform such a task requires the application of different fusion techniques, such as feature-level, decision-level, and hybrid fusion. The performance of these fusion techniques and the classification algorithms applied, are influenced by the type of textual, audio, and visual features employed in the analysis.",
    "Controlled_natural_language": "Controlled natural languages (CNLs) are subsets of natural languages that are obtained by restricting the grammar and vocabulary in order to reduce or eliminate ambiguity and complexity. Traditionally, controlled languages fall into two major types: those that improve readability for human readers (e.g. non-native speakers),and those that enable reliable automatic semantic analysis of the language. The first type of languages (often called \"simplified\" or \"technical\" languages), for example ASD Simplified Technical English, Caterpillar Technical English, IBM's Easy English, are used in the industry to increase the quality of technical documentation, and possibly simplify the (semi-)automatic translation of the documentation. These languages restrict the writer by general rules such as \"Keep sentences short\", \"Avoid the use of pronouns\", \"Only use dictionary-approved words\", and \"Use only the active voice\". The second type of languages have a formal syntax and semantics, and can be mapped to an existing formal language, such as first-order logic. Thus, those languages can be used as knowledge representation languages, and writing of those languages is supported by fully automatic consistency and redundancy checks, query answering, etc.",
    "European_Union": "The European Union (EU) is a political and economic union of 27 member states that are located primarily in Europe. Its members have a combined area of 4,233,255.3 km2 (1,634,469.0 sq mi) and an estimated total population of about 447 million. The EU has developed an internal single market through a standardised system of laws that apply in all member states in those matters, and only those matters, where members have agreed to act as one. EU policies aim to ensure the free movement of people, goods, services and capital within the internal market; enact legislation in justice and home affairs; and maintain common policies on trade, agriculture, fisheries and regional development. Passport controls have been abolished for travel within the Schengen Area. A monetary union was established in 1999, coming into full force in 2002, and is composed of 19 EU member states which use the euro currency. The EU has often been described as a sui generis political entity (without precedent or comparison). The EU and European citizenship were established when the Maastricht Treaty came into force in 1993. The EU traces its origins to the European Coal and Steel Community (ECSC) and the European Economic Community (EEC), established, respectively, by the 1951 Treaty of Paris and 1957 Treaty of Rome. The original members of what came to be known as the European Communities were the Inner Six: Belgium, France, Italy, Luxembourg, the Netherlands, and West Germany. The Communities and their successors have grown in size by the accession of new member states and in power by the addition of policy areas to their remit. The latest major amendment to the constitutional basis of the EU, the Treaty of Lisbon, came into force in 2009. On 31 January 2020, the United Kingdom became the first member state to leave the EU. Following a 2016 referendum, the UK signified its intention to leave and negotiated a withdrawal agreement. The UK is in a transitional phase until at least 31 December 2020, during which it remains subject to EU law and part of the EU single market and customs union. Before this, three territories of member states had left the EU or its forerunners, these being French Algeria (in 1962, upon independence), Greenland (in 1985, following a referendum) and Saint Barth\u00e9lemy (in 2012). Containing in 2020 some 5.8% of the world population, the EU (excluding the United Kingdom) had generated a nominal gross domestic product (GDP) of around US$15.5 trillion in 2019, constituting approximately 18% of global nominal GDP. Additionally, all EU countries have a very high Human Development Index according to the United Nations Development Programme. In 2012, the EU was awarded the Nobel Peace Prize. Through the Common Foreign and Security Policy, the union has developed a role in external relations and defence. It maintains permanent diplomatic missions throughout the world and represents itself at the United Nations, the World Trade Organization, the G7 and the G20. Due to its global influence, the European Union has been described as an emerging superpower.",
    "Corpus_linguistics": "Corpus linguistics is the study of language as expressed in corpora (samples) of \"real world\" text. Corpus linguistics proposes that reliable language analysis is more feasible with corpora collected in the field in its natural context (\"realia\"), and with minimal experimental-interference. The field of corpus linguistics features divergent views about the value of corpus annotation. These views range from John McHardy Sinclair, who advocates minimal annotation so texts speak for themselves, to the Survey of English Usage team (University College, London), who advocate annotation as allowing greater linguistic understanding through rigorous recording. The text-corpus method is a digestive approach that derives a set of abstract rules that govern a natural language from texts in that language, and explores how that language relates to other languages. Originally derived manually, corpora now are automatically derived from source texts. In addition to linguistics research, assembled corpora have been used to compile dictionaries (starting with The American Heritage Dictionary of the English Language in 1969) and grammar guides, such as A Comprehensive Grammar of the English Language, published in 1985.",
    "Computer-assisted_reviewing": "Computer-assisted reviewing (CAR) tools are pieces of software based on text-comparison and analysis algorithms. These tools focus on the differences between two documents, taking into account each document's typeface through an intelligent analysis.",
    "Computer_science": "Computer science is the study of computation and information. Computer science deals with theory of computation, algorithms, computational problems and the design of computer systems hardware, software and applications. Computer science addresses both human-made and natural information processes, such as communication, control, perception, learning and intelligence especially in human-made computing systems and machines. According to Peter Denning, the fundamental question underlying computer science is, What can be automated? Its fields can be divided into theoretical and practical disciplines. Computational complexity theory is highly abstract, while computer graphics and computational geometry emphasizes real-world applications. Algorithmics is called the heart of computer science. Programming language theory considers approaches to the description of computational processes, while software engineering involves the use of programming languages and complex systems. Computer architecture and computer engineering deals with construction of computer components and computer-controlled equipment. Human\u2013computer interaction considers the challenges in making computers useful, usable, and accessible. Artificial intelligence aims to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, motion planning, learning, and communication found in humans and animals.",
    "Person-centered_therapy": "Person-centered therapy, also known as person-centered psychotherapy, person-centered counseling, client-centered therapy and Rogerian psychotherapy, is a form of psychotherapy developed by psychologist Carl Rogers beginning in the 1940s and extending into the 1980s. Person-centered therapy seeks to facilitate a client's self-actualizing tendency, \"an inbuilt proclivity toward growth and fulfillment\", via acceptance (unconditional positive regard), therapist congruence (genuineness), an empathic understanding.",
    "Closed-world_assumption": "The closed-world assumption (CWA), in a formal system of logic used for knowledge representation, is the presumption that a statement that is true is also known to be true. Therefore, conversely, what is not currently known to be true, is false. The same name also refers to a logical formalization of this assumption by Raymond Reiter. The opposite of the closed-world assumption is the open-world assumption (OWA), stating that lack of knowledge does not imply falsity. Decisions on CWA vs. OWA determine the understanding of the actual semantics of a conceptual expression with the same notations of concepts. A successful formalization of natural language semantics usually cannot avoid an explicit revelation of whether the implicit logical backgrounds are based on CWA or OWA. Negation as failure is related to the closed-world assumption, as it amounts to believing false every predicate that cannot be proved to be true.",
    "Frame_semantics_(linguistics)": "Frame semantics is a theory of linguistic meaning developed by Charles J. Fillmore that extends his earlier case grammar. It relates linguistic semantics to encyclopedic knowledge. The basic idea is that one cannot understand the meaning of a single word without access to all the essential knowledge that relates to that word. For example, one would not be able to understand the word \"sell\" without knowing anything about the situation of commercial transfer, which also involves, among other things, a seller, a buyer, goods, money, the relation between the money and the goods, the relations between the seller and the goods and the money, the relation between the buyer and the goods and the money and so on. Thus, a word activates, or evokes, a frame of semantic knowledge relating to the specific concept to which it refers (or highlights, in frame semantic terminology). The idea of the encyclopedic organisation of knowledge itself is old and was discussed by Age of Enlightenment philosophers such as Denis Diderot and Giambattista Vico. Fillmore and other evolutionary and cognitive linguists like John Haiman and Adele Goldberg, however, make an argument against generative grammar and truth-conditional semantics. As is elementary for Lakoffian\u2013Langackerian Cognitive Linguistics, it is claimed that knowledge of language is no different from other types of knowledge; therefore there is no grammar in the traditional sense, and language is not an independent cognitive function. Instead, the spreading and survival of linguistic units is directly comparable to that of other types of units of cultural evolution, like in memetics and other cultural replicator theories.",
    "Computing_Machinery_and_Intelligence": "\"Computing Machinery and Intelligence\" is a seminal paper written by Alan Turing on the topic of artificial intelligence. The paper, published in 1950 in Mind, was the first to introduce his concept of what is now known as the Turing test to the general public. Turing's paper considers the question \"Can machines think?\" Since the words \"think\" and \"machine\" cannot be defined in a clear way that satisfies everyone, Turing suggests we \"replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.\" To do this, he must first find a simple and unambiguous idea to replace the word \"think\", second he must explain exactly which \"machines\" he is considering, and finally, armed with these tools, he formulates a new question, related to the first, that he believes he can answer in the affirmative.",
    "Stemming": "In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form\u2014generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation. A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer.",
    "Turkish_language": "Turkish (T\u00fcrk\u00e7e ()), also referred to as Istanbul Turkish (\u0130stanbul T\u00fcrk\u00e7esi) or Turkey Turkish (T\u00fcrkiye T\u00fcrk\u00e7esi), is the most widely spoken of the Turkic languages, with around 70 to 80 million speakers, mostly in Turkey. Outside its native country, significant smaller groups of speakers exist in Germany, Austria, Bulgaria, North Macedonia, Northern Cyprus, Greece, the Caucasus, and other parts of Europe and Central Asia. Cyprus has requested that the European Union add Turkish as an official language, even though Turkey is not a member state. To the west, the influence of Ottoman Turkish\u2014the variety of the Turkish language that was used as the administrative and literary language of the Ottoman Empire\u2014spread as the Ottoman Empire expanded. In 1928, as one of Atat\u00fcrk's Reforms in the early years of the Republic of Turkey, the Ottoman Turkish alphabet was replaced with a Latin alphabet. The distinctive characteristics of the Turkish language are vowel harmony and extensive agglutination. The basic word order of Turkish is subject\u2013object\u2013verb. Turkish has no noun classes or grammatical gender. The language makes usage of honorifics and has a strong T\u2013V distinction which distinguishes varying levels of politeness, social distance, age, courtesy or familiarity toward the addressee. The plural second-person pronoun and verb forms are used referring to a single person out of respect.",
    "Cognitive_science": "Cognitive science is the interdisciplinary, scientific study of the mind and its processes. It examines the nature, the tasks, and the functions of cognition (in a broad sense). Cognitive scientists study intelligence and behavior, with a focus on how nervous systems represent, process, and transform information. Mental faculties of concern to cognitive scientists include language, perception, memory, attention, reasoning, and emotion; to understand these faculties, cognitive scientists borrow from fields such as linguistics, psychology, artificial intelligence, philosophy, neuroscience, and anthropology. The typical analysis of cognitive science spans many levels of organization, from learning and decision to logic and planning; from neural circuitry to modular brain organization. One of the fundamental concepts of cognitive science is that \"thinking can best be understood in terms of representational structures in the mind and computational procedures that operate on those structures.\" Cognitive science is the interdisciplinary study of cognition in humans, animals, and machines. It encompasses the traditional disciplines of psychology, computer science, neuroscience, anthropology, linguistics and philosophy. The goal of cognitive science is to understand the principles of intelligence with the hope that this will lead to better comprehension of the mind and of learning and to develop intelligent devices.The cognitive sciences began as an intellectual movement in the 1950s often referred to as the cognitive revolution.",
    "Semantic_parsing": "Semantic parsing is the task of converting a natural language utterance to a logical form: a machine-understandable representation of its meaning. Semantic parsing can thus be understood as extracting the precise meaning of an utterance. Applications of semantic parsing include machine translation, question answering, ontology induction, automated reasoning, and code generation. The phrase was first used in the 1970s by Yorick Wilks as the basis for machine translation programs working with only semantic representations.",
    "Thai_language": "Thai, Central Thai (historically Siamese; : \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22), is the national language of Thailand and de facto official language; it is the first language of the Central Thai people. It is a member of the Tai group of the Kra\u2013Dai language family, and one of over 60 languages of Thailand. Over half of Thai vocabulary is derived from or borrowed from Pali, Sanskrit, Mon and Old Khmer. It is a tonal and analytic language, similar to Chinese and Vietnamese. Thai has a complex orthography and system of relational markers. Spoken Thai is partly, depending on standard sociolinguistic factors such as age, gender, class, spatial proximity, and the urban/rural divide, mutually intelligible with Lao, Isan, and some fellow Southwestern Tai languages. These languages are written with slightly different scripts but are linguistically similar and effectively form a dialect continuum.",
    "Apertium": "Apertium is a free/open-source rule-based machine translation platform. It is free software and released under the terms of the GNU General Public License.",
    "Referring_expression": "In linguistics, a referring expression (RE) is any noun phrase, or surrogate for a noun phrase, whose function in discourse is to identify some individual object. The technical terminology for identify differs a great deal from one school of linguistics to another. The most widespread term is probably refer, and a thing identified is a referent, as for example in the work of John Lyons. In linguistics, the study of reference relations belongs to pragmatics, the study of language use, though it is also a matter of great interest to philosophers, especially those wishing to understand the nature of knowledge, perception and cognition more generally. Various devices can be used for reference including determiners, pronouns, proper names. Reference relations can be of different kinds; referents can be in a \"real\" or imaginary world, or in discourse itself, and they may be singular, plural, or collective.",
    "Language_and_Communication_Technologies": "Language and Communication Technologies (LCT; also known as human language technologies or language technology for short) is the scientific study of technologies that explore language and communication. It is an interdisciplinary field that encompasses the fields of computer science, linguistics and cognitive science.",
    "George_Lakoff": "George Philip Lakoff (; born May 24, 1941) is an American cognitive linguist and philosopher, best known for his thesis that lives of individuals are significantly influenced by the central metaphors they use to explain complex phenomena. The conceptual metaphor thesis, introduced in his and Mark Johnson's 1980 book Metaphors We Live By has found applications in a number of academic disciplines. Applying it to politics, literature, philosophy and mathematics has led Lakoff into territory normally considered basic to political science. In his 1996 book Moral Politics, Lakoff described conservative voters as being influenced by the \"strict father model\" as a central metaphor for such a complex phenomenon as the state, and liberal/progressive voters as being influenced by the \"nurturant parent model\" as the folk psychological metaphor for this complex phenomenon. According to him, an individual's experience and attitude towards sociopolitical issues is influenced by being framed in linguistic constructions. In Metaphor and War: The Metaphor System Used to Justify War in the Gulf (1991), he argues that the American involvement in the Gulf war was obscured or \"spun\" by the metaphors which were used by the first Bush administration to justify it.Between 2003 and 2008, Lakoff was involved with a progressive think tank, the now defunct Rockridge Institute. He is a member of the scientific committee of the Fundaci\u00f3n IDEAS (IDEAS Foundation), Spain's Socialist Party's think tank. The more general theory that elaborated his thesis is known as embodied mind. Lakoff served as a professor of linguistics at the University of California, Berkeley, from 1972 until his retirement in 2016.",
    "Chatbot": "A chatbot is a software application used to conduct an on-line chat conversation via text or text-to-speech, in lieu of providing direct contact with a live human agent. Designed to convincingly simulate the way a human would behave as a conversational partner, chatbot systems typically require continuous tuning and testing, and many in production remain unable to adequately converse or pass the industry standard Turing test. The term \"ChatterBot\" was originally coined by Michael Mauldin (creator of the first Verbot) in 1994 to describe these conversational programs. Chatbots are typically used in dialog systems for various purposes including customer service, request routing, or for information gathering. While some chatbot applications use extensive word-classification processes, Natural Language processors, and sophisticated AI, others simply scan for general keywords and generate responses using common phrases obtained from an associated library or database. Today, most chatbots are accessed on-line via website popups, or through virtual assistants such as Google Assistant, Amazon Alexa, or messaging apps such as Facebook Messenger or WeChat. Chatbots are typically classified into usage categories that include: commerce (e-commerce via chat), education, entertainment, finance, health, news, and productivity.",
    "Moore's_law": "Moore's law is the observation that the number of transistors in a dense integrated circuit (IC) doubles about every two years. Moore's law is an observation and projection of a historical trend. Rather than a law of physics, it is an empirical relationship linked to gains from experience in production. The observation is named after Gordon Moore, the co-founder of Fairchild Semiconductor and CEO and co-founder of Intel, who in 1965 posited a doubling every year in the number of components per integrated circuit, and projected this rate of growth would continue for at least another decade. In 1975, looking forward to the next decade, he revised the forecast to doubling every two years, a compound annual growth rate (CAGR) of 40%. While Moore did not use empirical evidence in forecasting that the historical trend would continue, his prediction held since 1975 and has since become known as a \"law.\" Moore's prediction has been used in the semiconductor industry to guide long-term planning and to set targets for research and development. Advancements in digital electronics, such as the reduction in quality-adjusted microprocessor prices, the increase in memory capacity (RAM and flash), the improvement of sensors, and even the number and size of pixels in digital cameras, are strongly linked to Moore's law. These step changes in digital electronics have been a driving force of technological and social change, productivity, and economic growth. Industry experts have not reached a consensus on exactly when Moore's law will cease to apply. Microprocessor architects report that semiconductor advancement has slowed industry-wide since around 2010, below the pace predicted by Moore's law. However, as of 2018, leading semiconductor manufacturers have developed IC fabrication processes in mass production which are claimed to keep pace with Moore's law.",
    "Semi-supervised_learning": "Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). Unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render large, fully labeled training sets infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning. A set of  independently identically distributed examples  with corresponding labels  and  unlabeled examples  are processed. Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning. Semi-supervised learning may refer to either transductive learning or inductive learning. The goal of transductive learning is to infer the correct labels for the given unlabeled data  only. The goal of inductive learning is to infer the correct mapping from  to . Intuitively, the learning problem can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam. It is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.",
    "1_the_Road": "1 the Road is an experimental novel composed by artificial intelligence (AI). Emulating Jack Kerouac's On the Road, Ross Goodwin drove from New York to New Orleans in March 2017 with an AI in a laptop hooked up to various sensors, whose output the AI turned into words that were printed on rolls of receipt paper. The novel was published in 2018 by Jean Bo\u00eete \u00c9ditions. Goodwin left the text unedited. Although he felt the prose was \"choppy\", and contained typographical errors, he wanted to present the machine-generated text verbatim, for future study. The story begins: \"It was nine seventeen in the morning, and the house was heavy\".",
    "Semantic_role_labeling": "In natural language processing, semantic role labeling (also called shallow semantic parsing or slot-filling) is the process that assigns labels to words or phrases in a sentence that indicate their semantic role in the sentence, such as that of an agent, goal, or result. It consists of the detection of the semantic arguments associated with the predicate or verb of a sentence and their classification into their specific roles. For example, given a sentence like \"Mary sold the book to John\", the task would be to recognize the verb \"to sell\" as representing the predicate, \"Mary\" as representing the seller (agent), \"the book\" as representing the goods (theme), and \"John\" as representing the recipient. This is an important step towards making sense of the meaning of a sentence. A semantic analysis of this sort is at a lower-level of abstraction than a syntax tree, i.e. it has more categories, thus groups fewer clauses in each category. For instance, \"the book belongs to me\" would need two labels such as \"possessed\" and \"possessor\" whereas \"the book was sold to John\" would need two other labels such as \"goal\" (or \"theme\") and \"receiver\" (or \"recipient\") even though these two clauses would be very similar as far as \"subject\" and \"object\" functions are concerned.",
    "Reification_(linguistics)": "Reification in natural language processing refers to where a natural language statement is transformed so actions and events in it become quantifiable variables. For example \"John chased the duck furiously\" can be transformed into something like (Exists e)(chasing(e) & past_tense(e) & actor(e,John) & furiously(e) & patient(e,duck)). Another example would be \"Sally said John is mean\", which could be expressed as something like (Exists u,v)(saying(u) & past_tense(u) & actor(u,Sally) & that(u,v) & is(v) & actor(v,John) & mean(v)). Such representations allow one to use the tools of classical first-order predicate calculus even for statements which, due to their use of tense, modality, adverbial constructions, propositional arguments (e.g. \"Sally said that X\"), etc., would have seemed intractable. This is an advantage because predicate calculus is better understood and simpler than the more complex alternatives (higher-order logics, modal logics, temporal logics, etc.), and there exist better automated tools (e.g. automated theorem provers and model checkers) for manipulating it. Reified forms can be used for other purposes besides the application of first-order logic; one example is the automatic discovery of synonymous phrases. The reified forms are sometimes called quasi-logical forms, and the existential variables are sometimes treated as Skolem constants. Not all natural language constructs admit a uniform translation to first order logic. See donkey sentence for examples and a discussion.",
    "Deep_linguistic_processing": "Deep linguistic processing is a natural language processing framework which draws on theoretical and descriptive linguistics. It models language predominantly by way of theoretical syntactic/semantic theory (e.g. CCG, HPSG, LFG, TAG, the Prague School). Deep linguistic processing approaches differ from \"shallower\" methods in that they yield more expressive and structural representations which directly capture long-distance dependencies and underlying predicate-argument structures. The knowledge-intensive approach of deep linguistic processing requires considerable computational power, and has in the past sometimes been judged as being intractable. However, research in the early 2000s had made considerable advancement in efficiency of deep processing. Today, efficiency is no longer a major problem for applications using deep linguistic processing.",
    "Lesk_algorithm": "The Lesk algorithm is a classical algorithm for word sense disambiguation introduced by Michael E. Lesk in 1986.",
    "Category:Speech_recognition": "",
    "Speech_act": "In the philosophy of language and linguistics, speech act is something expressed by an individual that not only presents information, but performs an action as well. For example, the phrase \"I would like the kimchi, could you please pass it to me?\" is considered a speech act as it expresses the speaker's desire to acquire the kimchi, as well as presenting a request that someone pass the kimchi to them. According to Kent Bach, \"almost any speech act is really the performance of several acts at once, distinguished by different aspects of the speaker's intention: there is the act of saying something, what one does in saying it, such as requesting or promising, and how one is trying to affect one's audience\". The contemporary use of the term goes back to J. L. Austin's development of performative utterances and his theory of locutionary, illocutionary, and perlocutionary acts. Speech acts serve their function once they are said or communicated. These are commonly taken to include acts such as apologizing, promising, ordering, answering, requesting, complaining, warning, inviting, refusing, and congratulating.",
    "Statistical_model": "A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process. A statistical model is usually specified as a mathematical relationship between one or more random variables and other non-random variables. As such, a statistical model is \"a formal representation of a theory\" (Herman Ad\u00e8r quoting Kenneth Bollen). All statistical hypothesis tests and all statistical estimators are derived via statistical models. More generally, statistical models are part of the foundation of statistical inference.",
    "Terminology_extraction": "Terminology extraction (also known as term extraction, glossary extraction, term recognition, or terminology mining) is a subtask of information extraction. The goal of terminology extraction is to automatically extract relevant terms from a given corpus. In the semantic web era, a growing number of communities and networked enterprises started to access and interoperate through the internet. Modeling these communities and their information needs is important for several web applications, like topic-driven web crawlers, web services, recommender systems, etc. The development of terminology extraction is also essential to the language industry. One of the first steps to model a knowledge domain is to collect a vocabulary of domain-relevant terms, constituting the linguistic surface manifestation of domain concepts. Several methods to automatically extract technical terms from domain-specific document warehouses have been described in the literature. Typically, approaches to automatic term extraction make use of linguistic processors (part of speech tagging, phrase chunking) to extract terminological candidates, i.e. syntactically plausible terminological noun phrases. Noun phrases include compounds (e.g. \"credit card\"), adjective noun phrases (e.g. \"local tourist information office\"), and prepositional noun phrases (e.g. \"board of directors\"). In English, the first two (compounds and adjective noun phrases) are the most frequent. Terminological entries are then filtered from the candidate list using statistical and machine learning methods. Once filtered, because of their low ambiguity and high specificity, these terms are particularly useful for conceptualizing a knowledge domain or for supporting the creation of a domain ontology or a terminology base. Furthermore, terminology extraction is a very useful starting point for semantic similarity, knowledge management, human translation and machine translation, etc.",
    "Georgetown\u2013IBM_experiment": "The Georgetown\u2013IBM experiment was an influential demonstration of machine translation, which was performed during January 7, 1954. Developed jointly by the Georgetown University and IBM, the experiment involved completely automatic translation of more than sixty Russian sentences into English.",
    "Native-language_identification": "Native-language identification (NLI) is the task of determining an author's native language (L1) based only on their writings in a second language (L2). NLI works through identifying language-usage patterns that are common to specific L1 groups and then applying this knowledge to predict the native language of previously unseen texts. This is motivated in part by applications in second-language acquisition, language teaching and forensic linguistics, amongst others.",
    "Optical_character_recognition": "Optical character recognition or optical character reader (OCR) is the electronic or mechanical conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example: from a television broadcast). Widely used as a form of data entry from printed paper data records \u2013 whether passport documents, invoices, bank statements, computerized receipts, business cards, mail, printouts of static-data, or any suitable documentation \u2013 it is a common method of digitizing printed texts so that they can be electronically edited, searched, stored more compactly, displayed on-line, and used in machine processes such as cognitive computing, machine translation, (extracted) text-to-speech, key data and text mining. OCR is a field of research in pattern recognition, artificial intelligence and computer vision. Early versions needed to be trained with images of each character, and worked on one font at a time. Advanced systems capable of producing a high degree of recognition accuracy for most fonts are now common, and with support for a variety of digital image file format inputs. Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components.",
    "Statistical_machine_translation": "Statistical machine translation (SMT) is a machine translation paradigm where translations are generated on the basis of statistical models whose parameters are derived from the analysis of bilingual text corpora. The statistical approach contrasts with the rule-based approaches to machine translation as well as with example-based machine translation. The first ideas of statistical machine translation were introduced by Warren Weaver in 1949, including the ideas of applying Claude Shannon's information theory. Statistical machine translation was re-introduced in the late 1980s and early 1990s by researchers at IBM's Thomas J. Watson Research Center and has contributed to the significant resurgence in interest in machine translation in recent years. Before the introduction of neural machine translation, it was by far the most widely studied machine translation method.",
    "Language_model": "A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability  to the whole sequence. The language model provides context to distinguish between words and phrases that sound similar. For example, in American English, the phrases \"recognize speech\" and \"wreck a nice beach\" sound similar, but mean different things. Data sparsity is a major problem in building language models. Most possible word sequences are not observed in training. One solution is to make the assumption that the probability of a word only depends on the previous n words. This is known as an n-gram model or unigram model when n = 1. The unigram model is also known as the bag of words model. Estimating the relative likelihood of different phrases is useful in many natural language processing applications, especially those that generate text as an output. Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications. In speech recognition, sounds are matched with word sequences. Ambiguities are easier to resolve when evidence from the language model is integrated with a pronunciation model and an acoustic model. Language models are used in information retrieval in the query likelihood model. There, a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document's language model : . Commonly, the unigram language model is used for this purpose.",
    "Abbreviation": "An abbreviation (from Latin brevis, meaning short ) is a shortened form of a word or phrase, by any method. It may consist of a group of letters, or words taken from the full version of the word or phrase; for example, the word abbreviation can itself be represented by the abbreviation abbr., abbrv., or abbrev.; NBM, for nil (or nothing) by mouth is an abbreviated medical instruction. It may also consist of initials only, a mixture of initials and words, or words or letters representing words in another language (for example, e.g., i.e. or RSVP). Some types of abbreviations are acronyms (which are pronounceable), initialisms (using initials only), or grammatical contractions or crasis. An abbreviation is a shortening by any of these, or other, methods.",
    "Text_simplification": "Text simplification is an operation used in natural language processing to modify, enhance, classify or otherwise process an existing corpus of human-readable text in such a way that the grammar and structure of the prose is greatly simplified, while the underlying meaning and information remains the same. Text simplification is an important area of research, because natural human languages ordinarily contain large vocabularies and complex compound constructions that are not easily processed through automation. In terms of reducing language diversity, semantic compression can be employed to limit and simplify a set of words used in given texts.",
    "Noun": "A noun (from Latin n\u014dmen, literally 'name') is a word that functions as the name of some specific thing or set of things, such as living creatures, objects, places, actions, qualities, states of existence, or ideas. However, noun is not a semantic category, so that it cannot be characterized in terms of its meaning. Thus, actions and states of existence can also be expressed by verbs, qualities by adjectives, and places by adverbs. Linguistically, a noun is a member of a large, open part of speech whose members can occur as the main word in the subject of a clause, the object of a verb, or the object of a preposition. Lexical categories (parts of speech) are defined in terms of the ways in which their members combine with other kinds of expressions. The syntactic rules for nouns differ from language to language. In English, nouns are those words which can occur with articles and attributive adjectives and can function as the head of a noun phrase. \"As far as we know, every language makes a grammatical distinction that looks like a noun verb distinction.\"",
    "Category:Natural_language_processing": "",
    "Time_complexity": "In computer science, the time complexity is the computational complexity that describes the amount of time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor. Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the worst-case time complexity, which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the average-case complexity, which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size). In both cases, the time complexity is generally expressed as a function of the size of the input. Since this function is generally difficult to compute exactly, and the running time for small inputs is usually not consequential, one commonly focuses on the behavior of the complexity when the input size increases\u2014that is, the asymptotic behavior of the complexity. Therefore, the time complexity is commonly expressed using big O notation, typically    etc., where n is the input size in units of bits needed to represent the input. Algorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity  is a linear time algorithm and an algorithm with time complexity  for some constant  is a polynomial time algorithm.",
    "Value_(mathematics)": "In mathematics, value may refer to several, strongly related notions. In general, a mathematical value may be any definite mathematical object. In elementary mathematics, this is most often a number \u2013 for example, a real number such as \u03c0 or an integer such as 42. \n* The value of a variable or a constant is any number or other mathematical object assigned to it. \n* The value of a mathematical expression is the result of the computation described by this expression when the variables and constants in it are assigned values. \n* The value of a function, given the value(s) assigned to its argument(s), is the value assumed by the function for these argument values. For example, if the function f is defined by f(x) = 2x2 \u2013 3x + 1, then assigning the value 3 to its argument x yields the function value 10, since f(3) = 2\u00b732 \u2013 3\u00b73 + 1 = 10. If the variable, expression or function only assumes real values, it is called real-valued. Likewise, a complex-valued variable, expression or function only assumes complex values.",
    "File:Automated_online_assistant.png": "",
    "French_language": "French (fran\u00e7ais [f\u0281\u0251\u0303s\u025b] or langue fran\u00e7aise [l\u0251\u0303\u0261 f\u0281\u0251\u0303s\u025b\u02d0z]) is a Romance language of the Indo-European family. It descended from the Vulgar Latin of the Roman Empire, as did all Romance languages. French evolved from Gallo-Romance, the Latin spoken in Gaul, and more specifically in Northern Gaul. Its closest relatives are the other langues d'o\u00efl\u2014languages historically spoken in northern France and in southern Belgium, which French (Francien) largely supplanted. French was also influenced by native Celtic languages of Northern Roman Gaul like Gallia Belgica and by the (Germanic) Frankish language of the post-Roman Frankish invaders. Today, owing to France's past overseas expansion, there are numerous French-based creole languages, most notably Haitian Creole. A French-speaking person or nation may be referred to as Francophone in both English and French. French is an official language in 29 countries across multiple continents, most of which are members of the Organisation internationale de la Francophonie (OIF), the community of 84 countries which share the official use or teaching of French. French is also one of six official languages used in the United Nations. It is spoken as a first language (in descending order of the number of speakers) in France, the Canadian provinces of Quebec, Ontario and New Brunswick as well as other Francophone regions, Belgium (Wallonia and the Brussels-Capital Region), western Switzerland (cantons of Bern, Fribourg, Geneva, Jura, Neuch\u00e2tel, Vaud, Valais), Monaco, partly in Luxembourg, the states of Louisiana, Maine, New Hampshire and Vermont in the United States, and in northwestern Italy (region of Aosta Valley), and by various communities elsewhere. In 2015, approximately 40% of the francophone population (including L2 and partial speakers) lived in Europe, 35% in sub-Saharan Africa, 15% in North Africa and the Middle East, 8% in the Americas, and 1% in Asia and Oceania. French is the fourth most widely spoken mother tongue in the European Union. Of Europeans who speak other languages natively, approximately one-fifth are able to speak French as a second language. French is the second most taught foreign language in the EU. French is also the 18th most natively spoken language in the world, 6th most spoken language by total number of speakers and the second or third most studied language worldwide (with about 120 million current learners). As a result of French and Belgian colonialism from the 16th century onward, French was introduced to new territories in the Americas, Africa and Asia. Most second-language speakers reside in Francophone Africa, in particular Gabon, Algeria, Morocco, Tunisia, Mauritius, Senegal and Ivory Coast. French is estimated to have about 76 million native speakers and about 235 million daily, fluent speakers and another 77 to 110 million secondary speakers who speak it as a second language to varying degrees of proficiency, mainly in Africa. According to the Organisation internationale de la Francophonie (OIF), approximately 300 million people worldwide are \"able to speak the language\", without specifying the criteria for this estimation or whom it encompasses. According to a demographic projection led by the Universit\u00e9 Laval and the R\u00e9seau D\u00e9mographie de l'Agence universitaire de la francophonie, the total number of French speakers will reach approximately 500 million in 2025 and 650 million by 2050. OIF estimates 700 million by 2050, 80% of whom will be in Africa. French has a long history as an international language of literature and scientific standards and is a primary or second language of many international organisations including the United Nations, the European Union, the North Atlantic Treaty Organization, the World Trade Organization, the International Olympic Committee, and the International Committee of the Red Cross. In 2011, Bloomberg Businessweek ranked French the third most useful language for business, after English and Standard Mandarin Chinese.",
    "Sentiment_analysis": "Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine."
}